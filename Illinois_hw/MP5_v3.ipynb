{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install gym pyvirtualdisplay\n",
    "# !sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install --upgrade setuptools --user\n",
    "# !pip3 install ez_setup \n",
    "# !pip3 install gym[atari] \n",
    "# !pip3 install gym[accept-rom-license] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "11.8\n",
      "CUDA device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if CUDA is available\n",
    "print(torch.version.cuda)  # Should print 11.8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"CUDA device: {device}\")  # Expected output: \"cuda:0\" if GPU is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "# import gym\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from model import *\n",
    "from config import *\n",
    "from checkpoint import *\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create metadata for checkpointing puposes\n",
    "def create_metadata(agent, global_episode, global_frame, eval_rewards, rewards, episodes, last_10_ep_losses, loss_tracker, epsilon, q_stats_hist, qmean_win, qmin_win, qmax_win, beta=None, alpha=None):\n",
    "    return {\n",
    "    'global_episode': global_episode,\n",
    "    'global_frame': global_frame,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'sch_gamma': scheduler_gamma,\n",
    "    'sch_step_size': scheduler_step_size,\n",
    "    'tgt_update_freq': update_target_network_frequency,\n",
    "    'memory capacity': Memory_capacity,  \n",
    "    'explore steps': EXPLORE_STEPS, \n",
    "    'epsilon_decay_rate': agent.epsilon_decay_rate,\n",
    "    'sticky_action_prob': sticky_action_prob,\n",
    "    'eval_rewards': eval_rewards,\n",
    "    'rewards': rewards,\n",
    "    'episodes': episodes,\n",
    "    'last_10_ep_losses': last_10_ep_losses,\n",
    "    'loss_tracker': loss_tracker,\n",
    "    'epislon': epsilon,\n",
    "    'q_stats_hist': q_stats_hist,\n",
    "    'qmean_win': qmean_win,\n",
    "    'qmin_win': qmin_win,\n",
    "    'qmax_win': qmax_win,\n",
    "    'peralpha': alpha,\n",
    "    'per_beta': beta,\n",
    "    'lr': agent.scheduler.get_last_lr()[0],\n",
    "    'scheduler_step': agent.scheduler.last_epoch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "class TeeLogger:\n",
    "    def __init__(self, filepath):\n",
    "        self.original_stdout = sys.__stdout__  # raw terminal (useful fallback)\n",
    "        self.ipython_stdout = sys.stdout       # the notebook's visible output\n",
    "        self.log = open(filepath, \"w\")\n",
    "\n",
    "    def write(self, message):\n",
    "        self.ipython_stdout.write(message)\n",
    "        self.log.write(message)\n",
    "        self.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        self.ipython_stdout.flush()\n",
    "        self.log.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the UIlliniois assigment this code is based on, I will be using the Gymnasium (https://github.com/farama-Foundation/gymnasium) and the Atari Learning Environment (ALE, link here: https://ale.farama.org/), rather than gym, which has been deprecated.  We will still be playing Breakout on Atari.\n",
    "\n",
    "To replecate BreakoutDeterministic-v4, as a starting point we'll use ALE/Breakout-v5 with frameskip=4, no \"sticky actions\" (i.e. deterministic actions), and a limited action space of NOOP, FIRE, LEFT, RIGHT.  However we won't be training on FIRE, instead we will ensure FIRE only happens when the game first starts and when a ball/life is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.0+dfae0bd)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('BreakoutDeterministic-v4')\n",
    "env = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=0.0, full_action_space=False)  # Use equivalent parameters to BreakoutDeterministic-v4\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Env Frame Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(185, 160, 3)\n",
      "height:  185 width:  160\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 160x185 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create environments\n",
    "env = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=sticky_action_prob, full_action_space=False, render_mode=\"rgb_array\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "frame = env.render()\n",
    "frame = frame[20:-5, :, :]\n",
    "\n",
    "# Calculate figure size in inches to match pixel size\n",
    "height, width = frame.shape[:2]\n",
    "print(frame.shape)\n",
    "print(\"height: \", height, \"width: \", width)\n",
    "dpi = plt.rcParams['figure.dpi']\n",
    "figsize = (width / dpi, height / dpi)\n",
    "\n",
    "# Plot with exact pixel size\n",
    "plt.figure(figsize=figsize, dpi=dpi)\n",
    "plt.imshow(frame)\n",
    "plt.title(\"Raw Render Output (Exact Pixel Dimensions)\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #noop, left, and right.  Fire ball (action 1) is not trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Replay Memory Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ### Pickle Replay Creator ###\n",
    "# from gymnasium.wrappers import RecordVideo\n",
    "# from config import *\n",
    "\n",
    "\n",
    "# mem_name = 'CircularPERBuffer_for_testing_using_Run13_orig_get_frame'\n",
    "\n",
    "# #create fresh environment\n",
    "# env = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=sticky_action_prob, full_action_space=False, render_mode='rgb_array')\n",
    "# # env = RecordVideo(env, video_folder=\"./videos\", episode_trigger=lambda ep: ep % 20 == 0)  \n",
    "\n",
    "# # Choose whether to use double DQN\n",
    "# double_dqn = False # set to True if using double DQN agent\n",
    "# if double_dqn:\n",
    "#     from agent_double import Agent\n",
    "# else:\n",
    "#     from agent import Agent\n",
    "\n",
    "# # print(\"Instantiating agent\")\n",
    "# agent = Agent(action_size)\n",
    "# agent.load_policy_net(\"./save_model/run13/good_Run13_StdDQN_750K_frames_imprvdBatching_2477_eps.pth\")\n",
    "# agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "# agent.epsilon = 0.1\n",
    "\n",
    "# frame = 0   \n",
    "# evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "# rewards, episodes = [], []\n",
    "# best_eval_reward = 0\n",
    "# exit_flag = False\n",
    "\n",
    "# for e in range(EPISODES):\n",
    "#     done = False\n",
    "#     score = 0\n",
    "\n",
    "#     history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "#     step = 0\n",
    "#     state, _ = env.reset()\n",
    "#     next_state = state\n",
    "#     life = number_lives\n",
    "#     fire_ready = True\n",
    "\n",
    "#     get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "#     while not done:\n",
    "#         step += 1\n",
    "#         frame += 1\n",
    "\n",
    "#         # Selet Action (with robust check for FIRE action)\n",
    "#         if fire_ready:\n",
    "#             next_state, force_done = reset_after_life_loss(env, history)\n",
    "#             if force_done:\n",
    "#                 break\n",
    "#             action = 1\n",
    "#             fire_ready = False\n",
    "#         else:\n",
    "#             trainable_action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "#             action = TRAINABLE_ACTIONS[trainable_action]\n",
    "\n",
    "#         state = next_state\n",
    "#         next_state, reward, terminations, truncations, info = env.step(action)  \n",
    "#         done = truncations or terminations\n",
    "#         frame_next_state = get_frame(next_state)\n",
    "        \n",
    "\n",
    "#         history[4, :, :] = frame_next_state\n",
    "#         lost_life = check_live(life, info['lives'])  #check if the agent has lost a life\n",
    "\n",
    "#         if lost_life:\n",
    "#             fire_ready = True\n",
    "\n",
    "#         life = info['lives']\n",
    "#         r = reward\n",
    "\n",
    "#         # Store the transition in memory if it was not a FIRE action\n",
    "#         if action in TRAINABLE_ACTIONS:\n",
    "#             trainable_index = TRAINABLE_ACTIONS.index(action)\n",
    "#             term_state = done or lost_life\n",
    "#             if type(agent.memory).__name__ == \"CircularReplayMemoryPER\":\n",
    "#                 agent.memory.push(agent, deepcopy(frame_next_state), trainable_index, r, term_state)\n",
    "#             else:\n",
    "#                 agent.memory.push(deepcopy(frame_next_state), trainable_index, r, term_state)\n",
    "#         # When replay buffer is filled save to pickle and break\n",
    "#         if len(agent.memory) == train_frame:\n",
    "#             print(f\"Memory filled, saving pickle file\") \n",
    "#             agent.save_replay_buffer(mem_name, frame)\n",
    "#             exit_flag = True\n",
    "#             break \n",
    "#         score += reward\n",
    "#         history[:4, :, :] = history[1:, :, :]  # shift history by one erasing oldest frame\n",
    "\n",
    "#         if frame % 500 == 0:\n",
    "#             print(\"DEBUG: len(valid_flags), len(valid_indices), len(td_errors):\", sum(agent.memory.valid_flags), len(agent.memory.valid_indices), len(agent.memory.td_errors))\n",
    "\n",
    "#         if done:\n",
    "#             fire_ready = True\n",
    "#             evaluation_reward.append(score)\n",
    "#             rewards.append(np.mean(evaluation_reward))  # record moving average of last evaluation_reward_length episodes\n",
    "#             episodes.append(e)\n",
    "\n",
    "#             # print episode information \n",
    "#             if e % 1 == 0:\n",
    "#                 print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "#                   len(agent.memory), \"  epsilon:\", agent.epsilon, \"  steps:\", step,\n",
    "#                   \"  lr:\", agent.optimizer.param_groups[0]['lr'], \n",
    "#                   \"  evaluation reward:\", np.mean(evaluation_reward))\n",
    "#     if exit_flag:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start logging\n",
    "# log_file = \"./test_log.txt\"\n",
    "# tee = TeeLogger(log_file)\n",
    "# sys.stdout = tee\n",
    "# sys.stderr = tee\n",
    "\n",
    "# print(f\"Logging started. Output will be written to both notebook and {log_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stop logging safely\n",
    "# sys.stdout = tee.original_stdout\n",
    "# sys.stderr = tee.original_stdout\n",
    "# tee.close()\n",
    "\n",
    "# print(\"This goes only to the notebook now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup Run Name and Memory Paths ##\n",
    "mem_path = None\n",
    "mem_path = './checkpoints/CircularPERBuffer_for_testing_using_Run13_orig_get_frame_50498_replay_buffer.pkl'\n",
    "# mem_path = './checkpoints/Run1_DQN_Serial_20pctExplore_and_Sticky_100000_replay_buffer.pkl'\n",
    "# mem_path = './checkpoints/Buffer_for_testing_using_Run5_50K_replay_buffer.pkl'\n",
    "# mem_path = './checkpoints/Buffer_for_testing_using_Run13_new_get_frame_100974_replay_buffer.pkl'\n",
    "seed = 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dir = f\"./logs/run{run_num}\"\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# log_file = os.path.join(log_dir, f\"{run_name}_output.log\")\n",
    "# tee = TeeLogger(log_file)\n",
    "# sys.stdout = tee\n",
    "# sys.stderr = tee\n",
    "\n",
    "# print(f\"Logging started. Output will be written to both notebook and {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rbisker/miniconda3/envs/wsl_gym/lib/python3.12/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /mnt/c/Users/rbisk/Dropbox/GMU/cs747 Deep Learning/Final_Project/Illinois_hw/videos/run18 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/tmp/ipykernel_77833/4223516090.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('./checkpoints/Run17_StdDQN_750Kfr_newest_get_frame_CirularPER_wIS_2000_checkpoint.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run Run18_Bootstrapped_run17_stickyactions5percent\n",
      "Instantiating agent\n",
      "Replay buffer loaded from './checkpoints/Run17_StdDQN_750Kfr_newest_get_frame_CirularPER_wIS_159356_replay_buffer.pkl' with size 204356\n",
      "Updated memory capacity to  1000000\n",
      "epsilon max:  1.0 epsilon min:  0.1 epsilon_decay:  3e-06\n",
      "[SANITIZER] Fixing 0 invalid or nonpositive TD-errors...\n",
      "Starting training\n",
      "episode: 1   frame: 120   proc time: 12.0s   score: 0.0   memory length: 204471   epsilon: 0.99964   steps: 120 steps/sec: 9.98   lr: 0.0005   PER_beta: 0.4001 PER Alpha: 0.5999   reward MA: 0.0   mean loss: 0   mean max Q: 8.0839\n",
      "episode: 2   frame: 315   proc time: 14.9s   score: 2.0   memory length: 204661   epsilon: 0.99906   steps: 195 steps/sec: 13.05   lr: 0.0005   PER_beta: 0.40025 PER Alpha: 0.59975   reward MA: 1.0   mean loss: 0.0384   mean max Q: 8.3999\n",
      "episode: 3   frame: 435   proc time: 9.8s   score: 0.0   memory length: 204776   epsilon: 0.9987   steps: 120 steps/sec: 12.25   lr: 0.0005   PER_beta: 0.40035 PER Alpha: 0.59965   reward MA: 0.667   mean loss: 0.03515   mean max Q: 8.7489\n",
      "episode: 4   frame: 583   proc time: 12.2s   score: 1.0   memory length: 204919   epsilon: 0.99825   steps: 148 steps/sec: 12.18   lr: 0.0005   PER_beta: 0.40047 PER Alpha: 0.59953   reward MA: 0.75   mean loss: 0.03348   mean max Q: 8.5402\n",
      "episode: 5   frame: 843   proc time: 21.1s   score: 3.0   memory length: 205174   epsilon: 0.99747   steps: 260 steps/sec: 12.3   lr: 0.0005   PER_beta: 0.40067 PER Alpha: 0.59933   reward MA: 1.2   mean loss: 0.03318   mean max Q: 8.8466\n",
      "[PER STATS] TD-error mean: 0.06719625519782797 std: 0.13793636820081112 min: 0.0 max: 4.546875\n",
      "episode: 6   frame: 1066   proc time: 18.5s   score: 3.0   memory length: 205392   epsilon: 0.9968   steps: 223 steps/sec: 12.04   lr: 0.0005   PER_beta: 0.40085 PER Alpha: 0.59915   reward MA: 1.5   mean loss: 0.03405   mean max Q: 8.8364\n",
      "episode: 7   frame: 1186   proc time: 10.0s   score: 0.0   memory length: 205507   epsilon: 0.99644   steps: 120 steps/sec: 12.0   lr: 0.0005   PER_beta: 0.40095 PER Alpha: 0.59905   reward MA: 1.286   mean loss: 0.03268   mean max Q: 8.8624\n",
      "episode: 8   frame: 1306   proc time: 10.1s   score: 0.0   memory length: 205622   epsilon: 0.99608   steps: 120 steps/sec: 11.87   lr: 0.0005   PER_beta: 0.40104 PER Alpha: 0.59896   reward MA: 1.125   mean loss: 0.02785   mean max Q: 8.7767\n",
      "episode: 9   frame: 1426   proc time: 10.2s   score: 0.0   memory length: 205737   epsilon: 0.99572   steps: 120 steps/sec: 11.79   lr: 0.0005   PER_beta: 0.40114 PER Alpha: 0.59886   reward MA: 1.0   mean loss: 0.02809   mean max Q: 9.0351\n",
      "episode: 10   frame: 1592   proc time: 14.2s   score: 1.0   memory length: 205898   epsilon: 0.99522   steps: 166 steps/sec: 11.65   lr: 0.0005   PER_beta: 0.40127 PER Alpha: 0.59873   reward MA: 1.0   mean loss: 0.02821   mean max Q: 8.7455\n",
      "episode: 11   frame: 1758   proc time: 14.4s   score: 1.0   memory length: 206059   epsilon: 0.99473   steps: 166 steps/sec: 11.5   lr: 0.0005   PER_beta: 0.40141 PER Alpha: 0.59859   reward MA: 1.0   mean loss: 0.02901   mean max Q: 8.6357\n",
      "episode: 12   frame: 1973   proc time: 18.7s   score: 2.0   memory length: 206269   epsilon: 0.99408   steps: 215 steps/sec: 11.51   lr: 0.0005   PER_beta: 0.40158 PER Alpha: 0.59842   reward MA: 1.083   mean loss: 0.02823   mean max Q: 8.7224\n",
      "[PER STATS] TD-error mean: 0.06813350854822693 std: 0.13373336834561134 min: 0.0 max: 3.544921875\n",
      "episode: 13   frame: 2167   proc time: 17.2s   score: 2.0   memory length: 206458   epsilon: 0.9935   steps: 194 steps/sec: 11.26   lr: 0.0005   PER_beta: 0.40173 PER Alpha: 0.59827   reward MA: 1.154   mean loss: 0.02824   mean max Q: 8.5988\n",
      "episode: 14   frame: 2315   proc time: 13.3s   score: 1.0   memory length: 206601   epsilon: 0.99306   steps: 148 steps/sec: 11.12   lr: 0.0005   PER_beta: 0.40185 PER Alpha: 0.59815   reward MA: 1.143   mean loss: 0.02781   mean max Q: 8.7739\n",
      "episode: 15   frame: 2435   proc time: 10.7s   score: 0.0   memory length: 206716   epsilon: 0.9927   steps: 120 steps/sec: 11.2   lr: 0.0005   PER_beta: 0.40195 PER Alpha: 0.59805   reward MA: 1.067   mean loss: 0.02612   mean max Q: 8.4231\n",
      "episode: 16   frame: 2583   proc time: 13.3s   score: 1.0   memory length: 206859   epsilon: 0.99225   steps: 148 steps/sec: 11.16   lr: 0.0005   PER_beta: 0.40207 PER Alpha: 0.59793   reward MA: 1.062   mean loss: 0.02898   mean max Q: 8.4142\n",
      "episode: 17   frame: 2703   proc time: 10.8s   score: 0.0   memory length: 206974   epsilon: 0.99189   steps: 120 steps/sec: 11.11   lr: 0.0005   PER_beta: 0.40216 PER Alpha: 0.59784   reward MA: 1.0   mean loss: 0.02669   mean max Q: 8.7332\n",
      "episode: 18   frame: 2851   proc time: 14.8s   score: 1.0   memory length: 207117   epsilon: 0.99145   steps: 148 steps/sec: 10.01   lr: 0.0005   PER_beta: 0.40228 PER Alpha: 0.59772   reward MA: 1.0   mean loss: 0.02606   mean max Q: 8.6542\n",
      "episode: 19   frame: 2971   proc time: 11.4s   score: 0.0   memory length: 207232   epsilon: 0.99109   steps: 120 steps/sec: 10.56   lr: 0.0005   PER_beta: 0.40238 PER Alpha: 0.59762   reward MA: 0.947   mean loss: 0.02552   mean max Q: 8.7595\n",
      "[PER STATS] TD-error mean: 0.06870985596530504 std: 0.13048656940537554 min: 0.0 max: 2.734375\n",
      "episode: 20   frame: 3137   proc time: 16.1s   score: 1.0   memory length: 207393   epsilon: 0.99059   steps: 166 steps/sec: 10.28   lr: 0.0005   PER_beta: 0.40251 PER Alpha: 0.59749   reward MA: 0.95   mean loss: 0.02455   mean max Q: 8.5849\n",
      "episode: 21   frame: 3257   proc time: 11.4s   score: 0.0   memory length: 207508   epsilon: 0.99023   steps: 120 steps/sec: 10.55   lr: 0.0005   PER_beta: 0.40261 PER Alpha: 0.59739   reward MA: 0.905   mean loss: 0.02747   mean max Q: 9.2324\n",
      "episode: 22   frame: 3377   proc time: 11.5s   score: 0.0   memory length: 207623   epsilon: 0.98987   steps: 120 steps/sec: 10.47   lr: 0.0005   PER_beta: 0.4027 PER Alpha: 0.5973   reward MA: 0.864   mean loss: 0.02603   mean max Q: 8.4535\n",
      "episode: 23   frame: 3525   proc time: 14.0s   score: 1.0   memory length: 207766   epsilon: 0.98943   steps: 148 steps/sec: 10.54   lr: 0.0005   PER_beta: 0.40282 PER Alpha: 0.59718   reward MA: 0.87   mean loss: 0.026   mean max Q: 8.8153\n",
      "episode: 24   frame: 3739   proc time: 20.7s   score: 2.0   memory length: 207975   epsilon: 0.98878   steps: 214 steps/sec: 10.32   lr: 0.0005   PER_beta: 0.40299 PER Alpha: 0.59701   reward MA: 0.917   mean loss: 0.02324   mean max Q: 8.704\n",
      "episode: 25   frame: 3962   proc time: 23.2s   score: 3.0   memory length: 208193   epsilon: 0.98811   steps: 223 steps/sec: 9.61   lr: 0.0005   PER_beta: 0.40317 PER Alpha: 0.59683   reward MA: 1.0   mean loss: 0.0237   mean max Q: 8.3137\n",
      "[SAVED PLOT] ./save_graph/run18/Run18_Bootstrapped_run17_stickyactions5percent_ep25_Qstats.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PER STATS] TD-error mean: 0.06832158233088685 std: 0.127618347999232 min: 0.0 max: 3.26953125\n",
      "episode: 26   frame: 4110   proc time: 14.9s   score: 1.0   memory length: 208336   epsilon: 0.98767   steps: 148 steps/sec: 9.95   lr: 0.0005   PER_beta: 0.40329 PER Alpha: 0.59671   reward MA: 1.0   mean loss: 0.02332   mean max Q: 8.3417\n",
      "episode: 27   frame: 4306   proc time: 18.7s   score: 2.0   memory length: 208527   epsilon: 0.98708   steps: 196 steps/sec: 10.47   lr: 0.0005   PER_beta: 0.40344 PER Alpha: 0.59656   reward MA: 1.037   mean loss: 0.02216   mean max Q: 8.6844\n",
      "episode: 28   frame: 4454   proc time: 14.2s   score: 1.0   memory length: 208670   epsilon: 0.98664   steps: 148 steps/sec: 10.46   lr: 0.0005   PER_beta: 0.40356 PER Alpha: 0.59644   reward MA: 1.036   mean loss: 0.02297   mean max Q: 8.552\n",
      "episode: 29   frame: 4620   proc time: 16.0s   score: 1.0   memory length: 208831   epsilon: 0.98614   steps: 166 steps/sec: 10.4   lr: 0.0005   PER_beta: 0.4037 PER Alpha: 0.5963   reward MA: 1.034   mean loss: 0.02389   mean max Q: 8.4969\n",
      "episode: 30   frame: 4740   proc time: 12.0s   score: 0.0   memory length: 208946   epsilon: 0.98578   steps: 120 steps/sec: 9.97   lr: 0.0005   PER_beta: 0.40379 PER Alpha: 0.59621   reward MA: 1.0   mean loss: 0.02442   mean max Q: 8.4967\n",
      "episode: 31   frame: 4860   proc time: 12.7s   score: 0.0   memory length: 209061   epsilon: 0.98542   steps: 120 steps/sec: 9.48   lr: 0.0005   PER_beta: 0.40389 PER Alpha: 0.59611   reward MA: 0.968   mean loss: 0.02135   mean max Q: 8.5643\n",
      "episode: 32   frame: 4980   proc time: 12.8s   score: 0.0   memory length: 209176   epsilon: 0.98506   steps: 120 steps/sec: 9.4   lr: 0.0005   PER_beta: 0.40398 PER Alpha: 0.59602   reward MA: 0.938   mean loss: 0.02477   mean max Q: 8.5273\n",
      "Target network updated at frame:  5000\n",
      "[PER STATS] TD-error mean: 0.06812057456927899 std: 0.12576851644242693 min: 0.0 max: 3.671875\n",
      "episode: 33   frame: 5157   proc time: 18.5s   score: 2.0   memory length: 209348   epsilon: 0.98453   steps: 177 steps/sec: 9.56   lr: 0.0005   PER_beta: 0.40413 PER Alpha: 0.59587   reward MA: 0.97   mean loss: 0.02187   mean max Q: 8.5731\n",
      "episode: 34   frame: 5277   proc time: 11.8s   score: 0.0   memory length: 209463   epsilon: 0.98417   steps: 120 steps/sec: 10.16   lr: 0.0005   PER_beta: 0.40422 PER Alpha: 0.59578   reward MA: 0.941   mean loss: 0.02376   mean max Q: 8.799\n",
      "episode: 35   frame: 5397   proc time: 11.6s   score: 0.0   memory length: 209578   epsilon: 0.98381   steps: 120 steps/sec: 10.38   lr: 0.0005   PER_beta: 0.40432 PER Alpha: 0.59568   reward MA: 0.914   mean loss: 0.02322   mean max Q: 8.8017\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 223\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m(training_started_flg): \n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m frame % train_interval == \u001b[32m0\u001b[39m: \u001b[38;5;66;03m# Use adaptive training interval\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m         loss, q_stats = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_policy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m         episode_losses.append(loss)\n\u001b[32m    225\u001b[39m         episode_q_means.append(q_stats[\u001b[33m'\u001b[39m\u001b[33mq_mean\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/rbisk/Dropbox/GMU/cs747 Deep Learning/Final_Project/Illinois_hw/agent.py:117\u001b[39m, in \u001b[36mAgent.train_policy_net\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    115\u001b[39m history = np.stack(mini_batch[\u001b[32m0\u001b[39m], axis=\u001b[32m0\u001b[39m)\n\u001b[32m    116\u001b[39m states = np.float16(history[:, :HISTORY_SIZE, :, :]) / \u001b[32m255.\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m states = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m.cuda()\n\u001b[32m    118\u001b[39m actions = \u001b[38;5;28mlist\u001b[39m(mini_batch[\u001b[32m1\u001b[39m])\n\u001b[32m    119\u001b[39m actions = torch.LongTensor(actions).cuda()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "import time\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "set_seed(seed)\n",
    "run_num = \"18\"\n",
    "name = \"Bootstrapped_run17_stickyactions5percent\"\n",
    "run_name = \"Run\"+ str(run_num) + \"_\" + name\n",
    "\n",
    "\n",
    "from config import *\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# choose which Agent to use\n",
    "from agent import Agent\n",
    "\n",
    "#create fresh environment\n",
    "env = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=sticky_action_prob, full_action_space=False, render_mode='rgb_array')  # Use equivalent parameters to BreakoutDeterministic-v4\n",
    "\n",
    "# setup video recording\n",
    "def video_trigger(_):\n",
    "    if len(episodes) == 0:\n",
    "        return False\n",
    "    else:\n",
    "        curr_ep = episodes[-1]\n",
    "        return (curr_ep > 99 and curr_ep % 100 == 0) \n",
    "\n",
    "video_path = f\"./videos/run{run_num}\"\n",
    "if not os.path.exists(video_path):\n",
    "    os.makedirs(video_path)\n",
    "env = RecordVideo(env, video_folder=video_path, episode_trigger=video_trigger)\n",
    "\n",
    "\n",
    "print(f\"Starting run {run_name}\")\n",
    "train_interval = 1\n",
    "    \n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "rewards, episodes = [], []\n",
    "reward = 0\n",
    "best_eval_reward = curr_mean_reward = last_save_reward = 0\n",
    "\n",
    "#initialize trackers\n",
    "## LOSS TRACKERS ##\n",
    "losses_window = deque(maxlen=10)\n",
    "loss_history = []\n",
    "episode_losses = []\n",
    "loss = mean_loss = 0\n",
    "\n",
    "# ## Q-VALUE TRACKERS ##\n",
    "episode_q_means = []\n",
    "episode_q_maxs = []\n",
    "episode_q_mins = []\n",
    "q_mean_window = deque(maxlen=10)\n",
    "q_max_window = deque(maxlen=10)\n",
    "q_min_window = deque(maxlen=10)\n",
    "q_stats_history = {\n",
    "    'mean': [],\n",
    "    'max': [],\n",
    "    'min': [],\n",
    "    'episode': []\n",
    "}\n",
    "\n",
    "\n",
    "# Epsilon Bump Control Variables\n",
    "plateau_patience = 300\n",
    "episodes_since_improvement = 0\n",
    "epsilon_bump = 0.15  # amount to re-increase epsilon\n",
    "soonest_bump = 2000  # earliest episode to apply epsilon bump\n",
    "\n",
    "frame = 0\n",
    "ep_start = 0\n",
    "training_started_flg = False\n",
    "\n",
    "print(\"Instantiating agent\")\n",
    "\n",
    "### Bootstrapped loading run 17 ###\n",
    "mem_path = './checkpoints/Run17_StdDQN_750Kfr_newest_get_frame_CirularPER_wIS_159356_replay_buffer.pkl'\n",
    "checkpoint = torch.load('./checkpoints/Run17_StdDQN_750Kfr_newest_get_frame_CirularPER_wIS_2000_checkpoint.pt')\n",
    "agent = Agent(action_size, mem_path)  #use when starting with prefilled replay buffer\n",
    "\n",
    "# extend the loaded mem buffer capacity\n",
    "new_capacity = 1_000_000\n",
    "agent.memory.capacity = new_capacity\n",
    "agent.memory.memory.extend([None] * (new_capacity - len(agent.memory.memory)))\n",
    "agent.memory.valid_flags.extend([False] * (new_capacity - len(agent.memory.valid_flags)))\n",
    "assert len(agent.memory.memory) == len(agent.memory.valid_flags) == agent.memory.capacity == new_capacity\n",
    "assert sum(agent.memory.valid_flags) == len(agent.memory.valid_indices) == len(agent.memory.td_errors)\n",
    "print(\"Updated memory capacity to \", agent.memory.capacity)\n",
    "\n",
    "# Verify correct episilon\n",
    "print(\"epsilon max: \", agent.epsilon_max, \"epsilon min: \", agent.epsilon_min, \"epsilon_decay: \", agent.epsilon_decay)\n",
    "\n",
    "td_errors_np = np.array(agent.memory.td_errors, dtype=np.float32)\n",
    "\n",
    "# Print memory stats before\n",
    "num_invalid = np.sum(~np.isfinite(td_errors_np)) + np.sum(td_errors_np <= 0)\n",
    "print(f\"[SANITIZER] Fixing {num_invalid} invalid or nonpositive TD-errors...\")\n",
    "# Fix all issues\n",
    "td_errors_np = np.where(~np.isfinite(td_errors_np) | (td_errors_np <= 0), 1.0, td_errors_np)\n",
    "agent.memory._priority_cache_dirty = True\n",
    "# Convert back to list\n",
    "agent.memory.td_errors = td_errors_np.tolist()\n",
    "\n",
    "#load prior model weights\n",
    "agent.policy_net.load_state_dict(checkpoint['policy_net']) # load partially trained policy network weights\n",
    "agent.policy_net = torch.compile(agent.policy_net)\n",
    "print(agent.policy_net)\n",
    "print(\"Compiled:\", isinstance(agent.policy_net, torch._dynamo.eval_frame.OptimizedModule))\n",
    "agent.target_net.load_state_dict(checkpoint['target_net'])\n",
    "\n",
    "agent.beta = IS_BETA\n",
    "agent.alpha = PER_ALPHA\n",
    "\n",
    "### END BOOTSTRAPPING ####\n",
    "\n",
    "# agent = Agent(action_size, mem_path)  \n",
    "# agent = Agent(action_size)\n",
    "\n",
    "#########################\n",
    "#### LOAD CHECKPOINT ####\n",
    "# metadata = agent.load_checkpoint(\"Run8_Stdized_DDQN_750K_frames\", 2999)  #Edit episode number\n",
    "# frame = metadata['global_frame']\n",
    "# agent.load_replay_buffer(\"Run8_Stdized_DDQN_750K_frames\", 711354)\n",
    "# ep_start = metadata['global_episode']\n",
    "# evaluation_reward = metadata['eval_rewards']\n",
    "# rewards = metadata['rewards']    \n",
    "# episodes = metadata['episodes']\n",
    "# losses_window = metadata['last_10_ep_losses']\n",
    "# loss_history = metadata['loss_tracker']\n",
    "# training_started_flg = True\n",
    "########################\n",
    "\n",
    "\n",
    "start_train_immediate = True\n",
    "frame_max = TRAINING_STEPS\n",
    "e = ep_start\n",
    "\n",
    "\n",
    "while e < EPISODES:\n",
    "    #limit number of frames for consistent testing\n",
    "    if frame >= frame_max:\n",
    "        break\n",
    "    ep_start_time = time.time()\n",
    "    done = False\n",
    "    score = 0\n",
    "    episode_losses = []\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state, _ = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "    fire_ready = True\n",
    "    no_reward_steps = 0\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        #limit number of frames for consitent testing\n",
    "        if frame >= frame_max:\n",
    "            break\n",
    "\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Selet Action (with robust check for FIRE action)\n",
    "        if fire_ready:  \n",
    "            next_state, force_done = reset_after_life_loss(env, history)\n",
    "            if force_done:\n",
    "                break\n",
    "            action = 1\n",
    "            fire_ready = False\n",
    "        else:\n",
    "            trainable_action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "            action = TRAINABLE_ACTIONS[trainable_action]\n",
    "\n",
    "        # Step the environment\n",
    "        state = next_state\n",
    "        next_state, reward, terminations, truncations, info = env.step(action)  \n",
    "        done = truncations or terminations\n",
    "        \n",
    "        # Failsafe to force reset if no reward for 3000 steps (prevents agent from getting stuck)\n",
    "        stuck_limit = 3000\n",
    "        if no_reward_steps > stuck_limit:\n",
    "            done = True\n",
    "            print(f\"[WARNING] No reward for {stuck_limit} steps, forcing reset | \", \"Episode:\", e, \"  Frame:\", frame, ) \n",
    "             \n",
    "        frame_next_state = get_frame(next_state)\n",
    "             \n",
    "        # append next state to history\n",
    "        history[4, :, :] = frame_next_state\n",
    "        \n",
    "        # life handling\n",
    "        lost_life = check_live(life, info['lives'])  #check if the agent has lost a life\n",
    "        if lost_life:\n",
    "            fire_ready = True\n",
    "        life = info['lives']\n",
    "        \n",
    "        r = reward\n",
    "        if r == 0:\n",
    "            no_reward_steps += 1\n",
    "        else:\n",
    "            no_reward_steps = 0 \n",
    "\n",
    "        # Store the transition in replay buffer if it was not a FIRE action\n",
    "        \n",
    "        if action in TRAINABLE_ACTIONS:\n",
    "            trainable_index = TRAINABLE_ACTIONS.index(action)\n",
    "            term_state = done or lost_life\n",
    "            if type(agent.memory).__name__ == \"CircularReplayMemoryPER\":\n",
    "                # print(\"[DEBUG] Using CircularReplayMemoeryPER.push()\")\n",
    "                # agent.memory.push(agent, deepcopy(frame_next_state), trainable_index, r, term_state)  # for use when model is used to estimate new TD-errors\n",
    "                mean_td = np.mean(agent.recent_td_errors) if len(agent.recent_td_errors) > 200 else 1.0\n",
    "                agent.memory.push(frame_next_state.copy(), trainable_index, r, term_state, mean_recent_td_error=mean_td)\n",
    "            else:\n",
    "                agent.memory.push(frame_next_state.copy(), trainable_index, r, term_state)\n",
    "        \n",
    "        # Start training after random sample generation\n",
    "        if training_started_flg == False and (frame == train_frame or (start_train_immediate and frame == 1)):\n",
    "            print(\"Starting training\")\n",
    "            training_started_flg = True\n",
    "            e = ep_start  #reset episode counter when training starts\n",
    "        if(training_started_flg): \n",
    "            if frame % train_interval == 0: # Use adaptive training interval\n",
    "                loss, q_stats = agent.train_policy_net()\n",
    "                episode_losses.append(loss)\n",
    "                episode_q_means.append(q_stats['q_mean'])\n",
    "                episode_q_maxs.append(q_stats['q_max'])\n",
    "                episode_q_mins.append(q_stats['q_min'])\n",
    "            # Update the target network\n",
    "            if (frame % (train_interval * update_target_network_frequency)) == 0:\n",
    "                agent.update_target_net()\n",
    "                print(\"Target network updated at frame: \", frame)\n",
    "        \n",
    "        # Update score and history\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]  # shift history by one erasing oldest frame\n",
    "\n",
    "        ## DEBUG ##\n",
    "        if frame % 1000 == 0 and len(agent.memory.td_errors) > 0:\n",
    "            agent.memory.log_td_error_distribution()\n",
    "            \n",
    "        if done:\n",
    "            e += 1\n",
    "            fire_ready = True\n",
    "            evaluation_reward.append(score)\n",
    "            ep_duration = time.time() - ep_start_time            \n",
    "            \n",
    "\n",
    "            # print episode information every X episodes\n",
    "            if e % 1 == 0:\n",
    "                print(\"episode:\", e, \"  frame:\", frame, \"  proc time:\", str(round(ep_duration, 1)) +'s', \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", round(agent.epsilon, 5), \n",
    "                  \"  steps:\", step,  \"steps/sec:\", round(step/ep_duration, 2), \"  lr:\", agent.optimizer.param_groups[0]['lr'], \"  PER_beta:\", round(agent.beta,5), \"PER Alpha:\", round(agent.alpha,5),\n",
    "                  \"  reward MA:\", round(np.mean(evaluation_reward), 3), \n",
    "                  \"  mean loss:\", round(mean_loss, 5), \"  mean max Q:\", round(np.mean(episode_q_maxs), 4))\n",
    "                #   \"  latest step Q max:\", round(q_stats['q_max'], 4))\n",
    "\n",
    "\n",
    "            \n",
    "            if training_started_flg:\n",
    "\n",
    "\n",
    "\n",
    "                episodes.append(e)\n",
    "                rewards.append(np.mean(evaluation_reward))  # record moving average of last evaluation_reward_length episodes\n",
    "\n",
    "                # # adapt training interval to agent performance\n",
    "                # if np.mean(evaluation_reward) < 8:\n",
    "                #     train_interval = 4\n",
    "                # elif np.mean(evaluation_reward) < 15:\n",
    "                #     train_interval = 2\n",
    "                # else:\n",
    "                #     train_interval = 1\n",
    "\n",
    "                ## DEBUG ##\n",
    "                # Check TD-error distribution in Replay Buffer\n",
    "                if e>0 and e % 100 == 0:\n",
    "                    agent.memory.log_td_error_distribution()\n",
    "\n",
    "                # save rolling loss everages every X episodes\n",
    "                if episode_losses:\n",
    "                    mean_loss = sum(episode_losses) / len(episode_losses)\n",
    "                    losses_window.append(mean_loss)\n",
    "                    if e > 9 and e % 10 == 0:\n",
    "                        loss_history.append((np.mean(losses_window), e))\n",
    "\n",
    "                # save rolling Q-score stat averages\n",
    "                q_mean_window.append(np.mean(episode_q_means))\n",
    "                q_max_window.append(np.mean(episode_q_maxs))\n",
    "                q_min_window.append(np.mean(episode_q_mins))\n",
    "                q_stats_history['mean'].append(np.mean(q_mean_window))\n",
    "                q_stats_history['max'].append(np.mean(q_max_window))\n",
    "                q_stats_history['min'].append(np.mean(q_min_window))\n",
    "                q_stats_history['episode'].append(e)\n",
    "                episode_q_means = []\n",
    "                episode_q_maxs = []\n",
    "                episode_q_mins = []\n",
    "\n",
    "                # plot the rewards every X episodes\n",
    "                if e > 0 and e % 50 == 0:\n",
    "                    # clear_output(wait=True)\n",
    "                    pylab.plot(episodes, rewards, 'b')\n",
    "                    pylab.xlabel('Episode #')\n",
    "                    pylab.ylabel('Rolling Mean Episode Scores') \n",
    "                    pylab.title('DQN w PER\\n Scores')\n",
    "                    plot_path = f\"./save_graph/run{run_num}/{run_name}_ep{e}_scores.png\"\n",
    "                    os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "                    pylab.savefig(plot_path)\n",
    "                    print(f\"[SAVED PLOT] {plot_path}\")\n",
    "                    pylab.show()\n",
    "                    pylab.close()\n",
    "                \n",
    "                # every X episodes, plot the mean losses\n",
    "                if e > 0 and e % 50 == 0:\n",
    "                    x = [entry[1] for entry in loss_history]\n",
    "                    y = [entry[0] for entry in loss_history]\n",
    "                    pylab.plot(x, y, 'r')\n",
    "                    pylab.xlabel('Episode #')\n",
    "                    pylab.ylabel('Rolling Mean Loss per Episode') \n",
    "                    pylab.title('DQN w PER\\n Loss')\n",
    "                    plot_path = f\"./save_graph/run{run_num}/{run_name}_ep{e}_losses.png\"\n",
    "                    os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "                    pylab.savefig(plot_path)\n",
    "                    print(f\"[SAVED PLOT] {plot_path}\")\n",
    "                    pylab.show()\n",
    "                    pylab.close()\n",
    "\n",
    "                # plot Q-value statistics every X episodes\n",
    "                if e > 0 and e % 25 == 0:\n",
    "                    pylab.plot(q_stats_history['episode'], q_stats_history['mean'], label='Q-Mean')\n",
    "                    pylab.plot(q_stats_history['episode'], q_stats_history['max'], label='Q-Max')\n",
    "                    pylab.plot(q_stats_history['episode'], q_stats_history['min'], label='Q-Min')\n",
    "                    pylab.xlabel('Episode #')\n",
    "                    pylab.ylabel('Q-Value')\n",
    "                    pylab.title('Avg per Episode Q-Value Stats')\n",
    "                    pylab.legend(loc='upper left')\n",
    "                    plot_path = f\"./save_graph/run{run_num}/{run_name}_ep{e}_Qstats.png\"\n",
    "                    os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "                    pylab.savefig(plot_path)\n",
    "                    print(f\"[SAVED PLOT] {plot_path}\")\n",
    "                    pylab.show()\n",
    "                    pylab.close()\n",
    "\n",
    "\n",
    "                    ## DEBUG ##\n",
    "                    # print(f\"[PLOT DEBUG] Last 5 q_means: {q_stats_history['mean'][-5:]}\")\n",
    "\n",
    "                # Checkpoint the training process every X episodes \n",
    "                if e > 0 and e % 100 == 0:\n",
    "                    metadata = create_metadata(agent, e, frame, evaluation_reward, rewards, episodes, losses_window, loss_history, q_stats_history, q_mean_window, \\\n",
    "                                               q_max_window, q_min_window, agent.epsilon, alpha=agent.alpha, beta=agent.beta)\n",
    "                    agent.save_checkpoint(metadata, run_name, e)\n",
    "                if e > 0 and e % 200 == 0:\n",
    "                    agent.save_replay_buffer(run_name, frame)\n",
    "\n",
    "                # Check if reward has improved\n",
    "                curr_mean_reward = np.mean(evaluation_reward)\n",
    "                if curr_mean_reward > best_eval_reward:\n",
    "                    best_eval_reward = curr_mean_reward\n",
    "                    episodes_since_improvement = 0\n",
    "                else:\n",
    "                    episodes_since_improvement += 1\n",
    "                \n",
    "                # save model if it is good\n",
    "                if curr_mean_reward > 8 and curr_mean_reward > (1.05 * last_save_reward):\n",
    "                    model_path = f\"./save_model/run{run_num}/good_{run_name}_{e}_eps.pth\"\n",
    "                    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "                    torch.save(agent.policy_net.state_dict(), model_path)\n",
    "                    print(f\"[SAVED MODEL] {model_path}\")\n",
    "                    last_save_reward = curr_mean_reward\n",
    "               \n",
    "                # # # Apply epsilon bump if plateauing\n",
    "                # if e > soonest_bump and episodes_since_improvement >= plateau_patience:\n",
    "                #     if agent.epsilon < agent.epsilon_max:\n",
    "                #         agent.epsilon = min(agent.epsilon + epsilon_bump, agent.epsilon_max)\n",
    "                #         print(f\"[BUMP] Epsilon bumped to {agent.epsilon:.4f} after {plateau_patience} stagnant episodes.\")\n",
    "                #     episodes_since_improvement = 0  # Reset counter after bump\n",
    "\n",
    "\n",
    "\n",
    "# Checkpoint the model at the end of training loop\n",
    "metadata = create_metadata(agent, e, frame, evaluation_reward, rewards, episodes, losses_window, loss_history, q_stats_history, q_mean_window, \\\n",
    "                            q_max_window, q_min_window, agent.epsilon, alpha=agent.alpha, beta=agent.beta)\n",
    "agent.save_checkpoint(metadata, run_name, e)\n",
    "agent.save_replay_buffer(run_name, frame)\n",
    "\n",
    "print(\"Training complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End logging\n",
    "sys.stdout = tee.ipython_stdout\n",
    "tee.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Load checkpoint files ===\n",
    "ddqn_ckpt = torch.load(\"./checkpoints/Run12_DDQN_750K_frames_imprvdBatching_3007_checkpoint.pt\")\n",
    "dqn_ckpt  = torch.load(\"./checkpoints/Run13_StdDQN_750K_frames_imprvdBatching_2673_checkpoint.pt\")\n",
    "\n",
    "# === Extract metadata ===\n",
    "ddqn_meta = ddqn_ckpt[\"metadata\"]\n",
    "dqn_meta = dqn_ckpt[\"metadata\"]\n",
    "\n",
    "# # === Extract episode indices and rolling mean rewards ===\n",
    "# ddqn_episodes = ddqn_meta[\"episodes\"]\n",
    "# ddqn_rewards = ddqn_meta[\"rewards\"]\n",
    "\n",
    "# dqn_episodes = dqn_meta[\"episodes\"]\n",
    "# dqn_rewards = dqn_meta[\"rewards\"]\n",
    "\n",
    "# # === Plot ===\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(dqn_episodes, dqn_rewards, label=\"DQN\", color=\"red\")\n",
    "# plt.plot(ddqn_episodes, ddqn_rewards, label=\"DDQN\", color=\"blue\")\n",
    "# plt.xlabel(\"Episode #\", fontweight=\"bold\")\n",
    "# plt.ylabel(\"Mean Episode Score (Moving Avg)\", fontweight=\"bold\")\n",
    "# plt.legend(loc = \"upper left\")\n",
    "# # Main title\n",
    "# pylab.text(0.5, 1.05, 'Std DQN vs DDQN Scores',\n",
    "#            ha='center', va='bottom', transform=pylab.gca().transAxes,\n",
    "#            fontsize=14, fontweight='semibold')\n",
    "\n",
    "# # Subtitle (smaller font)\n",
    "# pylab.text(0.5, 1.01, '(750K training steps)',\n",
    "#            ha='center', va='bottom', transform=pylab.gca().transAxes,\n",
    "#            fontsize=10, color='gray')\n",
    "\n",
    "\n",
    "# plot_path = f\"./presentation_assets/DQN_vs_DDQN_750K_steps_SCORES.png\"\n",
    "# os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "# pylab.savefig(plot_path, bbox_inches='tight')\n",
    "# print(f\"[SAVED PLOT] {plot_path}\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # === Extract loss data ===\n",
    "ddqn_loss_history = ddqn_meta[\"loss_tracker\"]\n",
    "dqn_loss_history = dqn_meta[\"loss_tracker\"]\n",
    "\n",
    "ddqn_episodes = [entry[1] for entry in ddqn_loss_history]\n",
    "ddqn_losses = [entry[0] for entry in ddqn_loss_history]\n",
    "\n",
    "dqn_episodes = [entry[1] for entry in dqn_loss_history]\n",
    "dqn_losses = [entry[0] for entry in dqn_loss_history]\n",
    "\n",
    "# === Plot ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dqn_episodes, dqn_losses, label=\"DQN\", color=\"red\")\n",
    "plt.plot(ddqn_episodes, ddqn_losses, label=\"DDQN\", color=\"blue\")\n",
    "plt.xlabel(\"Episode #\", fontweight=\"bold\")\n",
    "plt.ylabel(\"Mean Episode Loss (Moving Avg)\", fontweight=\"bold\")\n",
    "plt.legend(loc = \"best\")\n",
    "# Main title\n",
    "pylab.text(0.5, 1.05, 'Std DQN vs DDQN Huber Loss',\n",
    "           ha='center', va='bottom', transform=pylab.gca().transAxes,\n",
    "           fontsize=14, fontweight='semibold')\n",
    "\n",
    "# Subtitle (smaller font)\n",
    "pylab.text(0.5, 1.01, '(750K training steps)',\n",
    "           ha='center', va='bottom', transform=pylab.gca().transAxes,\n",
    "           fontsize=10, color='gray')\n",
    "\n",
    "\n",
    "plot_path = f\"./presentation_assets/DQN_vs_DDQN_750K_steps_LOSSES.png\"\n",
    "os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "pylab.savefig(plot_path, bbox_inches='tight')\n",
    "print(f\"[SAVED PLOT] {plot_path}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chkpt = torch.load(\"./checkpoints/Run12_DDQN_750K_frames_imprvdBatching_3007_checkpoint.pt\")\n",
    "from model import DQN_DualBranch\n",
    "action_size = len(TRAINABLE_ACTIONS)\n",
    "model = DQN_DualBranch(action_size)\n",
    "model.eval()\n",
    "\n",
    "#create dummy input for model\n",
    "dummy_input = torch.rand(BATCH_SIZE, 6, 84, 84)\n",
    "\n",
    "#export to ONNX\n",
    "torch.onnx.export(model, dummy_input, \"Branched Network.onnx\",\n",
    "                  input_names=[\"input\"], output_names=[\"output\"],\n",
    "                  dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "                  opset_version=11)\n",
    "\n",
    "print(\"ONNX model exported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timediff Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using a 4 frame history to train the model, here we will feed the model the 4 frames plus the \"diff\" between each of the 4 frames to help isolate the ball position which is the most important thing for the model to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play Game in Window or Save Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# set random seed to sync visual and recorded game\n",
    "seed = 55\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Load and setup Agent\n",
    "print(\"Instantiating agent\")\n",
    "agent = Agent(action_size)\n",
    "agent.load_policy_net(\"./save_model/run3/good_Run3_DDQN_Serial_InvTimeEpsilon_ddqn_886_eps.pth\")\n",
    "agent.policy_net.eval()\n",
    "agent.epsilon = 0  # Set agent to only exploit the best action\n",
    "\n",
    "\n",
    "# Create environments\n",
    "env_human = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=sticky_action_prob, full_action_space=False, render_mode=\"human\")\n",
    "# env_record = RecordVideo(env_record, video_folder=\"./videos\", episode_trigger=lambda e: True)\n",
    "\n",
    "# Reset to seed\n",
    "state_h, _ = env_human.reset(seed=seed)\n",
    "state_h = do_random_actions(env_human, 20)\n",
    "\n",
    "\n",
    "# Setup History\n",
    "history = np.zeros([HISTORY_SIZE + 1, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state_h, HISTORY_SIZE)\n",
    "\n",
    "done = False\n",
    "fire_ready = True\n",
    "life = number_lives\n",
    "score = 0\n",
    "step = 0\n",
    "\n",
    "while not done:\n",
    "    step += 1\n",
    "\n",
    "    # Select action\n",
    "    if fire_ready:\n",
    "        print(f\"[DEBUG] Agent is not acting  sending FIRE at step {step}\")\n",
    "        action = 1\n",
    "        fire_ready = False\n",
    "    else:\n",
    "        model_action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        action = TRAINABLE_ACTIONS[model_action]\n",
    "\n",
    "    # Step the environment\n",
    "    state_h, reward, term_h, trunc_h, info = env_human.step(action)\n",
    "    done = term_h or trunc_h\n",
    "\n",
    "    # update history\n",
    "    frame_next_state = get_frame(state_h)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    history[:4, :, :] = history[1:, :, :]  #shift history by one erasing oldest frame\n",
    "    \n",
    "    # check if life has been lost\n",
    "    lost_life = check_live(life, info['lives'])  #check if the agent has lost a life\n",
    "    if lost_life:\n",
    "        print(f\"[DEBUG] Lost life detected at step {step}\")\n",
    "        fire_ready = True\n",
    "    life = info['lives']\n",
    "    \n",
    "    # keep track of score\n",
    "    score += reward # update total score\n",
    "\n",
    "    \n",
    "env_human.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play Game Rendered in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display as ipythondisplay, clear_output\n",
    "\n",
    "def show_state_live(frame, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(frame)\n",
    "    plt.title(f\"Step: {step} {info}\")\n",
    "    plt.axis('off')\n",
    "    clear_output(wait=True)\n",
    "    ipythondisplay(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "\n",
    "# set random seed to sync visual and recorded game\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Load and setup Agent\n",
    "# Choose whether to use double DQN\n",
    "double_dqn = False # set to True if using double DQN agent\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "#Initialize agent\n",
    "print(\"Instantiating agent\")\n",
    "agent = Agent(action_size)\n",
    "agent.load_policy_net(\"./save_model/Run1_serial/good_breakout_dqn_1256_eps.pth\")\n",
    "agent.policy_net.eval()\n",
    "agent.epsilon = 0  # Set agent to only exploit the best action\n",
    "\n",
    "\n",
    "# Create environments\n",
    "env = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=sticky_action_prob, full_action_space=False, render_mode=\"rgb_array\")\n",
    "# Use RecordVideo to save the video to the \"videos\" directory\n",
    "# env = RecordVideo(env, video_folder=\"./videos\", episode_trigger=lambda episode_id: True)\n",
    "\n",
    "# Reset the environment\n",
    "state, _ = env.reset(seed=seed)\n",
    "\n",
    "# Setup History\n",
    "history = np.zeros([HISTORY_SIZE + 1, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "#Initialize variables\n",
    "step = 0\n",
    "done = False\n",
    "fire_ready = True\n",
    "life = number_lives\n",
    "score = 0\n",
    "\n",
    "while not done:\n",
    "\n",
    "    # Render the current frame live in the notebook\n",
    "    show_state_live(state, step)   \n",
    "\n",
    "    # Select action\n",
    "    if fire_ready:\n",
    "        action = 1\n",
    "        fire_ready = False\n",
    "    else:\n",
    "        model_action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        action = TRAINABLE_ACTIONS[model_action]\n",
    "\n",
    "    # Step the environment\n",
    "    next_state, reward, term, trunc, info = env.step(action)\n",
    "    done = term or trunc\n",
    "\n",
    "    # update history\n",
    "    frame_next_state = get_frame(state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    history[:4, :, :] = history[1:, :, :]  #shift history by one erasing oldest frame\n",
    "    \n",
    "    # check if life has been lost\n",
    "    lost_life = check_live(life, info['lives'])  #check if the agent has lost a life\n",
    "    if lost_life:\n",
    "        fire_ready = True  \n",
    "        # do_random_actions(env_human, 30) #IMPT: introduce randomness to game and paddle position before next life\n",
    "    life = info['lives']\n",
    "    \n",
    "    # keep track of score\n",
    "    score += reward # update total score\n",
    "\n",
    "    state = next_state\n",
    "    step += 1\n",
    "\n",
    "    \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from config import *\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "\n",
    "# Load and setup Agent\n",
    "# Choose which agent class to load\n",
    "from agent import Agent\n",
    "# from agent_timediff import Agent\n",
    "\n",
    "# 50 unique, fixed seeds\n",
    "seeds = [765,817,53,705,990,511,236,661,654,418,804,968,1,749,125,293,985,574, \n",
    "         447,948,687,317,280,645,927,842,309,616,717,930,778,323,595,798,195,11,\n",
    "         483,316,690,951,196,307,906,558,516,844,410,965,371,886]\n",
    "\n",
    "\n",
    "#Initialize agent\n",
    "print(\"Instantiating agent\")\n",
    "agent = Agent(action_size)\n",
    "# agent.load_policy_net(\"./save_model/run12/good_Run12_DDQN_750K_frames_imprvdBatching_2319_eps.pth\")\n",
    "agent.load_policy_net(\"./save_model/run13/good_Run13_StdDQN_750K_frames_imprvdBatching_2477_eps.pth\")\n",
    "# agent.load_policy_net(\"./save_model/run14/good_Run14_StdDQN_750Kfr_timediff_new_get_frame_2425_eps.pth\")\n",
    "agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "agent.target_net.eval()\n",
    "agent.policy_net.eval()\n",
    "agent.epsilon = 0.05  # Set agent to use model action 95% of the time for robustness\n",
    "\n",
    "\n",
    "# Create environments\n",
    "env = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=0.0, full_action_space=False, render_mode=\"rgb_array\")\n",
    "# Use RecordVideo to save the video to the \"videos\" directory\n",
    "# video_path = f\"./videos/testing/run13\"\n",
    "# if not os.path.exists(video_path):\n",
    "#     os.makedirs(video_path)\n",
    "# env = RecordVideo(env, video_folder=video_path, episode_trigger=lambda e: True)\n",
    "\n",
    "\n",
    "scores = []\n",
    "\n",
    "for e, seed in enumerate(seeds):\n",
    "    set_seed(seed)\n",
    "    # Reset the environment\n",
    "    state, _ = env.reset(seed=seed)\n",
    "\n",
    "    # Setup History\n",
    "    history = np.zeros([HISTORY_SIZE + 1, 84, 84], dtype=np.uint8)\n",
    "    get_init_state(history, state, HISTORY_SIZE)  #non cropped version\n",
    "    # new_get_init_state(history, state, HISTORY_SIZE)  #cropped version\n",
    "\n",
    "    #Initialize variables\n",
    "    step = 0\n",
    "    done = False\n",
    "    fire_ready = True\n",
    "    life = number_lives\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        # Selet Action (with robust check for FIRE action)\n",
    "        if fire_ready:\n",
    "            next_state, force_done = reset_after_life_loss(env, history)\n",
    "            if force_done:\n",
    "                break\n",
    "            action = 1\n",
    "            fire_ready = False\n",
    "        else:\n",
    "            trainable_action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "            action = TRAINABLE_ACTIONS[trainable_action]\n",
    "\n",
    "        # Step the environment\n",
    "    \n",
    "        next_state, reward, term, trunc, info = env.step(action)\n",
    "        done = term or trunc\n",
    "        # update total score\n",
    "        score += reward \n",
    "\n",
    "        # update history\n",
    "        history[4, :, :] = get_frame(next_state)  #non cropped version\n",
    "        # history[4, :, :] = new_get_frame(next_state)  #cropped version\n",
    "        history[:4, :, :] = history[1:, :, :]  #shift history by one erasing oldest frame\n",
    "        \n",
    "        # check if life has been lost\n",
    "        lost_life = check_live(life, info['lives'])  #check if the agent has lost a life\n",
    "        if lost_life:\n",
    "            fire_ready = True  \n",
    "        life = info['lives']\n",
    "        \n",
    "        \n",
    "        step += 1\n",
    "    \n",
    "    print(\"episode:\", e, \"  seed:\", seed, \"  score:\", score, \"  epsilon:\", round(agent.epsilon, 5), \n",
    "    \"  steps:\", step)\n",
    "    \n",
    "    scores.append(score)\n",
    "\n",
    "env.close()\n",
    "print(\"Mean Score: \", np.mean(scores))\n",
    "print(\"Std Score: \", np.std(scores))\n",
    "print(\"Max Score: \", np.max(scores))\n",
    "print(\"Min Score: \", np.min(scores))\n",
    "print(\"Median Score: \", np.median(scores))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=0.0, full_action_space=False)\n",
    "env.reset()\n",
    "next_state, reward, terminations, truncations, info = env.step(torch.tensor([[1]]))\n",
    "done = truncations or terminations\n",
    "print(\"reward: \", reward)\n",
    "print(\"done: \", done)\n",
    "print(\"info: \", info)\n",
    "print(next_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)\n",
    "print(\"Expected type:\", type(env.action_space.sample()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"agent memory type: \", type(agent.memory.memory[0]))\n",
    "print(\"agent memory[0]: \", agent.memory.memory[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu118\n",
      "11.8\n",
      "True\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "torch._inductor.config.is_triton_enabled does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\utils\\_config_module.py:143\u001b[0m, in \u001b[0;36mConfigModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;66;03m# make hasattr() work properly\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'is_triton_enabled'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_built())\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTriton available:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inductor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_triton_enabled\u001b[49m())\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\utils\\_config_module.py:146\u001b[0m, in \u001b[0;36mConfigModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config[name]\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;66;03m# make hasattr() work properly\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: torch._inductor.config.is_triton_enabled does not exist"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cuda.is_built())\n",
    "print(\"Triton available:\", torch._inductor.config.is_triton_enabled())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\compile_fx.py:150: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "backend='inductor' raised:\nRuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m84\u001b[39m, \u001b[38;5;241m84\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 23\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompile + inference worked:\u001b[39m\u001b[38;5;124m\"\u001b[39m, out\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:433\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    428\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    429\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[0;32m    430\u001b[0m )\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[0;32m    437\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[0;32m    438\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1116\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[1;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[0;32m   1110\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[0;32m   1111\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[0;32m   1112\u001b[0m             )\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[1;32m-> 1116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:948\u001b[0m, in \u001b[0;36mConvertFrame.__call__\u001b[1;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[0;32m    946\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    947\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 948\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    951\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    952\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:472\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[1;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[0;32m    458\u001b[0m compile_id \u001b[38;5;241m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[0;32m    460\u001b[0m signpost_event(\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m     },\n\u001b[0;32m    470\u001b[0m )\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_utils_internal.py:84\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m     83\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStrobelightCompileTimeProfiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofile_compile_time\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_strobelight\\compile_time_profiler.py:129\u001b[0m, in \u001b[0;36mStrobelightCompileTimeProfiler.profile_compile_time\u001b[1;34m(cls, func, phase_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprofile_compile_time\u001b[39m(\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mcls\u001b[39m, func: Any, phase_name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m--> 129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprofiler is not set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:817\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[0;32m    815\u001b[0m guarded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 817\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[0;32m    820\u001b[0m     Unsupported,\n\u001b[0;32m    821\u001b[0m     TorchRuntimeError,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    828\u001b[0m     BisectValidationException,\n\u001b[0;32m    829\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\utils.py:231\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    230\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 231\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    233\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:636\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[1;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[0;32m    634\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 636\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py:1185\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[1;34m(code, transformations, safe)\u001b[0m\n\u001b[0;32m   1182\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[0;32m   1183\u001b[0m propagate_line_nums(instructions)\n\u001b[1;32m-> 1185\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:178\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m cleanup \u001b[38;5;241m=\u001b[39m setup_compile_debug()\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:582\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[1;34m(instructions, code_options)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[1;32m--> 582\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[0;32m    584\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2451\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 2451\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:893\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 893\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    894\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:805\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 805\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException:\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2642\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[1;34m(self, inst)\u001b[0m\n\u001b[0;32m   2641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mRETURN_VALUE\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[1;32m-> 2642\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2627\u001b[0m, in \u001b[0;36mInstructionTranslator._return\u001b[1;34m(self, inst)\u001b[0m\n\u001b[0;32m   2622\u001b[0m _step_logger()(\n\u001b[0;32m   2623\u001b[0m     logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[0;32m   2624\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo done tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2625\u001b[0m )\n\u001b[0;32m   2626\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m triggered compile\u001b[39m\u001b[38;5;124m\"\u001b[39m, inst\u001b[38;5;241m.\u001b[39mopname)\n\u001b[1;32m-> 2627\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_subgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraphCompileReason\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   2631\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2632\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2633\u001b[0m return_inst \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2634\u001b[0m     create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2636\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_CONST\u001b[39m\u001b[38;5;124m\"\u001b[39m, argval\u001b[38;5;241m=\u001b[39minst\u001b[38;5;241m.\u001b[39margval)\n\u001b[0;32m   2637\u001b[0m )\n\u001b[0;32m   2638\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39madd_output_instructions([return_inst])\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\output_graph.py:1098\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[1;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[0;32m   1095\u001b[0m append_prefix_insts()\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;66;03m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_call_fx_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mreversed\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstack_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1099\u001b[0m     \u001b[38;5;241m+\u001b[39m [create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNPACK_SEQUENCE\u001b[39m\u001b[38;5;124m\"\u001b[39m, arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(stack_values))]\n\u001b[0;32m   1100\u001b[0m )\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;66;03m# restore all the live local vars\u001b[39;00m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[0;32m   1103\u001b[0m     [PyCodegen(tx)\u001b[38;5;241m.\u001b[39mcreate_store(var) \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(restore_vars)]\n\u001b[0;32m   1104\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\output_graph.py:1318\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[1;34m(self, tx, rv, root)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracing_context\u001b[38;5;241m.\u001b[39mfake_mode \u001b[38;5;241m=\u001b[39m backend_fake_mode\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_global_state():\n\u001b[1;32m-> 1318\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lazy_graph_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _LazyGraphModule\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compiled_fn, _LazyGraphModule) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(compiled_fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), _LazyGraphModule)\n\u001b[0;32m   1324\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m compiled_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_lazy_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1328\u001b[0m     \u001b[38;5;66;03m# this is a _LazyGraphModule. This makes it easier for dynamo to\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m     \u001b[38;5;66;03m# optimize a _LazyGraphModule.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\utils.py:231\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    230\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 231\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    233\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\output_graph.py:1409\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[1;34m(self, gm)\u001b[0m\n\u001b[0;32m   1407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendCompilerFailed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiler_fn, e)\u001b[38;5;241m.\u001b[39mwith_traceback(\n\u001b[0;32m   1410\u001b[0m         e\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[0;32m   1411\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m signpost_event(\n\u001b[0;32m   1414\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1415\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1421\u001b[0m     },\n\u001b[0;32m   1422\u001b[0m )\n\u001b[0;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\output_graph.py:1390\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[1;34m(self, gm)\u001b[0m\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverify_correctness:\n\u001b[0;32m   1389\u001b[0m     compiler_fn \u001b[38;5;241m=\u001b[39m WrapperBackend(compiler_fn)\n\u001b[1;32m-> 1390\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1392\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(compiled_fn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiler_fn did not return callable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py:129\u001b[0m, in \u001b[0;36mWrapBackendDebug.__call__\u001b[1;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\__init__.py:1951\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[1;34m(self, model_, inputs_)\u001b[0m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_, inputs_):\n\u001b[0;32m   1949\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_fx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[1;32m-> 1951\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\compile_fx.py:1505\u001b[0m, in \u001b[0;36mcompile_fx\u001b[1;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m inference_compiler(unlifted_gm, example_inputs_)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m V\u001b[38;5;241m.\u001b[39mset_fake_mode(fake_mode), torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mtracing(\n\u001b[0;32m   1503\u001b[0m     tracing_context\n\u001b[0;32m   1504\u001b[0m ), compiled_autograd\u001b[38;5;241m.\u001b[39mdisable(), functorch_config\u001b[38;5;241m.\u001b[39mpatch(unlift_effect_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m-> 1505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maot_autograd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1512\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\backends\\common.py:69\u001b[0m, in \u001b[0;36mAotAutograd.__call__\u001b[1;34m(self, gm, example_inputs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# NB: NOT cloned!\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[1;32m---> 69\u001b[0m         cg \u001b[38;5;241m=\u001b[39m \u001b[43maot_module_simplified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m         counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m disable(cg)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_functorch\\aot_autograd.py:954\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[1;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler)\u001b[0m\n\u001b[0;32m    938\u001b[0m aot_config \u001b[38;5;241m=\u001b[39m AOTConfig(\n\u001b[0;32m    939\u001b[0m     fw_compiler\u001b[38;5;241m=\u001b[39mfw_compiler,\n\u001b[0;32m    940\u001b[0m     bw_compiler\u001b[38;5;241m=\u001b[39mbw_compiler,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    950\u001b[0m     no_tangents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    951\u001b[0m )\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compiled_autograd\u001b[38;5;241m.\u001b[39mdisable():\n\u001b[1;32m--> 954\u001b[0m     compiled_fn, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctional_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mGmWrapper):\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# This function is called by the flatten_graph_inputs wrapper, which boxes\u001b[39;00m\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;66;03m# the inputs so that they can be freed before the end of this scope.\u001b[39;00m\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;66;03m# For overhead reasons, this is not the default wrapper, see comment:\u001b[39;00m\n\u001b[0;32m    964\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/122535/files#r1560096481\u001b[39;00m\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mboxed_forward\u001b[39m(runtime_args: List[Any]):\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\utils.py:231\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    230\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 231\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    233\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_functorch\\aot_autograd.py:687\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[1;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[0;32m    683\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m aot_dispatch_base\n\u001b[0;32m    685\u001b[0m compiler_fn \u001b[38;5;241m=\u001b[39m choose_dispatcher(needs_autograd, aot_config)\n\u001b[1;32m--> 687\u001b[0m compiled_fn, fw_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn, fw_metadata\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py:168\u001b[0m, in \u001b[0;36maot_dispatch_base\u001b[1;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[0;32m    161\u001b[0m     tracing_context\u001b[38;5;241m.\u001b[39mfw_metadata \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    162\u001b[0m         fw_metadata\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m maybe_subclass_meta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m maybe_subclass_meta\u001b[38;5;241m.\u001b[39mfw_metadata\n\u001b[0;32m    165\u001b[0m     )\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TracingContext\u001b[38;5;241m.\u001b[39mreport_output_strides() \u001b[38;5;28;01mas\u001b[39;00m fwd_output_strides:\n\u001b[1;32m--> 168\u001b[0m     compiled_fw \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfw_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdated_flat_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fakified_out_wrapper\u001b[38;5;241m.\u001b[39mneeds_post_compile:\n\u001b[0;32m    171\u001b[0m     fakified_out_wrapper\u001b[38;5;241m.\u001b[39mset_fwd_output_strides(fwd_output_strides)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\utils.py:231\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    230\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 231\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    233\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\compile_fx.py:1410\u001b[0m, in \u001b[0;36mcompile_fx.<locals>.fw_compiler_base\u001b[1;34m(model, example_inputs, is_inference)\u001b[0m\n\u001b[0;32m   1402\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m orig_output_end_idx \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m num_model_outputs\n\u001b[0;32m   1404\u001b[0m     user_visible_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mfromkeys(\n\u001b[0;32m   1405\u001b[0m         n\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m   1406\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m model_outputs[original_output_start_index:orig_output_end_idx]\n\u001b[0;32m   1407\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mNode)\n\u001b[0;32m   1408\u001b[0m     )\n\u001b[1;32m-> 1410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_input_idxs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_static_input_idxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfixed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_inference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboxed_forward_device_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforward_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1418\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_visible_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_visible_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1419\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py:84\u001b[0m, in \u001b[0;36mwrap_compiler_debug.<locals>.debug_wrapper\u001b[1;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrepro_after \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# Call the compiler_fn - which is either aot_autograd or inductor\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;66;03m# with fake inputs\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     inner_compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# TODO: Failures here are troublesome because no real inputs,\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# need a different serialization strategy\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrepro_after \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\debug.py:304\u001b[0m, in \u001b[0;36mDebugContext.wrap.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m DebugContext():\n\u001b[1;32m--> 304\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\utils.py:231\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    230\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 231\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    233\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\compile_fx.py:527\u001b[0m, in \u001b[0;36mcompile_fx_inner\u001b[1;34m(gm, example_inputs, cudagraphs, static_input_idxs, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, boxed_forward_device_index, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[0;32m    517\u001b[0m     compiled_graph \u001b[38;5;241m=\u001b[39m FxGraphCache\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    518\u001b[0m         fx_codegen_and_compile,\n\u001b[0;32m    519\u001b[0m         gm,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m         remote\u001b[38;5;241m=\u001b[39mfx_graph_remote_cache,\n\u001b[0;32m    525\u001b[0m     )\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 527\u001b[0m     compiled_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFX codegen and compilation took \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start)\n\u001b[0;32m    533\u001b[0m \u001b[38;5;66;03m# check cudagraph disabling reasons from inductor lowering\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\compile_fx.py:831\u001b[0m, in \u001b[0;36mfx_codegen_and_compile\u001b[1;34m(gm, example_inputs, cudagraphs, static_input_idxs, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[0;32m    828\u001b[0m             output_strides\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    830\u001b[0m _check_triton_bf16_support(graph)\n\u001b[1;32m--> 831\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    832\u001b[0m num_bytes, nodes_num_elem, node_runtimes \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mcount_bytes()\n\u001b[0;32m    833\u001b[0m metrics\u001b[38;5;241m.\u001b[39mnum_bytes_accessed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_bytes\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\graph.py:1751\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_fn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AotCodeCompiler\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;28mself\u001b[39m, code, serialized_extern_kernel_nodes, cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda\n\u001b[0;32m   1749\u001b[0m     )\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcall\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\utils.py:231\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    230\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 231\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    233\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\graph.py:1680\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1675\u001b[0m \u001b[38;5;129m@dynamo_timed\u001b[39m(phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_gen\u001b[39m\u001b[38;5;124m\"\u001b[39m, fwd_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1677\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodecache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyCodeCache\n\u001b[0;32m   1679\u001b[0m     code, linemap \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1680\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodegen_with_cpp_wrapper() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpp_wrapper \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1681\u001b[0m     )\n\u001b[0;32m   1683\u001b[0m     output_code_log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput code: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, code)\n\u001b[0;32m   1684\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\graph.py:1636\u001b[0m, in \u001b[0;36mGraphLowering.codegen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscheduler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scheduler\n\u001b[0;32m   1634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_wrapper_code()\n\u001b[1;32m-> 1636\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m \u001b[43mScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1637\u001b[0m V\u001b[38;5;241m.\u001b[39mdebug\u001b[38;5;241m.\u001b[39mdraw_orig_fx_graph(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_gm, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mnodes)\n\u001b[0;32m   1639\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapper_code\u001b[38;5;241m.\u001b[39mpush_codegened_graph(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\utils.py:231\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    230\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 231\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    233\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\scheduler.py:1364\u001b[0m, in \u001b[0;36mScheduler.__init__\u001b[1;34m(self, nodes)\u001b[0m\n\u001b[0;32m   1356\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_grad_graph_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(_post_grad_graph_counter)\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_buffer_names \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1359\u001b[0m     \u001b[38;5;241m*\u001b[39mV\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mgraph_inputs\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[0;32m   1360\u001b[0m     \u001b[38;5;241m*\u001b[39mV\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mconstants\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[0;32m   1361\u001b[0m     \u001b[38;5;241m*\u001b[39mV\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mtorchbind_constants\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[0;32m   1362\u001b[0m }\n\u001b[1;32m-> 1364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_scheduler_node(n) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes]\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;66;03m# some new constants could have been created above\u001b[39;00m\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_buffer_names\u001b[38;5;241m.\u001b[39mupdate(V\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mconstants\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\scheduler.py:1364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1356\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_grad_graph_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(_post_grad_graph_counter)\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_buffer_names \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1359\u001b[0m     \u001b[38;5;241m*\u001b[39mV\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mgraph_inputs\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[0;32m   1360\u001b[0m     \u001b[38;5;241m*\u001b[39mV\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mconstants\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[0;32m   1361\u001b[0m     \u001b[38;5;241m*\u001b[39mV\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mtorchbind_constants\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[0;32m   1362\u001b[0m }\n\u001b[1;32m-> 1364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_scheduler_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes]\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;66;03m# some new constants could have been created above\u001b[39;00m\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_buffer_names\u001b[38;5;241m.\u001b[39mupdate(V\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mconstants\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\scheduler.py:1462\u001b[0m, in \u001b[0;36mScheduler.create_scheduler_node\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m   1460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NopKernelSchedulerNode(\u001b[38;5;28mself\u001b[39m, node)\n\u001b[0;32m   1461\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, (ir\u001b[38;5;241m.\u001b[39mComputedBuffer, ir\u001b[38;5;241m.\u001b[39mTemplateBuffer)):\n\u001b[1;32m-> 1462\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSchedulerNode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, ir\u001b[38;5;241m.\u001b[39mExternKernel):\n\u001b[0;32m   1464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ExternKernelSchedulerNode(\u001b[38;5;28mself\u001b[39m, node)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\scheduler.py:731\u001b[0m, in \u001b[0;36mSchedulerNode.__init__\u001b[1;34m(self, scheduler, node)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    726\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    727\u001b[0m     scheduler: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    728\u001b[0m     node: Union[ir\u001b[38;5;241m.\u001b[39mComputedBuffer, ir\u001b[38;5;241m.\u001b[39mTemplateBuffer],\n\u001b[0;32m    729\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(scheduler, node)\n\u001b[1;32m--> 731\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_attrs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\scheduler.py:742\u001b[0m, in \u001b[0;36mSchedulerNode._compute_attrs\u001b[1;34m(self, extra_indexing_constraints)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode, (ir\u001b[38;5;241m.\u001b[39mComputedBuffer, ir\u001b[38;5;241m.\u001b[39mTemplateBuffer))\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sizes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39msimplify_and_reorder(\n\u001b[0;32m    739\u001b[0m     extra_indexing_constraints\u001b[38;5;241m=\u001b[39mextra_indexing_constraints\n\u001b[0;32m    740\u001b[0m )\n\u001b[1;32m--> 742\u001b[0m group_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgroup_fn\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mget_device(), group_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sizes))\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode, ir\u001b[38;5;241m.\u001b[39mTemplateBuffer):\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\scheduler.py:2663\u001b[0m, in \u001b[0;36mScheduler.get_backend\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m   2661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_backend\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: torch\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseScheduling\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2662\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackends:\n\u001b[1;32m-> 2663\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackends[device] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackends[device]\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\scheduler.py:2655\u001b[0m, in \u001b[0;36mScheduler.create_backend\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m   2651\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2652\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_props\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_props\u001b[38;5;241m.\u001b[39mmajor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_props\u001b[38;5;241m.\u001b[39mminor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: B950\u001b[39;00m\n\u001b[0;32m   2653\u001b[0m         )\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m is_gpu(device\u001b[38;5;241m.\u001b[39mtype):\n\u001b[1;32m-> 2655\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2656\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: B950\u001b[39;00m\n\u001b[0;32m   2657\u001b[0m         )\n\u001b[0;32m   2659\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m device_scheduling(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mBackendCompilerFailed\u001b[0m: backend='inductor' raised:\nRuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "class Simple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 84 * 84, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = Simple().cuda()\n",
    "compiled_model = torch.compile(model)\n",
    "\n",
    "x = torch.randn(1, 4, 84, 84, device='cuda')\n",
    "with torch.no_grad():\n",
    "    out = compiled_model(x)\n",
    "\n",
    "print(\"Compile + inference worked:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug Harnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import memory as mem\n",
    "\n",
    "# Use a small capacity for easy debugging\n",
    "capacity = 10\n",
    "history_size = 3\n",
    "memory = mem.CircularReplayMemoryPER(capacity, history_size)\n",
    "\n",
    "# Push 30 fake transitions (will cause multiple wraps)\n",
    "for step in range(30):\n",
    "    frame = np.ones((84, 84), dtype=np.uint8) * step\n",
    "    action = random.randint(0, 3)\n",
    "    reward = random.random()\n",
    "    done = (step % 7 == 0)  # Random done flag every few steps\n",
    "\n",
    "    memory.push(frame, action, reward, done)\n",
    "\n",
    "    num_flags = sum(memory.valid_flags)\n",
    "    num_indices = len(memory.valid_indices)\n",
    "\n",
    "    if num_flags != num_indices:\n",
    "        print(f\" Mismatch at step {step}:\")\n",
    "        print(f\"   valid_flags count = {num_flags}\")\n",
    "        print(f\"   valid_indices count = {num_indices}\")\n",
    "        print(f\"   position = {memory.position}\")\n",
    "        print(f\"   size = {memory.size}\")\n",
    "        print(f\"   flags: {memory.valid_flags}\")\n",
    "        print(f\"   valid_indices: {memory.valid_indices}\")\n",
    "        raise AssertionError(\"valid_flags and valid_indices out of sync\")\n",
    "\n",
    "print(\" All checks passed  valid_flags == valid_indices through all pushes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import memory as RepBuff\n",
    "import traceback\n",
    "\n",
    "# Configuration\n",
    "capacity = 1000\n",
    "history_size = 4\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Configuration\n",
    "capacity = 1000\n",
    "history_size = 4\n",
    "batch_size = 16\n",
    "alpha = 0.6\n",
    "\n",
    "# Initialize memory\n",
    "memory = RepBuff.CircularReplayMemoryPER(capacity=capacity, history_size=history_size)\n",
    "\n",
    "# Track previous cache values for comparison\n",
    "previous_probs = None\n",
    "\n",
    "# Simulate a bunch of pushes\n",
    "for step in range(1500):\n",
    "    print(f\"Step {step}, \")\n",
    "    frame = np.ones((84, 84), dtype=np.uint8) * step\n",
    "    action = random.randint(0, 3)\n",
    "    reward = random.random()\n",
    "    done = (step % 50 == 0)\n",
    "\n",
    "\n",
    "    memory.push(frame, action, reward, done)\n",
    "\n",
    "    # Occasionally update TD-errors\n",
    "    if step > 100 and len(memory.valid_indices) >= batch_size:\n",
    "        indices = random.sample(range(len(memory.valid_indices)), min(len(memory.valid_indices), batch_size))\n",
    "        print(len(indices))\n",
    "        new_td = np.random.rand(batch_size).astype(np.float32)\n",
    "        memory.update_td_errors(indices, new_td)\n",
    "\n",
    "    # Check probability stability and cache correctness\n",
    "    if len(memory.valid_indices) >= batch_size:\n",
    "        try:\n",
    "            # Trigger cache update and record internal state\n",
    "            probs = memory.get_sampling_probs(memory.valid_indices)\n",
    "            total_prob = probs.sum()\n",
    "\n",
    "            # Cache sanity checks\n",
    "            assert hasattr(memory, \"_cached_probs\"), \"Missing cached_probs\"\n",
    "            assert memory._cached_probs is not None, \"Cached probs not populated\"\n",
    "            assert np.allclose(memory._cached_probs.sum(), 1.0, atol=1e-4), \"Cached probs do not sum to 1\"\n",
    "\n",
    "            # Compare with previous cache state\n",
    "            if previous_probs is not None:\n",
    "                if np.array_equal(previous_probs, memory._cached_probs):\n",
    "                    print(f\"[DEBUG] Cache unchanged at step {step}  may be okay if no new TDs pushed\")\n",
    "                else:\n",
    "                    print(f\"[DEBUG] Cache updated at step {step}\")\n",
    "            previous_probs = memory._cached_probs.copy()\n",
    "\n",
    "            if np.isnan(total_prob) or total_prob == 0:\n",
    "                print(\"[ERROR] Total probability is invalid at step:\", step)\n",
    "                print(\"TD errors:\", [memory.td_errors[i] for i in memory.valid_indices[:10]])\n",
    "                print(\"Sampling probs:\", probs[:10])\n",
    "                raise AssertionError(\"Invalid sampling distribution\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"[EXCEPTION]\", str(e))\n",
    "            traceback.print_exc()\n",
    "            break\n",
    "\n",
    "print(\" Debug harness completed without total_priority = 0 and with valid cache behavior\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsl_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
