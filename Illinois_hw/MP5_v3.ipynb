{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install gym pyvirtualdisplay\n",
    "# !sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install --upgrade setuptools --user\n",
    "# !pip3 install ez_setup \n",
    "# !pip3 install gym[atari] \n",
    "# !pip3 install gym[accept-rom-license] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "11.8\n",
      "CUDA device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if CUDA is available\n",
    "print(torch.version.cuda)  # Should print 11.8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"CUDA device: {device}\")  # Expected output: \"cuda:0\" if GPU is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "# import gym\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from model import *\n",
    "from config import *\n",
    "from checkpoint import *\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create metadata for checkpointing puposes\n",
    "def create_metadata(agent, global_episode, global_frame, eval_rewards, rewards, episodes, last_10_ep_losses, loss_tracker, epsilon, beta=None):\n",
    "    return {\n",
    "    'global_episode': global_episode,\n",
    "    'global_frame': global_frame,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'sch_gamma': scheduler_gamma,\n",
    "    'sch_step_size': scheduler_step_size,\n",
    "    'tgt_update_freq': update_target_network_frequency,\n",
    "    'memory capacity': Memory_capacity,  \n",
    "    'explore steps': EXPLORE_STEPS, \n",
    "    'epsilon_decay_rate': agent.epsilon_decay_rate,\n",
    "    'sticky_action_prob': sticky_action_prob,\n",
    "    'eval_rewards': eval_rewards,\n",
    "    'rewards': rewards,\n",
    "    'episodes': episodes,\n",
    "    'last_10_ep_losses': last_10_ep_losses,\n",
    "    'loss_tracker': loss_tracker,\n",
    "    'epislon': epsilon,\n",
    "    'per_beta': beta\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "class TeeLogger:\n",
    "    def __init__(self, filepath):\n",
    "        self.original_stdout = sys.__stdout__  # raw terminal (useful fallback)\n",
    "        self.ipython_stdout = sys.stdout       # the notebook's visible output\n",
    "        self.log = open(filepath, \"w\")\n",
    "\n",
    "    def write(self, message):\n",
    "        self.ipython_stdout.write(message)\n",
    "        self.log.write(message)\n",
    "        self.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        self.ipython_stdout.flush()\n",
    "        self.log.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the UIlliniois assigment this code is based on, I will be using the Gymnasium (https://github.com/farama-Foundation/gymnasium) and the Atari Learning Environment (ALE, link here: https://ale.farama.org/), rather than gym, which has been deprecated.  We will still be playing Breakout on Atari.\n",
    "\n",
    "To replecate BreakoutDeterministic-v4, as a starting point we'll use ALE/Breakout-v5 with frameskip=4, no \"sticky actions\" (i.e. deterministic actions), and a limited action space of NOOP, FIRE, LEFT, RIGHT.  However we won't be training on FIRE, instead we will ensure FIRE only happens when the game first starts and when a ball/life is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('BreakoutDeterministic-v4')\n",
    "env = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=0.0, full_action_space=False)  # Use equivalent parameters to BreakoutDeterministic-v4\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Env Frame Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(185, 160, 3)\n",
      "height:  185 width:  160\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 160x185 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create environments\n",
    "env = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=sticky_action_prob, full_action_space=False, render_mode=\"rgb_array\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "frame = env.render()\n",
    "frame = frame[20:-5, :, :]\n",
    "\n",
    "# Calculate figure size in inches to match pixel size\n",
    "height, width = frame.shape[:2]\n",
    "print(frame.shape)\n",
    "print(\"height: \", height, \"width: \", width)\n",
    "dpi = plt.rcParams['figure.dpi']\n",
    "figsize = (width / dpi, height / dpi)\n",
    "\n",
    "# Plot with exact pixel size\n",
    "plt.figure(figsize=figsize, dpi=dpi)\n",
    "plt.imshow(frame)\n",
    "plt.title(\"Raw Render Output (Exact Pixel Dimensions)\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #noop, left, and right.  Fire ball (action 1) is not trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Replay Memory Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN Agent initialized\n",
      "No replay buffer path provided. Creating new empty buffer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rbisk\\Dropbox\\GMU\\cs747 Deep Learning\\Final_Project\\Illinois_hw\\agent.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.policy_net.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_frame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m next_state, reward, terminations, truncations, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)  \n\u001b[0;32m     61\u001b[0m done \u001b[38;5;241m=\u001b[39m truncations \u001b[38;5;129;01mor\u001b[39;00m terminations\n\u001b[1;32m---> 62\u001b[0m frame_next_state \u001b[38;5;241m=\u001b[39m \u001b[43mget_frame\u001b[49m(next_state)\n\u001b[0;32m     63\u001b[0m frame_for_buffer \u001b[38;5;241m=\u001b[39m new_get_frame(next_state)\n\u001b[0;32m     65\u001b[0m history[\u001b[38;5;241m4\u001b[39m, :, :] \u001b[38;5;241m=\u001b[39m frame_next_state\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_frame' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# # # ### Pickle Replay Creator ###\n",
    "# from gymnasium.wrappers import RecordVideo\n",
    "# from config import *\n",
    "\n",
    "\n",
    "# mem_name = 'Buffer_for_testing_using_Run13_new_get_frame'\n",
    "\n",
    "# #create fresh environment\n",
    "# env = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=sticky_action_prob, full_action_space=False, render_mode='rgb_array')\n",
    "# # env = RecordVideo(env, video_folder=\"./videos\", episode_trigger=lambda ep: ep % 20 == 0)  \n",
    "\n",
    "# # Choose whether to use double DQN\n",
    "# double_dqn = False # set to True if using double DQN agent\n",
    "# if double_dqn:\n",
    "#     from agent_double import Agent\n",
    "# else:\n",
    "#     from agent import Agent\n",
    "\n",
    "# # print(\"Instantiating agent\")\n",
    "# agent = Agent(action_size)\n",
    "# agent.load_policy_net(\"./save_model/run13/good_Run13_StdDQN_750K_frames_imprvdBatching_2477_eps.pth\")\n",
    "# agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "# agent.epsilon = 0.1\n",
    "\n",
    "# frame = 0   \n",
    "# evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "# rewards, episodes = [], []\n",
    "# best_eval_reward = 0\n",
    "# exit_flag = False\n",
    "\n",
    "# for e in range(EPISODES):\n",
    "#     done = False\n",
    "#     score = 0\n",
    "\n",
    "#     history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "#     step = 0\n",
    "#     state, _ = env.reset()\n",
    "#     next_state = state\n",
    "#     life = number_lives\n",
    "#     fire_ready = True\n",
    "\n",
    "#     get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "#     while not done:\n",
    "#         step += 1\n",
    "#         frame += 1\n",
    "\n",
    "#         # Selet Action (with robust check for FIRE action)\n",
    "#         if fire_ready:\n",
    "#             next_state, force_done = reset_after_life_loss(env, history)\n",
    "#             if force_done:\n",
    "#                 break\n",
    "#             action = 1\n",
    "#             fire_ready = False\n",
    "#         else:\n",
    "#             trainable_action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "#             action = TRAINABLE_ACTIONS[trainable_action]\n",
    "\n",
    "#         state = next_state\n",
    "#         next_state, reward, terminations, truncations, info = env.step(action)  \n",
    "#         done = truncations or terminations\n",
    "#         frame_next_state = get_frame(next_state)\n",
    "#         frame_for_buffer = new_get_frame(next_state)\n",
    "\n",
    "#         history[4, :, :] = frame_next_state\n",
    "#         lost_life = check_live(life, info['lives'])  #check if the agent has lost a life\n",
    "\n",
    "#         if lost_life:\n",
    "#             fire_ready = True\n",
    "\n",
    "#         life = info['lives']\n",
    "#         r = reward\n",
    "\n",
    "#         # Store the transition in memory if it was not a FIRE action\n",
    "#         if action in TRAINABLE_ACTIONS:\n",
    "#             trainable_index = TRAINABLE_ACTIONS.index(action)\n",
    "#             agent.memory.push(deepcopy(frame_for_buffer), trainable_index, r, lost_life)\n",
    "#         # When replay buffer is filled save to pickle and break\n",
    "#         if len(agent.memory) == train_frame:\n",
    "#             print(f\"Memory filled, saving pickle file\") \n",
    "#             agent.save_replay_buffer(mem_name, frame)\n",
    "#             exit_flag = True\n",
    "#             break \n",
    "#         score += reward\n",
    "#         history[:4, :, :] = history[1:, :, :]  # shift history by one erasing oldest frame\n",
    "\n",
    "#         if done:\n",
    "#             fire_ready = True\n",
    "#             evaluation_reward.append(score)\n",
    "#             rewards.append(np.mean(evaluation_reward))  # record moving average of last evaluation_reward_length episodes\n",
    "#             episodes.append(e)\n",
    "\n",
    "#             # print episode information \n",
    "#             if e % 1 == 0:\n",
    "#                 print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "#                   len(agent.memory), \"  epsilon:\", agent.epsilon, \"  steps:\", step,\n",
    "#                   \"  lr:\", agent.optimizer.param_groups[0]['lr'], \n",
    "#                   \"  evaluation reward:\", np.mean(evaluation_reward))\n",
    "#     if exit_flag:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start logging\n",
    "# log_file = \"./test_log.txt\"\n",
    "# tee = TeeLogger(log_file)\n",
    "# sys.stdout = tee\n",
    "# sys.stderr = tee\n",
    "\n",
    "# print(f\"Logging started. Output will be written to both notebook and {log_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stop logging safely\n",
    "# sys.stdout = tee.original_stdout\n",
    "# sys.stderr = tee.original_stdout\n",
    "# tee.close()\n",
    "\n",
    "# print(\"This goes only to the notebook now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup Run Name and Memory Paths ##\n",
    "mem_path = None\n",
    "# mem_path = './checkpoints/Run1_DQN_Serial_20pctExplore_and_Sticky_100000_replay_buffer.pkl'\n",
    "# mem_path = './checkpoints/Buffer_for_testing_using_Run5_50K_replay_buffer.pkl'\n",
    "# mem_path = './checkpoints/Buffer_for_testing_using_Run13_new_get_frame_100974_replay_buffer.pkl'\n",
    "seed = 83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dir = f\"./logs/run{run_num}\"\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# log_file = os.path.join(log_dir, f\"{run_name}_output.log\")\n",
    "# tee = TeeLogger(log_file)\n",
    "# sys.stdout = tee\n",
    "# sys.stderr = tee\n",
    "\n",
    "# print(f\"Logging started. Output will be written to both notebook and {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run Run17_StdDQN_750Kfr_newest_get_frame_CirularPER_wIS\n",
      "Instantiating agent\n",
      "DQN Agent initialized\n",
      "Setting up Circular PER Replay Buffer of size 250000\n",
      "episode: 1   frame: 120   score: 0.0   memory length: 115   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.0   mean loss: 0   mean max Q: nan\n",
      "episode: 2   frame: 347   score: 3.0   memory length: 337   epsilon: 1.0   steps: 227   lr: 0.0005   PER_beta: 0.4   reward MA: 1.5   mean loss: 0   mean max Q: nan\n",
      "episode: 3   frame: 467   score: 0.0   memory length: 452   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 1.0   mean loss: 0   mean max Q: nan\n",
      "episode: 4   frame: 587   score: 0.0   memory length: 567   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.75   mean loss: 0   mean max Q: nan\n",
      "episode: 5   frame: 707   score: 0.0   memory length: 682   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.6   mean loss: 0   mean max Q: nan\n",
      "episode: 6   frame: 855   score: 1.0   memory length: 825   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.667   mean loss: 0   mean max Q: nan\n",
      "episode: 7   frame: 1003   score: 1.0   memory length: 968   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.714   mean loss: 0   mean max Q: nan\n",
      "episode: 8   frame: 1151   score: 1.0   memory length: 1111   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.75   mean loss: 0   mean max Q: nan\n",
      "episode: 9   frame: 1271   score: 0.0   memory length: 1226   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.667   mean loss: 0   mean max Q: nan\n",
      "episode: 10   frame: 1391   score: 0.0   memory length: 1341   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.6   mean loss: 0   mean max Q: nan\n",
      "episode: 11   frame: 1511   score: 0.0   memory length: 1456   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.545   mean loss: 0   mean max Q: nan\n",
      "episode: 12   frame: 1659   score: 1.0   memory length: 1599   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.583   mean loss: 0   mean max Q: nan\n",
      "episode: 13   frame: 1779   score: 0.0   memory length: 1714   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.538   mean loss: 0   mean max Q: nan\n",
      "episode: 14   frame: 1947   score: 1.0   memory length: 1877   epsilon: 1.0   steps: 168   lr: 0.0005   PER_beta: 0.4   reward MA: 0.571   mean loss: 0   mean max Q: nan\n",
      "episode: 15   frame: 2067   score: 0.0   memory length: 1992   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.533   mean loss: 0   mean max Q: nan\n",
      "episode: 16   frame: 2187   score: 0.0   memory length: 2107   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.5   mean loss: 0   mean max Q: nan\n",
      "episode: 17   frame: 2307   score: 0.0   memory length: 2222   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.471   mean loss: 0   mean max Q: nan\n",
      "episode: 18   frame: 2427   score: 0.0   memory length: 2337   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.444   mean loss: 0   mean max Q: nan\n",
      "episode: 19   frame: 2623   score: 2.0   memory length: 2528   epsilon: 1.0   steps: 196   lr: 0.0005   PER_beta: 0.4   reward MA: 0.526   mean loss: 0   mean max Q: nan\n",
      "episode: 20   frame: 2789   score: 1.0   memory length: 2689   epsilon: 1.0   steps: 166   lr: 0.0005   PER_beta: 0.4   reward MA: 0.55   mean loss: 0   mean max Q: nan\n",
      "episode: 21   frame: 2937   score: 1.0   memory length: 2832   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.571   mean loss: 0   mean max Q: nan\n",
      "episode: 22   frame: 3203   score: 3.0   memory length: 3093   epsilon: 1.0   steps: 266   lr: 0.0005   PER_beta: 0.4   reward MA: 0.682   mean loss: 0   mean max Q: nan\n",
      "episode: 23   frame: 3323   score: 0.0   memory length: 3208   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.652   mean loss: 0   mean max Q: nan\n",
      "episode: 24   frame: 3443   score: 0.0   memory length: 3323   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.625   mean loss: 0   mean max Q: nan\n",
      "episode: 25   frame: 3609   score: 1.0   memory length: 3484   epsilon: 1.0   steps: 166   lr: 0.0005   PER_beta: 0.4   reward MA: 0.64   mean loss: 0   mean max Q: nan\n",
      "episode: 26   frame: 3757   score: 1.0   memory length: 3627   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.654   mean loss: 0   mean max Q: nan\n",
      "episode: 27   frame: 3954   score: 2.0   memory length: 3819   epsilon: 1.0   steps: 197   lr: 0.0005   PER_beta: 0.4   reward MA: 0.704   mean loss: 0   mean max Q: nan\n",
      "episode: 28   frame: 4135   score: 2.0   memory length: 3995   epsilon: 1.0   steps: 181   lr: 0.0005   PER_beta: 0.4   reward MA: 0.75   mean loss: 0   mean max Q: nan\n",
      "episode: 29   frame: 4255   score: 0.0   memory length: 4110   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.724   mean loss: 0   mean max Q: nan\n",
      "episode: 30   frame: 4423   score: 1.0   memory length: 4273   epsilon: 1.0   steps: 168   lr: 0.0005   PER_beta: 0.4   reward MA: 0.733   mean loss: 0   mean max Q: nan\n",
      "episode: 31   frame: 4621   score: 2.0   memory length: 4466   epsilon: 1.0   steps: 198   lr: 0.0005   PER_beta: 0.4   reward MA: 0.774   mean loss: 0   mean max Q: nan\n",
      "episode: 32   frame: 4866   score: 3.0   memory length: 4706   epsilon: 1.0   steps: 245   lr: 0.0005   PER_beta: 0.4   reward MA: 0.844   mean loss: 0   mean max Q: nan\n",
      "episode: 33   frame: 4986   score: 0.0   memory length: 4821   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.818   mean loss: 0   mean max Q: nan\n",
      "episode: 34   frame: 5193   score: 3.0   memory length: 5023   epsilon: 1.0   steps: 207   lr: 0.0005   PER_beta: 0.4   reward MA: 0.882   mean loss: 0   mean max Q: nan\n",
      "episode: 35   frame: 5341   score: 1.0   memory length: 5166   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.886   mean loss: 0   mean max Q: nan\n",
      "episode: 36   frame: 5461   score: 0.0   memory length: 5281   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.861   mean loss: 0   mean max Q: nan\n",
      "episode: 37   frame: 5630   score: 1.0   memory length: 5445   epsilon: 1.0   steps: 169   lr: 0.0005   PER_beta: 0.4   reward MA: 0.865   mean loss: 0   mean max Q: nan\n",
      "episode: 38   frame: 5778   score: 1.0   memory length: 5588   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.868   mean loss: 0   mean max Q: nan\n",
      "episode: 39   frame: 5926   score: 1.0   memory length: 5731   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.872   mean loss: 0   mean max Q: nan\n",
      "episode: 40   frame: 6074   score: 1.0   memory length: 5874   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.875   mean loss: 0   mean max Q: nan\n",
      "episode: 41   frame: 6298   score: 3.0   memory length: 6093   epsilon: 1.0   steps: 224   lr: 0.0005   PER_beta: 0.4   reward MA: 0.927   mean loss: 0   mean max Q: nan\n",
      "episode: 42   frame: 6494   score: 2.0   memory length: 6284   epsilon: 1.0   steps: 196   lr: 0.0005   PER_beta: 0.4   reward MA: 0.952   mean loss: 0   mean max Q: nan\n",
      "episode: 43   frame: 6614   score: 0.0   memory length: 6399   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.93   mean loss: 0   mean max Q: nan\n",
      "episode: 44   frame: 6762   score: 1.0   memory length: 6542   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.932   mean loss: 0   mean max Q: nan\n",
      "episode: 45   frame: 6882   score: 0.0   memory length: 6657   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.911   mean loss: 0   mean max Q: nan\n",
      "episode: 46   frame: 7060   score: 2.0   memory length: 6830   epsilon: 1.0   steps: 178   lr: 0.0005   PER_beta: 0.4   reward MA: 0.935   mean loss: 0   mean max Q: nan\n",
      "episode: 47   frame: 7180   score: 0.0   memory length: 6945   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.915   mean loss: 0   mean max Q: nan\n",
      "episode: 48   frame: 7328   score: 1.0   memory length: 7088   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.917   mean loss: 0   mean max Q: nan\n",
      "episode: 49   frame: 7476   score: 1.0   memory length: 7231   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.918   mean loss: 0   mean max Q: nan\n",
      "episode: 50   frame: 7654   score: 2.0   memory length: 7404   epsilon: 1.0   steps: 178   lr: 0.0005   PER_beta: 0.4   reward MA: 0.94   mean loss: 0   mean max Q: nan\n",
      "episode: 51   frame: 7774   score: 0.0   memory length: 7519   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.94   mean loss: 0   mean max Q: nan\n",
      "episode: 52   frame: 7894   score: 0.0   memory length: 7634   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.88   mean loss: 0   mean max Q: nan\n",
      "episode: 53   frame: 8014   score: 0.0   memory length: 7749   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.88   mean loss: 0   mean max Q: nan\n",
      "episode: 54   frame: 8210   score: 2.0   memory length: 7940   epsilon: 1.0   steps: 196   lr: 0.0005   PER_beta: 0.4   reward MA: 0.92   mean loss: 0   mean max Q: nan\n",
      "episode: 55   frame: 8407   score: 2.0   memory length: 8132   epsilon: 1.0   steps: 197   lr: 0.0005   PER_beta: 0.4   reward MA: 0.96   mean loss: 0   mean max Q: nan\n",
      "episode: 56   frame: 8575   score: 1.0   memory length: 8295   epsilon: 1.0   steps: 168   lr: 0.0005   PER_beta: 0.4   reward MA: 0.96   mean loss: 0   mean max Q: nan\n",
      "episode: 57   frame: 8695   score: 0.0   memory length: 8410   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.94   mean loss: 0   mean max Q: nan\n",
      "episode: 58   frame: 8843   score: 1.0   memory length: 8553   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.94   mean loss: 0   mean max Q: nan\n",
      "episode: 59   frame: 8963   score: 0.0   memory length: 8668   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.94   mean loss: 0   mean max Q: nan\n",
      "episode: 60   frame: 9083   score: 0.0   memory length: 8783   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.94   mean loss: 0   mean max Q: nan\n",
      "episode: 61   frame: 9203   score: 0.0   memory length: 8898   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.94   mean loss: 0   mean max Q: nan\n",
      "episode: 62   frame: 9323   score: 0.0   memory length: 9013   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.92   mean loss: 0   mean max Q: nan\n",
      "episode: 63   frame: 9491   score: 1.0   memory length: 9176   epsilon: 1.0   steps: 168   lr: 0.0005   PER_beta: 0.4   reward MA: 0.94   mean loss: 0   mean max Q: nan\n",
      "episode: 64   frame: 9639   score: 1.0   memory length: 9319   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.94   mean loss: 0   mean max Q: nan\n",
      "episode: 65   frame: 9787   score: 1.0   memory length: 9462   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.96   mean loss: 0   mean max Q: nan\n",
      "episode: 66   frame: 9907   score: 0.0   memory length: 9577   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.96   mean loss: 0   mean max Q: nan\n",
      "episode: 67   frame: 10027   score: 0.0   memory length: 9692   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.96   mean loss: 0   mean max Q: nan\n",
      "episode: 68   frame: 10175   score: 1.0   memory length: 9835   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.98   mean loss: 0   mean max Q: nan\n",
      "episode: 69   frame: 10323   score: 1.0   memory length: 9978   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.96   mean loss: 0   mean max Q: nan\n",
      "episode: 70   frame: 10471   score: 1.0   memory length: 10121   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.96   mean loss: 0   mean max Q: nan\n",
      "episode: 71   frame: 10591   score: 0.0   memory length: 10236   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.94   mean loss: 0   mean max Q: nan\n",
      "episode: 72   frame: 10711   score: 0.0   memory length: 10351   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.88   mean loss: 0   mean max Q: nan\n",
      "episode: 73   frame: 10879   score: 1.0   memory length: 10514   epsilon: 1.0   steps: 168   lr: 0.0005   PER_beta: 0.4   reward MA: 0.9   mean loss: 0   mean max Q: nan\n",
      "episode: 74   frame: 10999   score: 0.0   memory length: 10629   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.9   mean loss: 0   mean max Q: nan\n",
      "episode: 75   frame: 11177   score: 2.0   memory length: 10802   epsilon: 1.0   steps: 178   lr: 0.0005   PER_beta: 0.4   reward MA: 0.92   mean loss: 0   mean max Q: nan\n",
      "episode: 76   frame: 11420   score: 3.0   memory length: 11040   epsilon: 1.0   steps: 243   lr: 0.0005   PER_beta: 0.4   reward MA: 0.96   mean loss: 0   mean max Q: nan\n",
      "episode: 77   frame: 11540   score: 0.0   memory length: 11155   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.92   mean loss: 0   mean max Q: nan\n",
      "episode: 78   frame: 11660   score: 0.0   memory length: 11270   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.88   mean loss: 0   mean max Q: nan\n",
      "episode: 79   frame: 11826   score: 1.0   memory length: 11431   epsilon: 1.0   steps: 166   lr: 0.0005   PER_beta: 0.4   reward MA: 0.9   mean loss: 0   mean max Q: nan\n",
      "episode: 80   frame: 11974   score: 1.0   memory length: 11574   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.9   mean loss: 0   mean max Q: nan\n",
      "episode: 81   frame: 12094   score: 0.0   memory length: 11689   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.86   mean loss: 0   mean max Q: nan\n",
      "episode: 82   frame: 12214   score: 0.0   memory length: 11804   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.8   mean loss: 0   mean max Q: nan\n",
      "episode: 83   frame: 12414   score: 2.0   memory length: 11999   epsilon: 1.0   steps: 200   lr: 0.0005   PER_beta: 0.4   reward MA: 0.84   mean loss: 0   mean max Q: nan\n",
      "episode: 84   frame: 12562   score: 1.0   memory length: 12142   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.8   mean loss: 0   mean max Q: nan\n",
      "episode: 85   frame: 12682   score: 0.0   memory length: 12257   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.78   mean loss: 0   mean max Q: nan\n",
      "episode: 86   frame: 12802   score: 0.0   memory length: 12372   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.78   mean loss: 0   mean max Q: nan\n",
      "episode: 87   frame: 12922   score: 0.0   memory length: 12487   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.76   mean loss: 0   mean max Q: nan\n",
      "episode: 88   frame: 13090   score: 1.0   memory length: 12650   epsilon: 1.0   steps: 168   lr: 0.0005   PER_beta: 0.4   reward MA: 0.76   mean loss: 0   mean max Q: nan\n",
      "episode: 89   frame: 13210   score: 0.0   memory length: 12765   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.74   mean loss: 0   mean max Q: nan\n",
      "episode: 90   frame: 13358   score: 1.0   memory length: 12908   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.74   mean loss: 0   mean max Q: nan\n",
      "episode: 91   frame: 13478   score: 0.0   memory length: 13023   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.68   mean loss: 0   mean max Q: nan\n",
      "episode: 92   frame: 13646   score: 1.0   memory length: 13186   epsilon: 1.0   steps: 168   lr: 0.0005   PER_beta: 0.4   reward MA: 0.66   mean loss: 0   mean max Q: nan\n",
      "episode: 93   frame: 13814   score: 1.0   memory length: 13349   epsilon: 1.0   steps: 168   lr: 0.0005   PER_beta: 0.4   reward MA: 0.68   mean loss: 0   mean max Q: nan\n",
      "episode: 94   frame: 13992   score: 2.0   memory length: 13522   epsilon: 1.0   steps: 178   lr: 0.0005   PER_beta: 0.4   reward MA: 0.7   mean loss: 0   mean max Q: nan\n",
      "episode: 95   frame: 14170   score: 2.0   memory length: 13695   epsilon: 1.0   steps: 178   lr: 0.0005   PER_beta: 0.4   reward MA: 0.74   mean loss: 0   mean max Q: nan\n",
      "episode: 96   frame: 14290   score: 0.0   memory length: 13810   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.7   mean loss: 0   mean max Q: nan\n",
      "episode: 97   frame: 14410   score: 0.0   memory length: 13925   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.7   mean loss: 0   mean max Q: nan\n",
      "episode: 98   frame: 14558   score: 1.0   memory length: 14068   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.7   mean loss: 0   mean max Q: nan\n",
      "episode: 99   frame: 14754   score: 2.0   memory length: 14259   epsilon: 1.0   steps: 196   lr: 0.0005   PER_beta: 0.4   reward MA: 0.72   mean loss: 0   mean max Q: nan\n",
      "episode: 100   frame: 14874   score: 0.0   memory length: 14374   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.68   mean loss: 0   mean max Q: nan\n",
      "episode: 101   frame: 15022   score: 1.0   memory length: 14517   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.7   mean loss: 0   mean max Q: nan\n",
      "episode: 102   frame: 15142   score: 0.0   memory length: 14632   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.7   mean loss: 0   mean max Q: nan\n",
      "episode: 103   frame: 15290   score: 1.0   memory length: 14775   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.72   mean loss: 0   mean max Q: nan\n",
      "episode: 104   frame: 15410   score: 0.0   memory length: 14890   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.68   mean loss: 0   mean max Q: nan\n",
      "episode: 105   frame: 15530   score: 0.0   memory length: 15005   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.64   mean loss: 0   mean max Q: nan\n",
      "episode: 106   frame: 15650   score: 0.0   memory length: 15120   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.62   mean loss: 0   mean max Q: nan\n",
      "episode: 107   frame: 15770   score: 0.0   memory length: 15235   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.62   mean loss: 0   mean max Q: nan\n",
      "episode: 108   frame: 15918   score: 1.0   memory length: 15378   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.62   mean loss: 0   mean max Q: nan\n",
      "episode: 109   frame: 16038   score: 0.0   memory length: 15493   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.62   mean loss: 0   mean max Q: nan\n",
      "episode: 110   frame: 16186   score: 1.0   memory length: 15636   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.64   mean loss: 0   mean max Q: nan\n",
      "episode: 111   frame: 16354   score: 1.0   memory length: 15799   epsilon: 1.0   steps: 168   lr: 0.0005   PER_beta: 0.4   reward MA: 0.66   mean loss: 0   mean max Q: nan\n",
      "episode: 112   frame: 16550   score: 2.0   memory length: 15990   epsilon: 1.0   steps: 196   lr: 0.0005   PER_beta: 0.4   reward MA: 0.7   mean loss: 0   mean max Q: nan\n",
      "episode: 113   frame: 16670   score: 0.0   memory length: 16105   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.68   mean loss: 0   mean max Q: nan\n",
      "episode: 114   frame: 16818   score: 1.0   memory length: 16248   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.68   mean loss: 0   mean max Q: nan\n",
      "episode: 115   frame: 16984   score: 1.0   memory length: 16409   epsilon: 1.0   steps: 166   lr: 0.0005   PER_beta: 0.4   reward MA: 0.68   mean loss: 0   mean max Q: nan\n",
      "episode: 116   frame: 17165   score: 2.0   memory length: 16585   epsilon: 1.0   steps: 181   lr: 0.0005   PER_beta: 0.4   reward MA: 0.72   mean loss: 0   mean max Q: nan\n",
      "episode: 117   frame: 17440   score: 4.0   memory length: 16855   epsilon: 1.0   steps: 275   lr: 0.0005   PER_beta: 0.4   reward MA: 0.8   mean loss: 0   mean max Q: nan\n",
      "episode: 118   frame: 17588   score: 1.0   memory length: 16998   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.8   mean loss: 0   mean max Q: nan\n",
      "episode: 119   frame: 17708   score: 0.0   memory length: 17113   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.78   mean loss: 0   mean max Q: nan\n",
      "episode: 120   frame: 17828   score: 0.0   memory length: 17228   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.76   mean loss: 0   mean max Q: nan\n",
      "episode: 121   frame: 17948   score: 0.0   memory length: 17343   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.76   mean loss: 0   mean max Q: nan\n",
      "episode: 122   frame: 18126   score: 2.0   memory length: 17516   epsilon: 1.0   steps: 178   lr: 0.0005   PER_beta: 0.4   reward MA: 0.8   mean loss: 0   mean max Q: nan\n",
      "episode: 123   frame: 18438   score: 4.0   memory length: 17823   epsilon: 1.0   steps: 312   lr: 0.0005   PER_beta: 0.4   reward MA: 0.86   mean loss: 0   mean max Q: nan\n",
      "episode: 124   frame: 18615   score: 2.0   memory length: 17995   epsilon: 1.0   steps: 177   lr: 0.0005   PER_beta: 0.4   reward MA: 0.9   mean loss: 0   mean max Q: nan\n",
      "episode: 125   frame: 18763   score: 1.0   memory length: 18138   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.88   mean loss: 0   mean max Q: nan\n",
      "episode: 126   frame: 18931   score: 1.0   memory length: 18301   epsilon: 1.0   steps: 168   lr: 0.0005   PER_beta: 0.4   reward MA: 0.84   mean loss: 0   mean max Q: nan\n",
      "episode: 127   frame: 19051   score: 0.0   memory length: 18416   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.84   mean loss: 0   mean max Q: nan\n",
      "episode: 128   frame: 19171   score: 0.0   memory length: 18531   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.84   mean loss: 0   mean max Q: nan\n",
      "episode: 129   frame: 19340   score: 1.0   memory length: 18695   epsilon: 1.0   steps: 169   lr: 0.0005   PER_beta: 0.4   reward MA: 0.84   mean loss: 0   mean max Q: nan\n",
      "episode: 130   frame: 19506   score: 1.0   memory length: 18856   epsilon: 1.0   steps: 166   lr: 0.0005   PER_beta: 0.4   reward MA: 0.84   mean loss: 0   mean max Q: nan\n",
      "episode: 131   frame: 19685   score: 2.0   memory length: 19030   epsilon: 1.0   steps: 179   lr: 0.0005   PER_beta: 0.4   reward MA: 0.88   mean loss: 0   mean max Q: nan\n",
      "episode: 132   frame: 19933   score: 3.0   memory length: 19273   epsilon: 1.0   steps: 248   lr: 0.0005   PER_beta: 0.4   reward MA: 0.94   mean loss: 0   mean max Q: nan\n",
      "episode: 133   frame: 20053   score: 0.0   memory length: 19388   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.9   mean loss: 0   mean max Q: nan\n",
      "episode: 134   frame: 20201   score: 1.0   memory length: 19531   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.9   mean loss: 0   mean max Q: nan\n",
      "episode: 135   frame: 20424   score: 3.0   memory length: 19749   epsilon: 1.0   steps: 223   lr: 0.0005   PER_beta: 0.4   reward MA: 0.96   mean loss: 0   mean max Q: nan\n",
      "episode: 136   frame: 20544   score: 0.0   memory length: 19864   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.96   mean loss: 0   mean max Q: nan\n",
      "episode: 137   frame: 20692   score: 1.0   memory length: 20007   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.98   mean loss: 0   mean max Q: nan\n",
      "episode: 138   frame: 20922   score: 3.0   memory length: 20232   epsilon: 1.0   steps: 230   lr: 0.0005   PER_beta: 0.4   reward MA: 1.02   mean loss: 0   mean max Q: nan\n",
      "episode: 139   frame: 21070   score: 1.0   memory length: 20375   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 1.04   mean loss: 0   mean max Q: nan\n",
      "episode: 140   frame: 21218   score: 1.0   memory length: 20518   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 1.04   mean loss: 0   mean max Q: nan\n",
      "episode: 141   frame: 21386   score: 1.0   memory length: 20681   epsilon: 1.0   steps: 168   lr: 0.0005   PER_beta: 0.4   reward MA: 1.06   mean loss: 0   mean max Q: nan\n",
      "episode: 142   frame: 21534   score: 1.0   memory length: 20824   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 1.06   mean loss: 0   mean max Q: nan\n",
      "episode: 143   frame: 21730   score: 2.0   memory length: 21015   epsilon: 1.0   steps: 196   lr: 0.0005   PER_beta: 0.4   reward MA: 1.08   mean loss: 0   mean max Q: nan\n",
      "episode: 144   frame: 21927   score: 2.0   memory length: 21207   epsilon: 1.0   steps: 197   lr: 0.0005   PER_beta: 0.4   reward MA: 1.08   mean loss: 0   mean max Q: nan\n",
      "episode: 145   frame: 22047   score: 0.0   memory length: 21322   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 1.04   mean loss: 0   mean max Q: nan\n",
      "episode: 146   frame: 22295   score: 3.0   memory length: 21565   epsilon: 1.0   steps: 248   lr: 0.0005   PER_beta: 0.4   reward MA: 1.1   mean loss: 0   mean max Q: nan\n",
      "episode: 147   frame: 22463   score: 1.0   memory length: 21728   epsilon: 1.0   steps: 168   lr: 0.0005   PER_beta: 0.4   reward MA: 1.12   mean loss: 0   mean max Q: nan\n",
      "episode: 148   frame: 22611   score: 1.0   memory length: 21871   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 1.12   mean loss: 0   mean max Q: nan\n",
      "episode: 149   frame: 22759   score: 1.0   memory length: 22014   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 1.1   mean loss: 0   mean max Q: nan\n",
      "episode: 150   frame: 22976   score: 2.0   memory length: 22226   epsilon: 1.0   steps: 217   lr: 0.0005   PER_beta: 0.4   reward MA: 1.14   mean loss: 0   mean max Q: nan\n",
      "episode: 151   frame: 23096   score: 0.0   memory length: 22341   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 1.12   mean loss: 0   mean max Q: nan\n",
      "episode: 152   frame: 23244   score: 1.0   memory length: 22484   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 1.14   mean loss: 0   mean max Q: nan\n",
      "episode: 153   frame: 23364   score: 0.0   memory length: 22599   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 1.12   mean loss: 0   mean max Q: nan\n",
      "episode: 154   frame: 23512   score: 1.0   memory length: 22742   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 1.14   mean loss: 0   mean max Q: nan\n",
      "episode: 155   frame: 23632   score: 0.0   memory length: 22857   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 1.14   mean loss: 0   mean max Q: nan\n",
      "episode: 156   frame: 23800   score: 1.0   memory length: 23020   epsilon: 1.0   steps: 168   lr: 0.0005   PER_beta: 0.4   reward MA: 1.16   mean loss: 0   mean max Q: nan\n",
      "episode: 157   frame: 23966   score: 1.0   memory length: 23181   epsilon: 1.0   steps: 166   lr: 0.0005   PER_beta: 0.4   reward MA: 1.18   mean loss: 0   mean max Q: nan\n",
      "episode: 158   frame: 24114   score: 1.0   memory length: 23324   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 1.18   mean loss: 0   mean max Q: nan\n",
      "episode: 159   frame: 24312   score: 2.0   memory length: 23517   epsilon: 1.0   steps: 198   lr: 0.0005   PER_beta: 0.4   reward MA: 1.22   mean loss: 0   mean max Q: nan\n",
      "episode: 160   frame: 24460   score: 1.0   memory length: 23660   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 1.22   mean loss: 0   mean max Q: nan\n",
      "episode: 161   frame: 24580   score: 0.0   memory length: 23775   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 1.2   mean loss: 0   mean max Q: nan\n",
      "episode: 162   frame: 24700   score: 0.0   memory length: 23890   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 1.16   mean loss: 0   mean max Q: nan\n",
      "episode: 163   frame: 24878   score: 2.0   memory length: 24063   epsilon: 1.0   steps: 178   lr: 0.0005   PER_beta: 0.4   reward MA: 1.2   mean loss: 0   mean max Q: nan\n",
      "episode: 164   frame: 25046   score: 1.0   memory length: 24226   epsilon: 1.0   steps: 168   lr: 0.0005   PER_beta: 0.4   reward MA: 1.2   mean loss: 0   mean max Q: nan\n",
      "episode: 165   frame: 25166   score: 0.0   memory length: 24341   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 1.18   mean loss: 0   mean max Q: nan\n",
      "episode: 166   frame: 25286   score: 0.0   memory length: 24456   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 1.14   mean loss: 0   mean max Q: nan\n",
      "episode: 167   frame: 25465   score: 2.0   memory length: 24630   epsilon: 1.0   steps: 179   lr: 0.0005   PER_beta: 0.4   reward MA: 1.1   mean loss: 0   mean max Q: nan\n",
      "episode: 168   frame: 25741   score: 4.0   memory length: 24901   epsilon: 1.0   steps: 276   lr: 0.0005   PER_beta: 0.4   reward MA: 1.16   mean loss: 0   mean max Q: nan\n",
      "episode: 169   frame: 25938   score: 2.0   memory length: 25093   epsilon: 1.0   steps: 197   lr: 0.0005   PER_beta: 0.4   reward MA: 1.2   mean loss: 0   mean max Q: nan\n",
      "episode: 170   frame: 26086   score: 1.0   memory length: 25236   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 1.22   mean loss: 0   mean max Q: nan\n",
      "episode: 171   frame: 26206   score: 0.0   memory length: 25351   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 1.22   mean loss: 0   mean max Q: nan\n",
      "episode: 172   frame: 26454   score: 3.0   memory length: 25594   epsilon: 1.0   steps: 248   lr: 0.0005   PER_beta: 0.4   reward MA: 1.24   mean loss: 0   mean max Q: nan\n",
      "episode: 173   frame: 26574   score: 0.0   memory length: 25709   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 1.16   mean loss: 0   mean max Q: nan\n",
      "episode: 174   frame: 26694   score: 0.0   memory length: 25824   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 1.12   mean loss: 0   mean max Q: nan\n",
      "episode: 175   frame: 26814   score: 0.0   memory length: 25939   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 1.1   mean loss: 0   mean max Q: nan\n",
      "episode: 176   frame: 27029   score: 2.0   memory length: 26149   epsilon: 1.0   steps: 215   lr: 0.0005   PER_beta: 0.4   reward MA: 1.12   mean loss: 0   mean max Q: nan\n",
      "episode: 177   frame: 27149   score: 0.0   memory length: 26264   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 1.12   mean loss: 0   mean max Q: nan\n",
      "episode: 178   frame: 27297   score: 1.0   memory length: 26407   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 1.14   mean loss: 0   mean max Q: nan\n",
      "episode: 179   frame: 27445   score: 1.0   memory length: 26550   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 1.14   mean loss: 0   mean max Q: nan\n",
      "episode: 180   frame: 27642   score: 2.0   memory length: 26742   epsilon: 1.0   steps: 197   lr: 0.0005   PER_beta: 0.4   reward MA: 1.16   mean loss: 0   mean max Q: nan\n",
      "episode: 181   frame: 27808   score: 1.0   memory length: 26903   epsilon: 1.0   steps: 166   lr: 0.0005   PER_beta: 0.4   reward MA: 1.14   mean loss: 0   mean max Q: nan\n",
      "episode: 182   frame: 27928   score: 0.0   memory length: 27018   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 1.08   mean loss: 0   mean max Q: nan\n",
      "episode: 183   frame: 28048   score: 0.0   memory length: 27133   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 1.08   mean loss: 0   mean max Q: nan\n",
      "episode: 184   frame: 28168   score: 0.0   memory length: 27248   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 1.06   mean loss: 0   mean max Q: nan\n",
      "episode: 185   frame: 28288   score: 0.0   memory length: 27363   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 1.0   mean loss: 0   mean max Q: nan\n",
      "episode: 186   frame: 28466   score: 2.0   memory length: 27536   epsilon: 1.0   steps: 178   lr: 0.0005   PER_beta: 0.4   reward MA: 1.04   mean loss: 0   mean max Q: nan\n",
      "episode: 187   frame: 28634   score: 1.0   memory length: 27699   epsilon: 1.0   steps: 168   lr: 0.0005   PER_beta: 0.4   reward MA: 1.04   mean loss: 0   mean max Q: nan\n",
      "episode: 188   frame: 28754   score: 0.0   memory length: 27814   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.98   mean loss: 0   mean max Q: nan\n",
      "episode: 189   frame: 29072   score: 4.0   memory length: 28127   epsilon: 1.0   steps: 318   lr: 0.0005   PER_beta: 0.4   reward MA: 1.04   mean loss: 0   mean max Q: nan\n",
      "episode: 190   frame: 29192   score: 0.0   memory length: 28242   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 1.02   mean loss: 0   mean max Q: nan\n",
      "episode: 191   frame: 29371   score: 2.0   memory length: 28416   epsilon: 1.0   steps: 179   lr: 0.0005   PER_beta: 0.4   reward MA: 1.04   mean loss: 0   mean max Q: nan\n",
      "episode: 192   frame: 29491   score: 0.0   memory length: 28531   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 1.02   mean loss: 0   mean max Q: nan\n",
      "episode: 193   frame: 29639   score: 1.0   memory length: 28674   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 1.0   mean loss: 0   mean max Q: nan\n",
      "episode: 194   frame: 29759   score: 0.0   memory length: 28789   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.96   mean loss: 0   mean max Q: nan\n",
      "episode: 195   frame: 29879   score: 0.0   memory length: 28904   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.96   mean loss: 0   mean max Q: nan\n",
      "episode: 196   frame: 30047   score: 1.0   memory length: 29067   epsilon: 1.0   steps: 168   lr: 0.0005   PER_beta: 0.4   reward MA: 0.92   mean loss: 0   mean max Q: nan\n",
      "episode: 197   frame: 30244   score: 2.0   memory length: 29259   epsilon: 1.0   steps: 197   lr: 0.0005   PER_beta: 0.4   reward MA: 0.94   mean loss: 0   mean max Q: nan\n",
      "episode: 198   frame: 30364   score: 0.0   memory length: 29374   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.92   mean loss: 0   mean max Q: nan\n",
      "episode: 199   frame: 30484   score: 0.0   memory length: 29489   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.9   mean loss: 0   mean max Q: nan\n",
      "episode: 200   frame: 30632   score: 1.0   memory length: 29632   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.88   mean loss: 0   mean max Q: nan\n",
      "episode: 201   frame: 30811   score: 2.0   memory length: 29806   epsilon: 1.0   steps: 179   lr: 0.0005   PER_beta: 0.4   reward MA: 0.92   mean loss: 0   mean max Q: nan\n",
      "episode: 202   frame: 31007   score: 2.0   memory length: 29997   epsilon: 1.0   steps: 196   lr: 0.0005   PER_beta: 0.4   reward MA: 0.94   mean loss: 0   mean max Q: nan\n",
      "episode: 203   frame: 31155   score: 1.0   memory length: 30140   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.96   mean loss: 0   mean max Q: nan\n",
      "episode: 204   frame: 31275   score: 0.0   memory length: 30255   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.94   mean loss: 0   mean max Q: nan\n",
      "episode: 205   frame: 31423   score: 1.0   memory length: 30398   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.96   mean loss: 0   mean max Q: nan\n",
      "episode: 206   frame: 31646   score: 3.0   memory length: 30616   epsilon: 1.0   steps: 223   lr: 0.0005   PER_beta: 0.4   reward MA: 1.0   mean loss: 0   mean max Q: nan\n",
      "episode: 207   frame: 31865   score: 2.0   memory length: 30830   epsilon: 1.0   steps: 219   lr: 0.0005   PER_beta: 0.4   reward MA: 1.02   mean loss: 0   mean max Q: nan\n",
      "episode: 208   frame: 31985   score: 0.0   memory length: 30945   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 1.0   mean loss: 0   mean max Q: nan\n",
      "episode: 209   frame: 32105   score: 0.0   memory length: 31060   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.96   mean loss: 0   mean max Q: nan\n",
      "episode: 210   frame: 32225   score: 0.0   memory length: 31175   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.94   mean loss: 0   mean max Q: nan\n",
      "episode: 211   frame: 32373   score: 1.0   memory length: 31318   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.96   mean loss: 0   mean max Q: nan\n",
      "episode: 212   frame: 32493   score: 0.0   memory length: 31433   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.96   mean loss: 0   mean max Q: nan\n",
      "episode: 213   frame: 32613   score: 0.0   memory length: 31548   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.92   mean loss: 0   mean max Q: nan\n",
      "episode: 214   frame: 32733   score: 0.0   memory length: 31663   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.9   mean loss: 0   mean max Q: nan\n",
      "episode: 215   frame: 32930   score: 2.0   memory length: 31855   epsilon: 1.0   steps: 197   lr: 0.0005   PER_beta: 0.4   reward MA: 0.94   mean loss: 0   mean max Q: nan\n",
      "episode: 216   frame: 33050   score: 0.0   memory length: 31970   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.94   mean loss: 0   mean max Q: nan\n",
      "episode: 217   frame: 33170   score: 0.0   memory length: 32085   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.9   mean loss: 0   mean max Q: nan\n",
      "episode: 218   frame: 33384   score: 2.0   memory length: 32294   epsilon: 1.0   steps: 214   lr: 0.0005   PER_beta: 0.4   reward MA: 0.86   mean loss: 0   mean max Q: nan\n",
      "episode: 219   frame: 33504   score: 0.0   memory length: 32409   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.82   mean loss: 0   mean max Q: nan\n",
      "episode: 220   frame: 33624   score: 0.0   memory length: 32524   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.8   mean loss: 0   mean max Q: nan\n",
      "episode: 221   frame: 33772   score: 1.0   memory length: 32667   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.82   mean loss: 0   mean max Q: nan\n",
      "episode: 222   frame: 33892   score: 0.0   memory length: 32782   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.76   mean loss: 0   mean max Q: nan\n",
      "episode: 223   frame: 34012   score: 0.0   memory length: 32897   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.76   mean loss: 0   mean max Q: nan\n",
      "episode: 224   frame: 34132   score: 0.0   memory length: 33012   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.76   mean loss: 0   mean max Q: nan\n",
      "episode: 225   frame: 34252   score: 0.0   memory length: 33127   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.76   mean loss: 0   mean max Q: nan\n",
      "episode: 226   frame: 34430   score: 2.0   memory length: 33300   epsilon: 1.0   steps: 178   lr: 0.0005   PER_beta: 0.4   reward MA: 0.76   mean loss: 0   mean max Q: nan\n",
      "episode: 227   frame: 34578   score: 1.0   memory length: 33443   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.78   mean loss: 0   mean max Q: nan\n",
      "episode: 228   frame: 34698   score: 0.0   memory length: 33558   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.76   mean loss: 0   mean max Q: nan\n",
      "episode: 229   frame: 34818   score: 0.0   memory length: 33673   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.74   mean loss: 0   mean max Q: nan\n",
      "episode: 230   frame: 35017   score: 2.0   memory length: 33867   epsilon: 1.0   steps: 199   lr: 0.0005   PER_beta: 0.4   reward MA: 0.74   mean loss: 0   mean max Q: nan\n",
      "episode: 231   frame: 35262   score: 3.0   memory length: 34107   epsilon: 1.0   steps: 245   lr: 0.0005   PER_beta: 0.4   reward MA: 0.78   mean loss: 0   mean max Q: nan\n",
      "episode: 232   frame: 35382   score: 0.0   memory length: 34222   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.78   mean loss: 0   mean max Q: nan\n",
      "episode: 233   frame: 35530   score: 1.0   memory length: 34365   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.8   mean loss: 0   mean max Q: nan\n",
      "episode: 234   frame: 35650   score: 0.0   memory length: 34480   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.8   mean loss: 0   mean max Q: nan\n",
      "episode: 235   frame: 35770   score: 0.0   memory length: 34595   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.8   mean loss: 0   mean max Q: nan\n",
      "episode: 236   frame: 35918   score: 1.0   memory length: 34738   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.78   mean loss: 0   mean max Q: nan\n",
      "episode: 237   frame: 36143   score: 3.0   memory length: 34958   epsilon: 1.0   steps: 225   lr: 0.0005   PER_beta: 0.4   reward MA: 0.82   mean loss: 0   mean max Q: nan\n",
      "episode: 238   frame: 36263   score: 0.0   memory length: 35073   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.82   mean loss: 0   mean max Q: nan\n",
      "episode: 239   frame: 36383   score: 0.0   memory length: 35188   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.74   mean loss: 0   mean max Q: nan\n",
      "episode: 240   frame: 36503   score: 0.0   memory length: 35303   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.74   mean loss: 0   mean max Q: nan\n",
      "episode: 241   frame: 36623   score: 0.0   memory length: 35418   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.7   mean loss: 0   mean max Q: nan\n",
      "episode: 242   frame: 36884   score: 4.0   memory length: 35674   epsilon: 1.0   steps: 261   lr: 0.0005   PER_beta: 0.4   reward MA: 0.78   mean loss: 0   mean max Q: nan\n",
      "episode: 243   frame: 37032   score: 1.0   memory length: 35817   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.78   mean loss: 0   mean max Q: nan\n",
      "episode: 244   frame: 37180   score: 1.0   memory length: 35960   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.8   mean loss: 0   mean max Q: nan\n",
      "episode: 245   frame: 37473   score: 4.0   memory length: 36248   epsilon: 1.0   steps: 293   lr: 0.0005   PER_beta: 0.4   reward MA: 0.88   mean loss: 0   mean max Q: nan\n",
      "episode: 246   frame: 37651   score: 2.0   memory length: 36421   epsilon: 1.0   steps: 178   lr: 0.0005   PER_beta: 0.4   reward MA: 0.9   mean loss: 0   mean max Q: nan\n",
      "episode: 247   frame: 37819   score: 1.0   memory length: 36584   epsilon: 1.0   steps: 168   lr: 0.0005   PER_beta: 0.4   reward MA: 0.88   mean loss: 0   mean max Q: nan\n",
      "episode: 248   frame: 37987   score: 1.0   memory length: 36747   epsilon: 1.0   steps: 168   lr: 0.0005   PER_beta: 0.4   reward MA: 0.9   mean loss: 0   mean max Q: nan\n",
      "episode: 249   frame: 38183   score: 2.0   memory length: 36938   epsilon: 1.0   steps: 196   lr: 0.0005   PER_beta: 0.4   reward MA: 0.94   mean loss: 0   mean max Q: nan\n",
      "episode: 250   frame: 38303   score: 0.0   memory length: 37053   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.92   mean loss: 0   mean max Q: nan\n",
      "episode: 251   frame: 38423   score: 0.0   memory length: 37168   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.88   mean loss: 0   mean max Q: nan\n",
      "episode: 252   frame: 38543   score: 0.0   memory length: 37283   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.84   mean loss: 0   mean max Q: nan\n",
      "episode: 253   frame: 38663   score: 0.0   memory length: 37398   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.82   mean loss: 0   mean max Q: nan\n",
      "episode: 254   frame: 38811   score: 1.0   memory length: 37541   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.84   mean loss: 0   mean max Q: nan\n",
      "episode: 255   frame: 38931   score: 0.0   memory length: 37656   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.82   mean loss: 0   mean max Q: nan\n",
      "episode: 256   frame: 39051   score: 0.0   memory length: 37771   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.76   mean loss: 0   mean max Q: nan\n",
      "episode: 257   frame: 39171   score: 0.0   memory length: 37886   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.72   mean loss: 0   mean max Q: nan\n",
      "episode: 258   frame: 39291   score: 0.0   memory length: 38001   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.72   mean loss: 0   mean max Q: nan\n",
      "episode: 259   frame: 39411   score: 0.0   memory length: 38116   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.72   mean loss: 0   mean max Q: nan\n",
      "episode: 260   frame: 39531   score: 0.0   memory length: 38231   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.72   mean loss: 0   mean max Q: nan\n",
      "episode: 261   frame: 39731   score: 2.0   memory length: 38426   epsilon: 1.0   steps: 200   lr: 0.0005   PER_beta: 0.4   reward MA: 0.74   mean loss: 0   mean max Q: nan\n",
      "episode: 262   frame: 39899   score: 1.0   memory length: 38589   epsilon: 1.0   steps: 168   lr: 0.0005   PER_beta: 0.4   reward MA: 0.76   mean loss: 0   mean max Q: nan\n",
      "episode: 263   frame: 40067   score: 1.0   memory length: 38752   epsilon: 1.0   steps: 168   lr: 0.0005   PER_beta: 0.4   reward MA: 0.78   mean loss: 0   mean max Q: nan\n",
      "episode: 264   frame: 40215   score: 1.0   memory length: 38895   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.8   mean loss: 0   mean max Q: nan\n",
      "episode: 265   frame: 40363   score: 1.0   memory length: 39038   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.78   mean loss: 0   mean max Q: nan\n",
      "episode: 266   frame: 40511   score: 1.0   memory length: 39181   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.8   mean loss: 0   mean max Q: nan\n",
      "episode: 267   frame: 40631   score: 0.0   memory length: 39296   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.8   mean loss: 0   mean max Q: nan\n",
      "episode: 268   frame: 40861   score: 3.0   memory length: 39521   epsilon: 1.0   steps: 230   lr: 0.0005   PER_beta: 0.4   reward MA: 0.82   mean loss: 0   mean max Q: nan\n",
      "episode: 269   frame: 41039   score: 2.0   memory length: 39694   epsilon: 1.0   steps: 178   lr: 0.0005   PER_beta: 0.4   reward MA: 0.86   mean loss: 0   mean max Q: nan\n",
      "episode: 270   frame: 41159   score: 0.0   memory length: 39809   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.86   mean loss: 0   mean max Q: nan\n",
      "episode: 271   frame: 41279   score: 0.0   memory length: 39924   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.84   mean loss: 0   mean max Q: nan\n",
      "episode: 272   frame: 41399   score: 0.0   memory length: 40039   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.84   mean loss: 0   mean max Q: nan\n",
      "episode: 273   frame: 41547   score: 1.0   memory length: 40182   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.86   mean loss: 0   mean max Q: nan\n",
      "episode: 274   frame: 41667   score: 0.0   memory length: 40297   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.86   mean loss: 0   mean max Q: nan\n",
      "episode: 275   frame: 41815   score: 1.0   memory length: 40440   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.88   mean loss: 0   mean max Q: nan\n",
      "episode: 276   frame: 41935   score: 0.0   memory length: 40555   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.84   mean loss: 0   mean max Q: nan\n",
      "episode: 277   frame: 42055   score: 0.0   memory length: 40670   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.82   mean loss: 0   mean max Q: nan\n",
      "episode: 278   frame: 42175   score: 0.0   memory length: 40785   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.82   mean loss: 0   mean max Q: nan\n",
      "episode: 279   frame: 42295   score: 0.0   memory length: 40900   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.82   mean loss: 0   mean max Q: nan\n",
      "episode: 280   frame: 42443   score: 1.0   memory length: 41043   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.8   mean loss: 0   mean max Q: nan\n",
      "episode: 281   frame: 42563   score: 0.0   memory length: 41158   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.74   mean loss: 0   mean max Q: nan\n",
      "episode: 282   frame: 42683   score: 0.0   memory length: 41273   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.74   mean loss: 0   mean max Q: nan\n",
      "episode: 283   frame: 42831   score: 1.0   memory length: 41416   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.74   mean loss: 0   mean max Q: nan\n",
      "episode: 284   frame: 42951   score: 0.0   memory length: 41531   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.74   mean loss: 0   mean max Q: nan\n",
      "episode: 285   frame: 43217   score: 4.0   memory length: 41792   epsilon: 1.0   steps: 266   lr: 0.0005   PER_beta: 0.4   reward MA: 0.82   mean loss: 0   mean max Q: nan\n",
      "episode: 286   frame: 43337   score: 0.0   memory length: 41907   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.8   mean loss: 0   mean max Q: nan\n",
      "episode: 287   frame: 43457   score: 0.0   memory length: 42022   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.74   mean loss: 0   mean max Q: nan\n",
      "episode: 288   frame: 43623   score: 1.0   memory length: 42183   epsilon: 1.0   steps: 166   lr: 0.0005   PER_beta: 0.4   reward MA: 0.76   mean loss: 0   mean max Q: nan\n",
      "episode: 289   frame: 43771   score: 1.0   memory length: 42326   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.78   mean loss: 0   mean max Q: nan\n",
      "episode: 290   frame: 43919   score: 1.0   memory length: 42469   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.8   mean loss: 0   mean max Q: nan\n",
      "episode: 291   frame: 44097   score: 2.0   memory length: 42642   epsilon: 1.0   steps: 178   lr: 0.0005   PER_beta: 0.4   reward MA: 0.84   mean loss: 0   mean max Q: nan\n",
      "episode: 292   frame: 44217   score: 0.0   memory length: 42757   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.76   mean loss: 0   mean max Q: nan\n",
      "episode: 293   frame: 44337   score: 0.0   memory length: 42872   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.74   mean loss: 0   mean max Q: nan\n",
      "episode: 294   frame: 44457   score: 0.0   memory length: 42987   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.72   mean loss: 0   mean max Q: nan\n",
      "episode: 295   frame: 44577   score: 0.0   memory length: 43102   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.64   mean loss: 0   mean max Q: nan\n",
      "episode: 296   frame: 44697   score: 0.0   memory length: 43217   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.6   mean loss: 0   mean max Q: nan\n",
      "episode: 297   frame: 44817   score: 0.0   memory length: 43332   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.58   mean loss: 0   mean max Q: nan\n",
      "episode: 298   frame: 44965   score: 1.0   memory length: 43475   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.58   mean loss: 0   mean max Q: nan\n",
      "episode: 299   frame: 45113   score: 1.0   memory length: 43618   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.56   mean loss: 0   mean max Q: nan\n",
      "episode: 300   frame: 45261   score: 1.0   memory length: 43761   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.58   mean loss: 0   mean max Q: nan\n",
      "episode: 301   frame: 45409   score: 1.0   memory length: 43904   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.6   mean loss: 0   mean max Q: nan\n",
      "episode: 302   frame: 45529   score: 0.0   memory length: 44019   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.6   mean loss: 0   mean max Q: nan\n",
      "episode: 303   frame: 45649   score: 0.0   memory length: 44134   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.6   mean loss: 0   mean max Q: nan\n",
      "episode: 304   frame: 45964   score: 4.0   memory length: 44444   epsilon: 1.0   steps: 315   lr: 0.0005   PER_beta: 0.4   reward MA: 0.66   mean loss: 0   mean max Q: nan\n",
      "episode: 305   frame: 46112   score: 1.0   memory length: 44587   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.68   mean loss: 0   mean max Q: nan\n",
      "episode: 306   frame: 46232   score: 0.0   memory length: 44702   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.68   mean loss: 0   mean max Q: nan\n",
      "episode: 307   frame: 46428   score: 2.0   memory length: 44893   epsilon: 1.0   steps: 196   lr: 0.0005   PER_beta: 0.4   reward MA: 0.72   mean loss: 0   mean max Q: nan\n",
      "episode: 308   frame: 46668   score: 4.0   memory length: 45128   epsilon: 1.0   steps: 240   lr: 0.0005   PER_beta: 0.4   reward MA: 0.8   mean loss: 0   mean max Q: nan\n",
      "episode: 309   frame: 46865   score: 2.0   memory length: 45320   epsilon: 1.0   steps: 197   lr: 0.0005   PER_beta: 0.4   reward MA: 0.84   mean loss: 0   mean max Q: nan\n",
      "episode: 310   frame: 46985   score: 0.0   memory length: 45435   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.84   mean loss: 0   mean max Q: nan\n",
      "episode: 311   frame: 47154   score: 1.0   memory length: 45599   epsilon: 1.0   steps: 169   lr: 0.0005   PER_beta: 0.4   reward MA: 0.82   mean loss: 0   mean max Q: nan\n",
      "episode: 312   frame: 47350   score: 2.0   memory length: 45790   epsilon: 1.0   steps: 196   lr: 0.0005   PER_beta: 0.4   reward MA: 0.84   mean loss: 0   mean max Q: nan\n",
      "episode: 313   frame: 47470   score: 0.0   memory length: 45905   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.82   mean loss: 0   mean max Q: nan\n",
      "episode: 314   frame: 47679   score: 3.0   memory length: 46109   epsilon: 1.0   steps: 209   lr: 0.0005   PER_beta: 0.4   reward MA: 0.86   mean loss: 0   mean max Q: nan\n",
      "episode: 315   frame: 47799   score: 0.0   memory length: 46224   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.84   mean loss: 0   mean max Q: nan\n",
      "episode: 316   frame: 47919   score: 0.0   memory length: 46339   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.82   mean loss: 0   mean max Q: nan\n",
      "episode: 317   frame: 48039   score: 0.0   memory length: 46454   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.82   mean loss: 0   mean max Q: nan\n",
      "episode: 318   frame: 48159   score: 0.0   memory length: 46569   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.76   mean loss: 0   mean max Q: nan\n",
      "episode: 319   frame: 48279   score: 0.0   memory length: 46684   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.72   mean loss: 0   mean max Q: nan\n",
      "episode: 320   frame: 48399   score: 0.0   memory length: 46799   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.72   mean loss: 0   mean max Q: nan\n",
      "episode: 321   frame: 48519   score: 0.0   memory length: 46914   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.72   mean loss: 0   mean max Q: nan\n",
      "episode: 322   frame: 48667   score: 1.0   memory length: 47057   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.74   mean loss: 0   mean max Q: nan\n",
      "episode: 323   frame: 48787   score: 0.0   memory length: 47172   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.72   mean loss: 0   mean max Q: nan\n",
      "episode: 324   frame: 48956   score: 1.0   memory length: 47336   epsilon: 1.0   steps: 169   lr: 0.0005   PER_beta: 0.4   reward MA: 0.74   mean loss: 0   mean max Q: nan\n",
      "episode: 325   frame: 49076   score: 0.0   memory length: 47451   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.72   mean loss: 0   mean max Q: nan\n",
      "episode: 326   frame: 49196   score: 0.0   memory length: 47566   epsilon: 1.0   steps: 120   lr: 0.0005   PER_beta: 0.4   reward MA: 0.72   mean loss: 0   mean max Q: nan\n",
      "episode: 327   frame: 49344   score: 1.0   memory length: 47709   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.74   mean loss: 0   mean max Q: nan\n",
      "episode: 328   frame: 49492   score: 1.0   memory length: 47852   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.76   mean loss: 0   mean max Q: nan\n",
      "episode: 329   frame: 49640   score: 1.0   memory length: 47995   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.78   mean loss: 0   mean max Q: nan\n",
      "episode: 330   frame: 49788   score: 1.0   memory length: 48138   epsilon: 1.0   steps: 148   lr: 0.0005   PER_beta: 0.4   reward MA: 0.78   mean loss: 0   mean max Q: nan\n",
      "episode: 331   frame: 49982   score: 2.0   memory length: 48327   epsilon: 1.0   steps: 194   lr: 0.0005   PER_beta: 0.4   reward MA: 0.82   mean loss: 0   mean max Q: nan\n",
      "Starting training\n",
      "Target network updated at frame:  50000\n",
      "episode: 1   frame: 50130   score: 1.0   memory length: 48470   epsilon: 0.99976   steps: 148   lr: 0.0005   PER_beta: 0.4001   reward MA: 0.84   mean loss: 0   mean max Q: 0.0862\n",
      "episode: 2   frame: 50250   score: 0.0   memory length: 48585   epsilon: 0.99955   steps: 120   lr: 0.0005   PER_beta: 0.4002   reward MA: 0.82   mean loss: 0.01191   mean max Q: 0.0433\n",
      "episode: 3   frame: 50398   score: 1.0   memory length: 48728   epsilon: 0.99928   steps: 148   lr: 0.0005   PER_beta: 0.40032   reward MA: 0.84   mean loss: 0.0031   mean max Q: 0.0417\n",
      "episode: 4   frame: 50546   score: 1.0   memory length: 48871   epsilon: 0.99902   steps: 148   lr: 0.0005   PER_beta: 0.40044   reward MA: 0.78   mean loss: 0.00363   mean max Q: 0.045\n",
      "episode: 5   frame: 50666   score: 0.0   memory length: 48986   epsilon: 0.9988   steps: 120   lr: 0.0005   PER_beta: 0.40053   reward MA: 0.78   mean loss: 0.0034   mean max Q: 0.0928\n",
      "episode: 6   frame: 50834   score: 1.0   memory length: 49149   epsilon: 0.9985   steps: 168   lr: 0.0005   PER_beta: 0.40067   reward MA: 0.8   mean loss: 0.00722   mean max Q: 0.3949\n",
      "episode: 7   frame: 50954   score: 0.0   memory length: 49264   epsilon: 0.99828   steps: 120   lr: 0.0005   PER_beta: 0.40076   reward MA: 0.78   mean loss: 0.00666   mean max Q: 0.7055\n",
      "episode: 8   frame: 51102   score: 1.0   memory length: 49407   epsilon: 0.99801   steps: 148   lr: 0.0005   PER_beta: 0.40088   reward MA: 0.78   mean loss: 0.00536   mean max Q: 0.7268\n",
      "episode: 9   frame: 51222   score: 0.0   memory length: 49522   epsilon: 0.9978   steps: 120   lr: 0.0005   PER_beta: 0.40098   reward MA: 0.76   mean loss: 0.00505   mean max Q: 0.8529\n",
      "episode: 10   frame: 51488   score: 3.0   memory length: 49783   epsilon: 0.99732   steps: 266   lr: 0.0005   PER_beta: 0.40119   reward MA: 0.78   mean loss: 0.0022   mean max Q: 0.7304\n",
      "episode: 11   frame: 51859   score: 6.0   memory length: 50149   epsilon: 0.99665   steps: 371   lr: 0.0005   PER_beta: 0.40149   reward MA: 0.9   mean loss: 0.00127   mean max Q: 0.8096\n",
      "episode: 12   frame: 52028   score: 1.0   memory length: 50313   epsilon: 0.99635   steps: 169   lr: 0.0005   PER_beta: 0.40162   reward MA: 0.92   mean loss: 0.00176   mean max Q: 0.7665\n",
      "episode: 13   frame: 52148   score: 0.0   memory length: 50428   epsilon: 0.99613   steps: 120   lr: 0.0005   PER_beta: 0.40172   reward MA: 0.92   mean loss: 0.00021   mean max Q: 0.764\n",
      "episode: 14   frame: 52296   score: 1.0   memory length: 50571   epsilon: 0.99587   steps: 148   lr: 0.0005   PER_beta: 0.40184   reward MA: 0.94   mean loss: 0.00019   mean max Q: 0.8241\n",
      "episode: 15   frame: 52416   score: 0.0   memory length: 50686   epsilon: 0.99565   steps: 120   lr: 0.0005   PER_beta: 0.40193   reward MA: 0.94   mean loss: 0.00019   mean max Q: 0.7751\n",
      "episode: 16   frame: 52536   score: 0.0   memory length: 50801   epsilon: 0.99543   steps: 120   lr: 0.0005   PER_beta: 0.40203   reward MA: 0.94   mean loss: 0.00029   mean max Q: 0.8116\n",
      "episode: 17   frame: 52704   score: 1.0   memory length: 50964   epsilon: 0.99513   steps: 168   lr: 0.0005   PER_beta: 0.40216   reward MA: 0.94   mean loss: 9e-05   mean max Q: 0.8142\n",
      "episode: 18   frame: 52824   score: 0.0   memory length: 51079   epsilon: 0.99491   steps: 120   lr: 0.0005   PER_beta: 0.40226   reward MA: 0.92   mean loss: 0.0001   mean max Q: 0.7456\n",
      "episode: 19   frame: 52944   score: 0.0   memory length: 51194   epsilon: 0.9947   steps: 120   lr: 0.0005   PER_beta: 0.40236   reward MA: 0.9   mean loss: 5e-05   mean max Q: 0.7812\n"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "\n",
    "set_seed(seed)\n",
    "run_num = \"17\"\n",
    "name = \"StdDQN_750Kfr_newest_get_frame_CirularPER_wIS\"\n",
    "run_name = \"Run\"+ str(run_num) + \"_\" + name\n",
    "\n",
    "\n",
    "from config import *\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# choose which Agent to use\n",
    "from agent import Agent\n",
    "\n",
    "#create fresh environment\n",
    "env = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=sticky_action_prob, full_action_space=False, render_mode='rgb_array')  # Use equivalent parameters to BreakoutDeterministic-v4\n",
    "\n",
    "# setup video recording\n",
    "def video_trigger(_):\n",
    "    if len(episodes) == 0:\n",
    "        return False\n",
    "    else:\n",
    "        curr_ep = episodes[-1]\n",
    "        return (curr_ep > 99 and curr_ep % 100 == 0) \n",
    "\n",
    "video_path = f\"./videos/run{run_num}\"\n",
    "if not os.path.exists(video_path):\n",
    "    os.makedirs(video_path)\n",
    "env = RecordVideo(env, video_folder=video_path, episode_trigger=video_trigger)\n",
    "\n",
    "\n",
    "print(f\"Starting run {run_name}\")\n",
    "train_interval = 1\n",
    "    \n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "rewards, episodes = [], []\n",
    "reward = 0\n",
    "best_eval_reward = curr_mean_reward = last_save_reward = 0\n",
    "\n",
    "#initialize trackers\n",
    "## LOSS TRACKERS ##\n",
    "losses_window = deque(maxlen=10)\n",
    "loss_history = []\n",
    "episode_losses = []\n",
    "loss = mean_loss = 0\n",
    "\n",
    "# ## Q-VALUE TRACKERS ##\n",
    "episode_q_means = []\n",
    "episode_q_maxs = []\n",
    "episode_q_mins = []\n",
    "q_mean_window = deque(maxlen=10)\n",
    "q_max_window = deque(maxlen=10)\n",
    "q_min_window = deque(maxlen=10)\n",
    "q_stats_history = {\n",
    "    'mean': [],\n",
    "    'max': [],\n",
    "    'min': [],\n",
    "    'episode': []\n",
    "}\n",
    "\n",
    "# Epsilon Bump Control Variables\n",
    "plateau_patience = 300\n",
    "episodes_since_improvement = 0\n",
    "epsilon_bump = 0.15  # amount to re-increase epsilon\n",
    "soonest_bump = 2000  # earliest episode to apply epsilon bump\n",
    "\n",
    "frame = 0\n",
    "ep_start = 0\n",
    "training_started_flg = False\n",
    "\n",
    "print(\"Instantiating agent\")\n",
    "# agent = Agent(action_size, mem_path)\n",
    "agent = Agent(action_size)\n",
    "\n",
    "#########################\n",
    "#### LOAD CHECKPOINT ####\n",
    "# metadata = agent.load_checkpoint(\"Run8_Stdized_DDQN_750K_frames\", 2999)  #Edit episode number\n",
    "# frame = metadata['global_frame']\n",
    "# agent.load_replay_buffer(\"Run8_Stdized_DDQN_750K_frames\", 711354)\n",
    "# ep_start = metadata['global_episode']\n",
    "# evaluation_reward = metadata['eval_rewards']\n",
    "# rewards = metadata['rewards']    \n",
    "# episodes = metadata['episodes']\n",
    "# losses_window = metadata['last_10_ep_losses']\n",
    "# loss_history = metadata['loss_tracker']\n",
    "# training_started_flg = True\n",
    "########################\n",
    "\n",
    "\n",
    "start_train_immediate = False\n",
    "frame_max = 50_000 + TRAINING_STEPS\n",
    "e = ep_start\n",
    "\n",
    "while e < EPISODES:\n",
    "    #limit number of frames for consistent testing\n",
    "    if frame >= frame_max:\n",
    "        break\n",
    "\n",
    "    done = False\n",
    "    score = 0\n",
    "    episode_losses = []\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state, _ = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "    fire_ready = True\n",
    "    no_reward_steps = 0\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        #limit number of frames for consitent testing\n",
    "        if frame >= frame_max:\n",
    "            break\n",
    "\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Selet Action (with robust check for FIRE action)\n",
    "        if fire_ready:  \n",
    "            next_state, force_done = reset_after_life_loss(env, history)\n",
    "            if force_done:\n",
    "                break\n",
    "            action = 1\n",
    "            fire_ready = False\n",
    "        else:\n",
    "            trainable_action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "            action = TRAINABLE_ACTIONS[trainable_action]\n",
    "\n",
    "        # Step the environment\n",
    "        state = next_state\n",
    "        next_state, reward, terminations, truncations, info = env.step(action)  \n",
    "        done = truncations or terminations\n",
    "        \n",
    "        # Failsafe to force reset if no reward for 3000 steps (prevents agent from getting stuck)\n",
    "        stuck_limit = 3000\n",
    "        if no_reward_steps > stuck_limit:\n",
    "            done = True\n",
    "            print(f\"[WARNING] No reward for {stuck_limit} steps, forcing reset | \", \"Episode:\", e, \"  Frame:\", frame, ) \n",
    "             \n",
    "        frame_next_state = new_get_frame(next_state)\n",
    "             \n",
    "        # append next state to history\n",
    "        history[4, :, :] = frame_next_state\n",
    "        \n",
    "        # life handling\n",
    "        lost_life = check_live(life, info['lives'])  #check if the agent has lost a life\n",
    "        if lost_life:\n",
    "            fire_ready = True\n",
    "        life = info['lives']\n",
    "        \n",
    "        r = reward\n",
    "        if r == 0:\n",
    "            no_reward_steps += 1\n",
    "        else:\n",
    "            no_reward_steps = 0 \n",
    "\n",
    "        # Store the transition in replay buffer if it was not a FIRE action\n",
    "        if action in TRAINABLE_ACTIONS:\n",
    "            trainable_index = TRAINABLE_ACTIONS.index(action)\n",
    "            term_state = done or lost_life\n",
    "            agent.memory.push(deepcopy(frame_next_state), trainable_index, r, term_state)\n",
    "        \n",
    "        # Start training after random sample generation\n",
    "        if frame == train_frame or (start_train_immediate and frame == 1):\n",
    "            print(\"Starting training\")\n",
    "            training_started_flg = True\n",
    "            e = ep_start  #reset episode counter when training starts\n",
    "        if(training_started_flg): \n",
    "            if frame % train_interval == 0: # Use adaptive training interval\n",
    "                loss, q_stats = agent.train_policy_net()\n",
    "                episode_losses.append(loss)\n",
    "                episode_q_means.append(q_stats['q_mean'])\n",
    "                episode_q_maxs.append(q_stats['q_max'])\n",
    "                episode_q_mins.append(q_stats['q_min'])\n",
    "            # Update the target network\n",
    "            if (frame % (train_interval * update_target_network_frequency)) == 0:\n",
    "                agent.update_target_net()\n",
    "                print(\"Target network updated at frame: \", frame)\n",
    "        \n",
    "        # Update score and history\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]  # shift history by one erasing oldest frame\n",
    "            \n",
    "        if done:\n",
    "            e += 1\n",
    "            fire_ready = True\n",
    "            evaluation_reward.append(score)            \n",
    "            \n",
    "\n",
    "            # print episode information every X episodes\n",
    "            if e % 1 == 0:\n",
    "                print(\"episode:\", e, \"  frame:\", frame, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", round(agent.epsilon, 5), \n",
    "                  \"  steps:\", step, \"  lr:\", agent.optimizer.param_groups[0]['lr'], \"  PER_beta:\", round(agent.beta,5),\n",
    "                  \"  reward MA:\", round(np.mean(evaluation_reward), 3), \n",
    "                  \"  mean loss:\", round(mean_loss, 5), \"  mean max Q:\", round(np.mean(episode_q_maxs), 4))\n",
    "                #   \"  latest step Q max:\", round(q_stats['q_max'], 4))\n",
    "\n",
    "\n",
    "            \n",
    "            if training_started_flg:\n",
    "\n",
    "                episodes.append(e)\n",
    "                rewards.append(np.mean(evaluation_reward))  # record moving average of last evaluation_reward_length episodes\n",
    "\n",
    "                # # adapt training interval to agent performance\n",
    "                # if np.mean(evaluation_reward) < 8:\n",
    "                #     train_interval = 4\n",
    "                # elif np.mean(evaluation_reward) < 15:\n",
    "                #     train_interval = 2\n",
    "                # else:\n",
    "                #     train_interval = 1\n",
    "\n",
    "                ## DEBUG ##\n",
    "                # Check TD-error distribution in Replay Buffer\n",
    "                if e>0 and e % 100 == 0:\n",
    "                    agent.memory.log_td_error_distribution()\n",
    "\n",
    "                # save rolling loss everages every X episodes\n",
    "                if episode_losses:\n",
    "                    mean_loss = sum(episode_losses) / len(episode_losses)\n",
    "                    losses_window.append(mean_loss)\n",
    "                    if e > 9 and e % 10 == 0:\n",
    "                        loss_history.append((np.mean(losses_window), e))\n",
    "\n",
    "                # save rolling Q-score stat averages\n",
    "                q_mean_window.append(np.mean(episode_q_means))\n",
    "                q_max_window.append(np.mean(episode_q_maxs))\n",
    "                q_min_window.append(np.mean(episode_q_mins))\n",
    "                q_stats_history['mean'].append(np.mean(q_mean_window))\n",
    "                q_stats_history['max'].append(np.mean(q_max_window))\n",
    "                q_stats_history['min'].append(np.mean(q_min_window))\n",
    "                q_stats_history['episode'].append(e)\n",
    "                episode_q_means = []\n",
    "                episode_q_maxs = []\n",
    "                episode_q_mins = []\n",
    "\n",
    "                # plot the rewards every X episodes\n",
    "                if e > 0 and e % 50 == 0:\n",
    "                    # clear_output(wait=True)\n",
    "                    pylab.plot(episodes, rewards, 'b')\n",
    "                    pylab.xlabel('Episode #')\n",
    "                    pylab.ylabel('Rolling Mean Episode Scores') \n",
    "                    pylab.title('DQN w PER & Cropped Scoreboard \\n Scores')\n",
    "                    plot_path = f\"./save_graph/run{run_num}/{run_name}_ep{e}_scores.png\"\n",
    "                    os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "                    pylab.savefig(plot_path)\n",
    "                    print(f\"[SAVED PLOT] {plot_path}\")\n",
    "                    pylab.show()\n",
    "                    pylab.close()\n",
    "                \n",
    "                # every X episodes, plot the mean losses\n",
    "                if e > 0 and e % 50 == 0:\n",
    "                    x = [entry[1] for entry in loss_history]\n",
    "                    y = [entry[0] for entry in loss_history]\n",
    "                    pylab.plot(x, y, 'r')\n",
    "                    pylab.xlabel('Episode #')\n",
    "                    pylab.ylabel('Rolling Mean Loss per Episode') \n",
    "                    pylab.title('DQN w PER & Cropped Scoreboard \\n Loss')\n",
    "                    plot_path = f\"./save_graph/run{run_num}/{run_name}_ep{e}_losses.png\"\n",
    "                    os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "                    pylab.savefig(plot_path)\n",
    "                    print(f\"[SAVED PLOT] {plot_path}\")\n",
    "                    pylab.show()\n",
    "                    pylab.close()\n",
    "\n",
    "                # plot Q-value statistics every X episodes\n",
    "                if e > 0 and e % 25 == 0:\n",
    "                    pylab.plot(q_stats_history['episode'], q_stats_history['mean'], label='Q-Mean')\n",
    "                    pylab.plot(q_stats_history['episode'], q_stats_history['max'], label='Q-Max')\n",
    "                    pylab.plot(q_stats_history['episode'], q_stats_history['min'], label='Q-Min')\n",
    "                    pylab.xlabel('Episode #')\n",
    "                    pylab.ylabel('Q-Value')\n",
    "                    pylab.title('Avg per Episode Q-Value Stats')\n",
    "                    pylab.legend(loc='upper left')\n",
    "                    plot_path = f\"./save_graph/run{run_num}/{run_name}_ep{e}_Qstats.png\"\n",
    "                    os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "                    pylab.savefig(plot_path)\n",
    "                    print(f\"[SAVED PLOT] {plot_path}\")\n",
    "                    pylab.show()\n",
    "                    pylab.close()\n",
    "\n",
    "\n",
    "                    ## DEBUG ##\n",
    "                    # print(f\"[PLOT DEBUG] Last 5 q_means: {q_stats_history['mean'][-5:]}\")\n",
    "\n",
    "                # Checkpoint the training process every X episodes \n",
    "                if e > 0 and e % 1000 == 0:\n",
    "                    metadata = create_metadata(agent, e, frame, evaluation_reward, rewards, episodes, losses_window, loss_history, agent.epsilon, agent.beta)\n",
    "                    agent.save_checkpoint(metadata, run_name, e)\n",
    "                if e > 0 and e % 1000 == 0:\n",
    "                    agent.save_replay_buffer(run_name, frame)\n",
    "\n",
    "                # Check if reward has improved\n",
    "                curr_mean_reward = np.mean(evaluation_reward)\n",
    "                if curr_mean_reward > best_eval_reward:\n",
    "                    best_eval_reward = curr_mean_reward\n",
    "                    episodes_since_improvement = 0\n",
    "                else:\n",
    "                    episodes_since_improvement += 1\n",
    "                \n",
    "                # save model if it is good\n",
    "                if curr_mean_reward > 8 and curr_mean_reward > (1.05 * last_save_reward):\n",
    "                    model_path = f\"./save_model/run{run_num}/good_{run_name}_{e}_eps.pth\"\n",
    "                    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "                    torch.save(agent.policy_net.state_dict(), model_path)\n",
    "                    print(f\"[SAVED MODEL] {model_path}\")\n",
    "                    last_save_reward = curr_mean_reward\n",
    "               \n",
    "                # # # Apply epsilon bump if plateauing\n",
    "                # if e > soonest_bump and episodes_since_improvement >= plateau_patience:\n",
    "                #     if agent.epsilon < agent.epsilon_max:\n",
    "                #         agent.epsilon = min(agent.epsilon + epsilon_bump, agent.epsilon_max)\n",
    "                #         print(f\"[BUMP] Epsilon bumped to {agent.epsilon:.4f} after {plateau_patience} stagnant episodes.\")\n",
    "                #     episodes_since_improvement = 0  # Reset counter after bump\n",
    "\n",
    "\n",
    "\n",
    "# Checkpoint the model at the end of training loop\n",
    "metadata = create_metadata(agent, e, frame, evaluation_reward, rewards, episodes, losses_window, loss_history, agent.epsilon, agent.beta)\n",
    "agent.save_checkpoint(metadata, run_name, e)\n",
    "agent.save_replay_buffer(run_name, frame)\n",
    "\n",
    "print(\"Training complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End logging\n",
    "sys.stdout = tee.ipython_stdout\n",
    "tee.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rbisk\\AppData\\Local\\Temp\\ipykernel_175372\\762919505.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ddqn_ckpt = torch.load(\"./checkpoints/Run12_DDQN_750K_frames_imprvdBatching_3007_checkpoint.pt\")\n",
      "C:\\Users\\rbisk\\AppData\\Local\\Temp\\ipykernel_175372\\762919505.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dqn_ckpt  = torch.load(\"./checkpoints/Run13_StdDQN_750K_frames_imprvdBatching_2673_checkpoint.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVED PLOT] ./presentation_assets/DQN_vs_DDQN_750K_steps_LOSSES.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# === Load checkpoint files ===\n",
    "ddqn_ckpt = torch.load(\"./checkpoints/Run12_DDQN_750K_frames_imprvdBatching_3007_checkpoint.pt\")\n",
    "dqn_ckpt  = torch.load(\"./checkpoints/Run13_StdDQN_750K_frames_imprvdBatching_2673_checkpoint.pt\")\n",
    "\n",
    "# === Extract metadata ===\n",
    "ddqn_meta = ddqn_ckpt[\"metadata\"]\n",
    "dqn_meta = dqn_ckpt[\"metadata\"]\n",
    "\n",
    "# # === Extract episode indices and rolling mean rewards ===\n",
    "# ddqn_episodes = ddqn_meta[\"episodes\"]\n",
    "# ddqn_rewards = ddqn_meta[\"rewards\"]\n",
    "\n",
    "# dqn_episodes = dqn_meta[\"episodes\"]\n",
    "# dqn_rewards = dqn_meta[\"rewards\"]\n",
    "\n",
    "# # === Plot ===\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(dqn_episodes, dqn_rewards, label=\"DQN\", color=\"red\")\n",
    "# plt.plot(ddqn_episodes, ddqn_rewards, label=\"DDQN\", color=\"blue\")\n",
    "# plt.xlabel(\"Episode #\", fontweight=\"bold\")\n",
    "# plt.ylabel(\"Mean Episode Score (Moving Avg)\", fontweight=\"bold\")\n",
    "# plt.legend(loc = \"upper left\")\n",
    "# # Main title\n",
    "# pylab.text(0.5, 1.05, 'Std DQN vs DDQN Scores',\n",
    "#            ha='center', va='bottom', transform=pylab.gca().transAxes,\n",
    "#            fontsize=14, fontweight='semibold')\n",
    "\n",
    "# # Subtitle (smaller font)\n",
    "# pylab.text(0.5, 1.01, '(750K training steps)',\n",
    "#            ha='center', va='bottom', transform=pylab.gca().transAxes,\n",
    "#            fontsize=10, color='gray')\n",
    "\n",
    "\n",
    "# plot_path = f\"./presentation_assets/DQN_vs_DDQN_750K_steps_SCORES.png\"\n",
    "# os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "# pylab.savefig(plot_path, bbox_inches='tight')\n",
    "# print(f\"[SAVED PLOT] {plot_path}\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # === Extract loss data ===\n",
    "ddqn_loss_history = ddqn_meta[\"loss_tracker\"]\n",
    "dqn_loss_history = dqn_meta[\"loss_tracker\"]\n",
    "\n",
    "ddqn_episodes = [entry[1] for entry in ddqn_loss_history]\n",
    "ddqn_losses = [entry[0] for entry in ddqn_loss_history]\n",
    "\n",
    "dqn_episodes = [entry[1] for entry in dqn_loss_history]\n",
    "dqn_losses = [entry[0] for entry in dqn_loss_history]\n",
    "\n",
    "# === Plot ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dqn_episodes, dqn_losses, label=\"DQN\", color=\"red\")\n",
    "plt.plot(ddqn_episodes, ddqn_losses, label=\"DDQN\", color=\"blue\")\n",
    "plt.xlabel(\"Episode #\", fontweight=\"bold\")\n",
    "plt.ylabel(\"Mean Episode Loss (Moving Avg)\", fontweight=\"bold\")\n",
    "plt.legend(loc = \"best\")\n",
    "# Main title\n",
    "pylab.text(0.5, 1.05, 'Std DQN vs DDQN Huber Loss',\n",
    "           ha='center', va='bottom', transform=pylab.gca().transAxes,\n",
    "           fontsize=14, fontweight='semibold')\n",
    "\n",
    "# Subtitle (smaller font)\n",
    "pylab.text(0.5, 1.01, '(750K training steps)',\n",
    "           ha='center', va='bottom', transform=pylab.gca().transAxes,\n",
    "           fontsize=10, color='gray')\n",
    "\n",
    "\n",
    "plot_path = f\"./presentation_assets/DQN_vs_DDQN_750K_steps_LOSSES.png\"\n",
    "os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "pylab.savefig(plot_path, bbox_inches='tight')\n",
    "print(f\"[SAVED PLOT] {plot_path}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model exported successfully!\n"
     ]
    }
   ],
   "source": [
    "# chkpt = torch.load(\"./checkpoints/Run12_DDQN_750K_frames_imprvdBatching_3007_checkpoint.pt\")\n",
    "from model import DQN_DualBranch\n",
    "action_size = len(TRAINABLE_ACTIONS)\n",
    "model = DQN_DualBranch(action_size)\n",
    "model.eval()\n",
    "\n",
    "#create dummy input for model\n",
    "dummy_input = torch.rand(BATCH_SIZE, 6, 84, 84)\n",
    "\n",
    "#export to ONNX\n",
    "torch.onnx.export(model, dummy_input, \"Branched Network.onnx\",\n",
    "                  input_names=[\"input\"], output_names=[\"output\"],\n",
    "                  dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "                  opset_version=11)\n",
    "\n",
    "print(\"ONNX model exported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timediff Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using a 4 frame history to train the model, here we will feed the model the 4 frames plus the \"diff\" between each of the 4 frames to help isolate the ball position which is the most important thing for the model to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play Game in Window or Save Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# set random seed to sync visual and recorded game\n",
    "seed = 55\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Load and setup Agent\n",
    "print(\"Instantiating agent\")\n",
    "agent = Agent(action_size)\n",
    "agent.load_policy_net(\"./save_model/run3/good_Run3_DDQN_Serial_InvTimeEpsilon_ddqn_886_eps.pth\")\n",
    "agent.policy_net.eval()\n",
    "agent.epsilon = 0  # Set agent to only exploit the best action\n",
    "\n",
    "\n",
    "# Create environments\n",
    "env_human = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=sticky_action_prob, full_action_space=False, render_mode=\"human\")\n",
    "# env_record = RecordVideo(env_record, video_folder=\"./videos\", episode_trigger=lambda e: True)\n",
    "\n",
    "# Reset to seed\n",
    "state_h, _ = env_human.reset(seed=seed)\n",
    "state_h = do_random_actions(env_human, 20)\n",
    "\n",
    "\n",
    "# Setup History\n",
    "history = np.zeros([HISTORY_SIZE + 1, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state_h, HISTORY_SIZE)\n",
    "\n",
    "done = False\n",
    "fire_ready = True\n",
    "life = number_lives\n",
    "score = 0\n",
    "step = 0\n",
    "\n",
    "while not done:\n",
    "    step += 1\n",
    "\n",
    "    # Select action\n",
    "    if fire_ready:\n",
    "        print(f\"[DEBUG] Agent is not acting — sending FIRE at step {step}\")\n",
    "        action = 1\n",
    "        fire_ready = False\n",
    "    else:\n",
    "        model_action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        action = TRAINABLE_ACTIONS[model_action]\n",
    "\n",
    "    # Step the environment\n",
    "    state_h, reward, term_h, trunc_h, info = env_human.step(action)\n",
    "    done = term_h or trunc_h\n",
    "\n",
    "    # update history\n",
    "    frame_next_state = get_frame(state_h)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    history[:4, :, :] = history[1:, :, :]  #shift history by one erasing oldest frame\n",
    "    \n",
    "    # check if life has been lost\n",
    "    lost_life = check_live(life, info['lives'])  #check if the agent has lost a life\n",
    "    if lost_life:\n",
    "        print(f\"[DEBUG] Lost life detected at step {step}\")\n",
    "        fire_ready = True\n",
    "    life = info['lives']\n",
    "    \n",
    "    # keep track of score\n",
    "    score += reward # update total score\n",
    "\n",
    "    \n",
    "env_human.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play Game Rendered in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display as ipythondisplay, clear_output\n",
    "\n",
    "def show_state_live(frame, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(frame)\n",
    "    plt.title(f\"Step: {step} {info}\")\n",
    "    plt.axis('off')\n",
    "    clear_output(wait=True)\n",
    "    ipythondisplay(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "\n",
    "# set random seed to sync visual and recorded game\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Load and setup Agent\n",
    "# Choose whether to use double DQN\n",
    "double_dqn = False # set to True if using double DQN agent\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "#Initialize agent\n",
    "print(\"Instantiating agent\")\n",
    "agent = Agent(action_size)\n",
    "agent.load_policy_net(\"./save_model/Run1_serial/good_breakout_dqn_1256_eps.pth\")\n",
    "agent.policy_net.eval()\n",
    "agent.epsilon = 0  # Set agent to only exploit the best action\n",
    "\n",
    "\n",
    "# Create environments\n",
    "env = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=sticky_action_prob, full_action_space=False, render_mode=\"rgb_array\")\n",
    "# Use RecordVideo to save the video to the \"videos\" directory\n",
    "# env = RecordVideo(env, video_folder=\"./videos\", episode_trigger=lambda episode_id: True)\n",
    "\n",
    "# Reset the environment\n",
    "state, _ = env.reset(seed=seed)\n",
    "\n",
    "# Setup History\n",
    "history = np.zeros([HISTORY_SIZE + 1, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "#Initialize variables\n",
    "step = 0\n",
    "done = False\n",
    "fire_ready = True\n",
    "life = number_lives\n",
    "score = 0\n",
    "\n",
    "while not done:\n",
    "\n",
    "    # Render the current frame live in the notebook\n",
    "    show_state_live(state, step)   \n",
    "\n",
    "    # Select action\n",
    "    if fire_ready:\n",
    "        action = 1\n",
    "        fire_ready = False\n",
    "    else:\n",
    "        model_action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        action = TRAINABLE_ACTIONS[model_action]\n",
    "\n",
    "    # Step the environment\n",
    "    next_state, reward, term, trunc, info = env.step(action)\n",
    "    done = term or trunc\n",
    "\n",
    "    # update history\n",
    "    frame_next_state = get_frame(state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    history[:4, :, :] = history[1:, :, :]  #shift history by one erasing oldest frame\n",
    "    \n",
    "    # check if life has been lost\n",
    "    lost_life = check_live(life, info['lives'])  #check if the agent has lost a life\n",
    "    if lost_life:\n",
    "        fire_ready = True  \n",
    "        # do_random_actions(env_human, 30) #IMPT: introduce randomness to game and paddle position before next life\n",
    "    life = info['lives']\n",
    "    \n",
    "    # keep track of score\n",
    "    score += reward # update total score\n",
    "\n",
    "    state = next_state\n",
    "    step += 1\n",
    "\n",
    "    \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating agent\n",
      "DQN Agent initialized\n",
      "No replay buffer path provided. Creating new empty buffer.\n",
      "episode: 0   seed: 765   score: 31.0   epsilon: 0.05   steps: 639\n",
      "episode: 1   seed: 817   score: 25.0   epsilon: 0.05   steps: 555\n",
      "episode: 2   seed: 53   score: 22.0   epsilon: 0.05   steps: 523\n",
      "episode: 3   seed: 705   score: 23.0   epsilon: 0.05   steps: 764\n",
      "episode: 4   seed: 990   score: 29.0   epsilon: 0.05   steps: 638\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 68\u001b[0m\n\u001b[0;32m     66\u001b[0m     fire_ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 68\u001b[0m     trainable_action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255.\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     action \u001b[38;5;241m=\u001b[39m TRAINABLE_ACTIONS[trainable_action]\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Step the environment\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\Dropbox\\GMU\\cs747 Deep Learning\\Final_Project\\Illinois_hw\\agent.py:75\u001b[0m, in \u001b[0;36mAgent.get_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     73\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 75\u001b[0m         q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(q_values, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(a)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\Dropbox\\GMU\\cs747 Deep Learning\\Final_Project\\Illinois_hw\\model.py:25\u001b[0m, in \u001b[0;36mDQN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)))\n\u001b[0;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Replaces .view(x.size(0), -1)\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from config import *\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "\n",
    "# Load and setup Agent\n",
    "# Choose which agent class to load\n",
    "from agent import Agent\n",
    "# from agent_timediff import Agent\n",
    "\n",
    "# 50 unique, fixed seeds\n",
    "seeds = [765,817,53,705,990,511,236,661,654,418,804,968,1,749,125,293,985,574, \n",
    "         447,948,687,317,280,645,927,842,309,616,717,930,778,323,595,798,195,11,\n",
    "         483,316,690,951,196,307,906,558,516,844,410,965,371,886]\n",
    "\n",
    "\n",
    "#Initialize agent\n",
    "print(\"Instantiating agent\")\n",
    "agent = Agent(action_size)\n",
    "# agent.load_policy_net(\"./save_model/run12/good_Run12_DDQN_750K_frames_imprvdBatching_2319_eps.pth\")\n",
    "agent.load_policy_net(\"./save_model/run13/good_Run13_StdDQN_750K_frames_imprvdBatching_2477_eps.pth\")\n",
    "# agent.load_policy_net(\"./save_model/run14/good_Run14_StdDQN_750Kfr_timediff_new_get_frame_2425_eps.pth\")\n",
    "agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "agent.target_net.eval()\n",
    "agent.policy_net.eval()\n",
    "agent.epsilon = 0.05  # Set agent to use model action 95% of the time for robustness\n",
    "\n",
    "\n",
    "# Create environments\n",
    "env = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=0.0, full_action_space=False, render_mode=\"rgb_array\")\n",
    "# Use RecordVideo to save the video to the \"videos\" directory\n",
    "# video_path = f\"./videos/testing/run13\"\n",
    "# if not os.path.exists(video_path):\n",
    "#     os.makedirs(video_path)\n",
    "# env = RecordVideo(env, video_folder=video_path, episode_trigger=lambda e: True)\n",
    "\n",
    "\n",
    "scores = []\n",
    "\n",
    "for e, seed in enumerate(seeds):\n",
    "    set_seed(seed)\n",
    "    # Reset the environment\n",
    "    state, _ = env.reset(seed=seed)\n",
    "\n",
    "    # Setup History\n",
    "    history = np.zeros([HISTORY_SIZE + 1, 84, 84], dtype=np.uint8)\n",
    "    get_init_state(history, state, HISTORY_SIZE)  #non cropped version\n",
    "    # new_get_init_state(history, state, HISTORY_SIZE)  #cropped version\n",
    "\n",
    "    #Initialize variables\n",
    "    step = 0\n",
    "    done = False\n",
    "    fire_ready = True\n",
    "    life = number_lives\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        # Selet Action (with robust check for FIRE action)\n",
    "        if fire_ready:\n",
    "            next_state, force_done = reset_after_life_loss(env, history)\n",
    "            if force_done:\n",
    "                break\n",
    "            action = 1\n",
    "            fire_ready = False\n",
    "        else:\n",
    "            trainable_action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "            action = TRAINABLE_ACTIONS[trainable_action]\n",
    "\n",
    "        # Step the environment\n",
    "    \n",
    "        next_state, reward, term, trunc, info = env.step(action)\n",
    "        done = term or trunc\n",
    "        # update total score\n",
    "        score += reward \n",
    "\n",
    "        # update history\n",
    "        history[4, :, :] = get_frame(next_state)  #non cropped version\n",
    "        # history[4, :, :] = new_get_frame(next_state)  #cropped version\n",
    "        history[:4, :, :] = history[1:, :, :]  #shift history by one erasing oldest frame\n",
    "        \n",
    "        # check if life has been lost\n",
    "        lost_life = check_live(life, info['lives'])  #check if the agent has lost a life\n",
    "        if lost_life:\n",
    "            fire_ready = True  \n",
    "        life = info['lives']\n",
    "        \n",
    "        \n",
    "        step += 1\n",
    "    \n",
    "    print(\"episode:\", e, \"  seed:\", seed, \"  score:\", score, \"  epsilon:\", round(agent.epsilon, 5), \n",
    "    \"  steps:\", step)\n",
    "    \n",
    "    scores.append(score)\n",
    "\n",
    "env.close()\n",
    "print(\"Mean Score: \", np.mean(scores))\n",
    "print(\"Std Score: \", np.std(scores))\n",
    "print(\"Max Score: \", np.max(scores))\n",
    "print(\"Min Score: \", np.min(scores))\n",
    "print(\"Median Score: \", np.median(scores))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=0.0, full_action_space=False)\n",
    "env.reset()\n",
    "next_state, reward, terminations, truncations, info = env.step(torch.tensor([[1]]))\n",
    "done = truncations or terminations\n",
    "print(\"reward: \", reward)\n",
    "print(\"done: \", done)\n",
    "print(\"info: \", info)\n",
    "print(next_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)\n",
    "print(\"Expected type:\", type(env.action_space.sample()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"agent memory type: \", type(agent.memory.memory[0]))\n",
    "print(\"agent memory[0]: \", agent.memory.memory[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Cell 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All checks passed — valid_flags == valid_indices through all pushes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import memory as mem\n",
    "\n",
    "# Use a small capacity for easy debugging\n",
    "capacity = 10\n",
    "history_size = 3\n",
    "memory = mem.CircularReplayMemoryPER(capacity, history_size)\n",
    "\n",
    "# Push 30 fake transitions (will cause multiple wraps)\n",
    "for step in range(30):\n",
    "    frame = np.ones((84, 84), dtype=np.uint8) * step\n",
    "    action = random.randint(0, 3)\n",
    "    reward = random.random()\n",
    "    done = (step % 7 == 0)  # Random done flag every few steps\n",
    "\n",
    "    memory.push(frame, action, reward, done)\n",
    "\n",
    "    num_flags = sum(memory.valid_flags)\n",
    "    num_indices = len(memory.valid_indices)\n",
    "\n",
    "    if num_flags != num_indices:\n",
    "        print(f\"❌ Mismatch at step {step}:\")\n",
    "        print(f\"   valid_flags count = {num_flags}\")\n",
    "        print(f\"   valid_indices count = {num_indices}\")\n",
    "        print(f\"   position = {memory.position}\")\n",
    "        print(f\"   size = {memory.size}\")\n",
    "        print(f\"   flags: {memory.valid_flags}\")\n",
    "        print(f\"   valid_indices: {memory.valid_indices}\")\n",
    "        raise AssertionError(\"valid_flags and valid_indices out of sync\")\n",
    "\n",
    "print(\"✅ All checks passed — valid_flags == valid_indices through all pushes.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
