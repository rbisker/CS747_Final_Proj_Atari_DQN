{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install gym pyvirtualdisplay\n",
    "# !sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install --upgrade setuptools --user\n",
    "# !pip3 install ez_setup \n",
    "# !pip3 install gym[atari] \n",
    "# !pip3 install gym[accept-rom-license] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "11.8\n",
      "CUDA device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if CUDA is available\n",
    "print(torch.version.cuda)  # Should print 11.8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"CUDA device: {device}\")  # Expected output: \"cuda:0\" if GPU is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "# import gym\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from model import *\n",
    "from config import *\n",
    "from checkpoint import *\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create metadata for checkpointing puposes\n",
    "def create_metadata(agent, global_episode, global_frame, eval_rewards, rewards, episodes, last_10_ep_losses, loss_tracker, epsilon, q_stats_hist, qmean_win, qmin_win, qmax_win, beta=None, alpha=None):\n",
    "    return {\n",
    "    'global_episode': global_episode,\n",
    "    'global_frame': global_frame,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'sch_gamma': scheduler_gamma,\n",
    "    'sch_step_size': scheduler_step_size,\n",
    "    'tgt_update_freq': update_target_network_frequency,\n",
    "    'memory capacity': Memory_capacity,  \n",
    "    'explore steps': EXPLORE_STEPS, \n",
    "    'epsilon_decay_rate': agent.epsilon_decay_rate,\n",
    "    'sticky_action_prob': sticky_action_prob,\n",
    "    'eval_rewards': eval_rewards,\n",
    "    'rewards': rewards,\n",
    "    'episodes': episodes,\n",
    "    'last_10_ep_losses': last_10_ep_losses,\n",
    "    'loss_tracker': loss_tracker,\n",
    "    'epislon': epsilon,\n",
    "    'q_stats_hist': q_stats_hist,\n",
    "    'qmean_win': qmean_win,\n",
    "    'qmin_win': qmin_win,\n",
    "    'qmax_win': qmax_win,\n",
    "    'peralpha': alpha,\n",
    "    'per_beta': beta,\n",
    "    'lr': agent.optimizer.get_last_lr(),\n",
    "    'scheduler_step': agent.scheduler.last_epoch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "class TeeLogger:\n",
    "    def __init__(self, filepath):\n",
    "        self.original_stdout = sys.__stdout__  # raw terminal (useful fallback)\n",
    "        self.ipython_stdout = sys.stdout       # the notebook's visible output\n",
    "        self.log = open(filepath, \"w\")\n",
    "\n",
    "    def write(self, message):\n",
    "        self.ipython_stdout.write(message)\n",
    "        self.log.write(message)\n",
    "        self.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        self.ipython_stdout.flush()\n",
    "        self.log.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the UIlliniois assigment this code is based on, I will be using the Gymnasium (https://github.com/farama-Foundation/gymnasium) and the Atari Learning Environment (ALE, link here: https://ale.farama.org/), rather than gym, which has been deprecated.  We will still be playing Breakout on Atari.\n",
    "\n",
    "To replecate BreakoutDeterministic-v4, as a starting point we'll use ALE/Breakout-v5 with frameskip=4, no \"sticky actions\" (i.e. deterministic actions), and a limited action space of NOOP, FIRE, LEFT, RIGHT.  However we won't be training on FIRE, instead we will ensure FIRE only happens when the game first starts and when a ball/life is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.0+dfae0bd)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('BreakoutDeterministic-v4')\n",
    "env = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=0.0, full_action_space=False)  # Use equivalent parameters to BreakoutDeterministic-v4\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Env Frame Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(185, 160, 3)\n",
      "height:  185 width:  160\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 160x185 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create environments\n",
    "env = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=sticky_action_prob, full_action_space=False, render_mode=\"rgb_array\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "frame = env.render()\n",
    "frame = frame[20:-5, :, :]\n",
    "\n",
    "# Calculate figure size in inches to match pixel size\n",
    "height, width = frame.shape[:2]\n",
    "print(frame.shape)\n",
    "print(\"height: \", height, \"width: \", width)\n",
    "dpi = plt.rcParams['figure.dpi']\n",
    "figsize = (width / dpi, height / dpi)\n",
    "\n",
    "# Plot with exact pixel size\n",
    "plt.figure(figsize=figsize, dpi=dpi)\n",
    "plt.imshow(frame)\n",
    "plt.title(\"Raw Render Output (Exact Pixel Dimensions)\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #noop, left, and right.  Fire ball (action 1) is not trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Replay Memory Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # # ### Pickle Replay Creator ###\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from config import *\n",
    "\n",
    "\n",
    "mem_name = 'CircularPERBuffer_for_testing_using_Run13_orig_get_frame'\n",
    "\n",
    "#create fresh environment\n",
    "env = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=sticky_action_prob, full_action_space=False, render_mode='rgb_array')\n",
    "# env = RecordVideo(env, video_folder=\"./videos\", episode_trigger=lambda ep: ep % 20 == 0)  \n",
    "\n",
    "# Choose whether to use double DQN\n",
    "double_dqn = False # set to True if using double DQN agent\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "# print(\"Instantiating agent\")\n",
    "agent = Agent(action_size)\n",
    "agent.load_policy_net(\"./save_model/run13/good_Run13_StdDQN_750K_frames_imprvdBatching_2477_eps.pth\")\n",
    "agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "agent.epsilon = 0.1\n",
    "\n",
    "frame = 0   \n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "exit_flag = False\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state, _ = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "    fire_ready = True\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Selet Action (with robust check for FIRE action)\n",
    "        if fire_ready:\n",
    "            next_state, force_done = reset_after_life_loss(env, history)\n",
    "            if force_done:\n",
    "                break\n",
    "            action = 1\n",
    "            fire_ready = False\n",
    "        else:\n",
    "            trainable_action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "            action = TRAINABLE_ACTIONS[trainable_action]\n",
    "\n",
    "        state = next_state\n",
    "        next_state, reward, terminations, truncations, info = env.step(action)  \n",
    "        done = truncations or terminations\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        \n",
    "\n",
    "        history[4, :, :] = frame_next_state\n",
    "        lost_life = check_live(life, info['lives'])  #check if the agent has lost a life\n",
    "\n",
    "        if lost_life:\n",
    "            fire_ready = True\n",
    "\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory if it was not a FIRE action\n",
    "        if action in TRAINABLE_ACTIONS:\n",
    "            trainable_index = TRAINABLE_ACTIONS.index(action)\n",
    "            term_state = done or lost_life\n",
    "            if type(agent.memory).__name__ == \"CircularReplayMemoryPER\":\n",
    "                agent.memory.push(agent, deepcopy(frame_next_state), trainable_index, r, term_state)\n",
    "            else:\n",
    "                agent.memory.push(deepcopy(frame_next_state), trainable_index, r, term_state)\n",
    "        # When replay buffer is filled save to pickle and break\n",
    "        if len(agent.memory) == train_frame:\n",
    "            print(f\"Memory filled, saving pickle file\") \n",
    "            agent.save_replay_buffer(mem_name, frame)\n",
    "            exit_flag = True\n",
    "            break \n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]  # shift history by one erasing oldest frame\n",
    "\n",
    "        if frame % 500 == 0:\n",
    "            print(\"DEBUG: len(valid_flags), len(valid_indices), len(td_errors):\", sum(agent.memory.valid_flags), len(agent.memory.valid_indices), len(agent.memory.td_errors))\n",
    "\n",
    "        if done:\n",
    "            fire_ready = True\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))  # record moving average of last evaluation_reward_length episodes\n",
    "            episodes.append(e)\n",
    "\n",
    "            # print episode information \n",
    "            if e % 1 == 0:\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"  steps:\", step,\n",
    "                  \"  lr:\", agent.optimizer.param_groups[0]['lr'], \n",
    "                  \"  evaluation reward:\", np.mean(evaluation_reward))\n",
    "    if exit_flag:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start logging\n",
    "# log_file = \"./test_log.txt\"\n",
    "# tee = TeeLogger(log_file)\n",
    "# sys.stdout = tee\n",
    "# sys.stderr = tee\n",
    "\n",
    "# print(f\"Logging started. Output will be written to both notebook and {log_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stop logging safely\n",
    "# sys.stdout = tee.original_stdout\n",
    "# sys.stderr = tee.original_stdout\n",
    "# tee.close()\n",
    "\n",
    "# print(\"This goes only to the notebook now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup Run Name and Memory Paths ##\n",
    "mem_path = None\n",
    "mem_path = './checkpoints/CircularPERBuffer_for_testing_using_Run13_orig_get_frame_50498_replay_buffer.pkl'\n",
    "# mem_path = './checkpoints/Run1_DQN_Serial_20pctExplore_and_Sticky_100000_replay_buffer.pkl'\n",
    "# mem_path = './checkpoints/Buffer_for_testing_using_Run5_50K_replay_buffer.pkl'\n",
    "# mem_path = './checkpoints/Buffer_for_testing_using_Run13_new_get_frame_100974_replay_buffer.pkl'\n",
    "seed = 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dir = f\"./logs/run{run_num}\"\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# log_file = os.path.join(log_dir, f\"{run_name}_output.log\")\n",
    "# tee = TeeLogger(log_file)\n",
    "# sys.stdout = tee\n",
    "# sys.stderr = tee\n",
    "\n",
    "# print(f\"Logging started. Output will be written to both notebook and {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rbisker/miniconda3/envs/wsl_gym/lib/python3.12/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /mnt/c/Users/rbisk/Dropbox/GMU/cs747 Deep Learning/Final_Project/Illinois_hw/videos/run18 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/tmp/ipykernel_225468/2806344182.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('./checkpoints/Run17_StdDQN_750Kfr_newest_get_frame_CirularPER_wIS_2000_checkpoint.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run Run18_Bootstrapped_run17_stickyactions5percent\n",
      "Instantiating agent\n",
      "Replay buffer loaded from './checkpoints/Run17_StdDQN_750Kfr_newest_get_frame_CirularPER_wIS_159356_replay_buffer.pkl' with size 204356\n",
      "Updated memory capacity to  1000000\n",
      "[SANITIZER] Fixing 0 invalid or nonpositive TD-errors...\n",
      "Starting training\n",
      "episode: 1   frame: 120   score: 0.0   memory length: 204471   epsilon: 0.99977   steps: 120   lr: 0.0005   PER_beta: 0.4001 PER Alpha: 0.5999   reward MA: 0.0   mean loss: 0   mean max Q: 8.3933\n",
      "episode: 2   frame: 268   score: 1.0   memory length: 204614   epsilon: 0.99949   steps: 148   lr: 0.0005   PER_beta: 0.40021 PER Alpha: 0.59979   reward MA: 0.5   mean loss: 0.03566   mean max Q: 8.2761\n",
      "episode: 3   frame: 388   score: 0.0   memory length: 204729   epsilon: 0.99926   steps: 120   lr: 0.0005   PER_beta: 0.40031 PER Alpha: 0.59969   reward MA: 0.333   mean loss: 0.03111   mean max Q: 8.6344\n",
      "episode: 4   frame: 556   score: 1.0   memory length: 204892   epsilon: 0.99894   steps: 168   lr: 0.0005   PER_beta: 0.40044 PER Alpha: 0.59956   reward MA: 0.5   mean loss: 0.03187   mean max Q: 8.4382\n",
      "episode: 5   frame: 676   score: 0.0   memory length: 205007   epsilon: 0.99872   steps: 120   lr: 0.0005   PER_beta: 0.40054 PER Alpha: 0.59946   reward MA: 0.4   mean loss: 0.03515   mean max Q: 8.7351\n",
      "episode: 6   frame: 796   score: 0.0   memory length: 205122   epsilon: 0.99849   steps: 120   lr: 0.0005   PER_beta: 0.40064 PER Alpha: 0.59936   reward MA: 0.333   mean loss: 0.03199   mean max Q: 8.5357\n",
      "episode: 7   frame: 916   score: 0.0   memory length: 205237   epsilon: 0.99826   steps: 120   lr: 0.0005   PER_beta: 0.40073 PER Alpha: 0.59927   reward MA: 0.286   mean loss: 0.02991   mean max Q: 8.4601\n",
      "[PER STATS] TD-error mean: 0.06773762069908262 std: 0.13770632873690042 min: 0.0 max: 3.46875\n",
      "episode: 8   frame: 1036   score: 0.0   memory length: 205352   epsilon: 0.99803   steps: 120   lr: 0.0005   PER_beta: 0.40083 PER Alpha: 0.59917   reward MA: 0.25   mean loss: 0.03178   mean max Q: 8.5535\n",
      "episode: 9   frame: 1202   score: 1.0   memory length: 205513   epsilon: 0.99772   steps: 166   lr: 0.0005   PER_beta: 0.40096 PER Alpha: 0.59904   reward MA: 0.333   mean loss: 0.03117   mean max Q: 8.5081\n",
      "episode: 10   frame: 1350   score: 1.0   memory length: 205656   epsilon: 0.99744   steps: 148   lr: 0.0005   PER_beta: 0.40108 PER Alpha: 0.59892   reward MA: 0.4   mean loss: 0.02785   mean max Q: 8.5452\n",
      "episode: 11   frame: 1470   score: 0.0   memory length: 205771   epsilon: 0.99721   steps: 120   lr: 0.0005   PER_beta: 0.40118 PER Alpha: 0.59882   reward MA: 0.364   mean loss: 0.03112   mean max Q: 8.6826\n",
      "episode: 12   frame: 1590   score: 0.0   memory length: 205886   epsilon: 0.99698   steps: 120   lr: 0.0005   PER_beta: 0.40127 PER Alpha: 0.59873   reward MA: 0.333   mean loss: 0.03039   mean max Q: 8.6189\n",
      "episode: 13   frame: 1710   score: 0.0   memory length: 206001   epsilon: 0.99675   steps: 120   lr: 0.0005   PER_beta: 0.40137 PER Alpha: 0.59863   reward MA: 0.308   mean loss: 0.03196   mean max Q: 8.6239\n",
      "episode: 14   frame: 1985   score: 4.0   memory length: 206271   epsilon: 0.99623   steps: 275   lr: 0.0005   PER_beta: 0.40159 PER Alpha: 0.59841   reward MA: 0.571   mean loss: 0.02781   mean max Q: 8.6986\n",
      "[PER STATS] TD-error mean: 0.06876462247131052 std: 0.13514624452939805 min: 0.0 max: 3.3671875\n",
      "episode: 15   frame: 2105   score: 0.0   memory length: 206386   epsilon: 0.996   steps: 120   lr: 0.0005   PER_beta: 0.40168 PER Alpha: 0.59832   reward MA: 0.533   mean loss: 0.0299   mean max Q: 8.8324\n",
      "episode: 16   frame: 2323   score: 2.0   memory length: 206599   epsilon: 0.99559   steps: 218   lr: 0.0005   PER_beta: 0.40186 PER Alpha: 0.59814   reward MA: 0.625   mean loss: 0.02834   mean max Q: 8.7304\n",
      "episode: 17   frame: 2583   score: 4.0   memory length: 206854   epsilon: 0.99509   steps: 260   lr: 0.0005   PER_beta: 0.40207 PER Alpha: 0.59793   reward MA: 0.824   mean loss: 0.02798   mean max Q: 8.7838\n",
      "episode: 18   frame: 2752   score: 1.0   memory length: 207018   epsilon: 0.99477   steps: 169   lr: 0.0005   PER_beta: 0.4022 PER Alpha: 0.5978   reward MA: 0.833   mean loss: 0.02536   mean max Q: 8.8601\n",
      "episode: 19   frame: 2872   score: 0.0   memory length: 207133   epsilon: 0.99454   steps: 120   lr: 0.0005   PER_beta: 0.4023 PER Alpha: 0.5977   reward MA: 0.789   mean loss: 0.02749   mean max Q: 8.4454\n",
      "[PER STATS] TD-error mean: 0.06872183321250135 std: 0.13107896352292242 min: 0.0 max: 3.74609375\n",
      "episode: 20   frame: 3069   score: 2.0   memory length: 207325   epsilon: 0.99417   steps: 197   lr: 0.0005   PER_beta: 0.40246 PER Alpha: 0.59754   reward MA: 0.85   mean loss: 0.02602   mean max Q: 8.6158\n",
      "episode: 21   frame: 3235   score: 1.0   memory length: 207486   epsilon: 0.99385   steps: 166   lr: 0.0005   PER_beta: 0.40259 PER Alpha: 0.59741   reward MA: 0.857   mean loss: 0.02487   mean max Q: 8.6816\n",
      "episode: 22   frame: 3383   score: 1.0   memory length: 207629   epsilon: 0.99357   steps: 148   lr: 0.0005   PER_beta: 0.40271 PER Alpha: 0.59729   reward MA: 0.864   mean loss: 0.02404   mean max Q: 8.7571\n",
      "episode: 23   frame: 3503   score: 0.0   memory length: 207744   epsilon: 0.99334   steps: 120   lr: 0.0005   PER_beta: 0.4028 PER Alpha: 0.5972   reward MA: 0.826   mean loss: 0.02587   mean max Q: 8.869\n",
      "episode: 24   frame: 3623   score: 0.0   memory length: 207859   epsilon: 0.99312   steps: 120   lr: 0.0005   PER_beta: 0.4029 PER Alpha: 0.5971   reward MA: 0.792   mean loss: 0.02451   mean max Q: 8.4266\n",
      "episode: 25   frame: 3771   score: 1.0   memory length: 208002   epsilon: 0.99284   steps: 148   lr: 0.0005   PER_beta: 0.40302 PER Alpha: 0.59698   reward MA: 0.8   mean loss: 0.02395   mean max Q: 8.5229\n",
      "[SAVED PLOT] ./save_graph/run18/Run18_Bootstrapped_run17_stickyactions5percent_ep25_Qstats.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PER STATS] TD-error mean: 0.06878996214921082 std: 0.1293453911204899 min: 0.0 max: 3.138671875\n",
      "episode: 26   frame: 4029   score: 4.0   memory length: 208255   epsilon: 0.99234   steps: 258   lr: 0.0005   PER_beta: 0.40322 PER Alpha: 0.59678   reward MA: 0.923   mean loss: 0.02405   mean max Q: 8.7214\n",
      "episode: 27   frame: 4281   score: 4.0   memory length: 208502   epsilon: 0.99187   steps: 252   lr: 0.0005   PER_beta: 0.40342 PER Alpha: 0.59658   reward MA: 1.037   mean loss: 0.02485   mean max Q: 8.7599\n",
      "episode: 28   frame: 4401   score: 0.0   memory length: 208617   epsilon: 0.99164   steps: 120   lr: 0.0005   PER_beta: 0.40352 PER Alpha: 0.59648   reward MA: 1.0   mean loss: 0.02389   mean max Q: 8.7814\n",
      "episode: 29   frame: 4645   score: 3.0   memory length: 208856   epsilon: 0.99117   steps: 244   lr: 0.0005   PER_beta: 0.40372 PER Alpha: 0.59628   reward MA: 1.069   mean loss: 0.02544   mean max Q: 8.5355\n",
      "episode: 30   frame: 4793   score: 1.0   memory length: 208999   epsilon: 0.99089   steps: 148   lr: 0.0005   PER_beta: 0.40383 PER Alpha: 0.59617   reward MA: 1.067   mean loss: 0.02432   mean max Q: 8.5374\n",
      "Target network updated at frame:  5000\n",
      "[PER STATS] TD-error mean: 0.06849430492758565 std: 0.12667454402434428 min: 0.0 max: 3.845703125\n",
      "episode: 31   frame: 5007   score: 2.0   memory length: 209208   epsilon: 0.99049   steps: 214   lr: 0.0005   PER_beta: 0.40401 PER Alpha: 0.59599   reward MA: 1.097   mean loss: 0.02207   mean max Q: 8.8242\n",
      "episode: 32   frame: 5127   score: 0.0   memory length: 209323   epsilon: 0.99026   steps: 120   lr: 0.0005   PER_beta: 0.4041 PER Alpha: 0.5959   reward MA: 1.062   mean loss: 0.0249   mean max Q: 9.0836\n",
      "episode: 33   frame: 5247   score: 0.0   memory length: 209438   epsilon: 0.99003   steps: 120   lr: 0.0005   PER_beta: 0.4042 PER Alpha: 0.5958   reward MA: 1.03   mean loss: 0.02565   mean max Q: 8.7545\n",
      "episode: 34   frame: 5367   score: 0.0   memory length: 209553   epsilon: 0.9898   steps: 120   lr: 0.0005   PER_beta: 0.40429 PER Alpha: 0.59571   reward MA: 1.0   mean loss: 0.02652   mean max Q: 8.6394\n"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "set_seed(seed)\n",
    "run_num = \"18\"\n",
    "name = \"Bootstrapped_run17_stickyactions5percent\"\n",
    "run_name = \"Run\"+ str(run_num) + \"_\" + name\n",
    "\n",
    "\n",
    "from config import *\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# choose which Agent to use\n",
    "from agent import Agent\n",
    "\n",
    "#create fresh environment\n",
    "env = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=sticky_action_prob, full_action_space=False, render_mode='rgb_array')  # Use equivalent parameters to BreakoutDeterministic-v4\n",
    "\n",
    "# setup video recording\n",
    "def video_trigger(_):\n",
    "    if len(episodes) == 0:\n",
    "        return False\n",
    "    else:\n",
    "        curr_ep = episodes[-1]\n",
    "        return (curr_ep > 99 and curr_ep % 100 == 0) \n",
    "\n",
    "video_path = f\"./videos/run{run_num}\"\n",
    "if not os.path.exists(video_path):\n",
    "    os.makedirs(video_path)\n",
    "env = RecordVideo(env, video_folder=video_path, episode_trigger=video_trigger)\n",
    "\n",
    "\n",
    "print(f\"Starting run {run_name}\")\n",
    "train_interval = 1\n",
    "    \n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "rewards, episodes = [], []\n",
    "reward = 0\n",
    "best_eval_reward = curr_mean_reward = last_save_reward = 0\n",
    "\n",
    "#initialize trackers\n",
    "## LOSS TRACKERS ##\n",
    "losses_window = deque(maxlen=10)\n",
    "loss_history = []\n",
    "episode_losses = []\n",
    "loss = mean_loss = 0\n",
    "\n",
    "# ## Q-VALUE TRACKERS ##\n",
    "episode_q_means = []\n",
    "episode_q_maxs = []\n",
    "episode_q_mins = []\n",
    "q_mean_window = deque(maxlen=10)\n",
    "q_max_window = deque(maxlen=10)\n",
    "q_min_window = deque(maxlen=10)\n",
    "q_stats_history = {\n",
    "    'mean': [],\n",
    "    'max': [],\n",
    "    'min': [],\n",
    "    'episode': []\n",
    "}\n",
    "\n",
    "# Epsilon Bump Control Variables\n",
    "plateau_patience = 300\n",
    "episodes_since_improvement = 0\n",
    "epsilon_bump = 0.15  # amount to re-increase epsilon\n",
    "soonest_bump = 2000  # earliest episode to apply epsilon bump\n",
    "\n",
    "frame = 0\n",
    "ep_start = 0\n",
    "training_started_flg = False\n",
    "\n",
    "print(\"Instantiating agent\")\n",
    "\n",
    "#Bootstrapped loading run 17\n",
    "mem_path = './checkpoints/Run17_StdDQN_750Kfr_newest_get_frame_CirularPER_wIS_159356_replay_buffer.pkl'\n",
    "checkpoint = torch.load('./checkpoints/Run17_StdDQN_750Kfr_newest_get_frame_CirularPER_wIS_2000_checkpoint.pt')\n",
    "agent = Agent(action_size, mem_path)  #use when starting with prefilled replay buffer\n",
    "agent.memory.capacity = 1_000_000  #update memory capacity\n",
    "print(\"Updated memory capacity to \", agent.memory.capacity)\n",
    "assert sum(agent.memory.valid_flags) == len(agent.memory.valid_indices) == len(agent.memory.td_errors)\n",
    "\n",
    "td_errors_np = np.array(agent.memory.td_errors, dtype=np.float32)\n",
    "\n",
    "# Print memory stats before\n",
    "num_invalid = np.sum(~np.isfinite(td_errors_np)) + np.sum(td_errors_np <= 0)\n",
    "print(f\"[SANITIZER] Fixing {num_invalid} invalid or nonpositive TD-errors...\")\n",
    "# Fix all issues\n",
    "td_errors_np = np.where(~np.isfinite(td_errors_np) | (td_errors_np <= 0), 1.0, td_errors_np)\n",
    "agent.memory._priority_cache_dirty = True\n",
    "# Convert back to list\n",
    "agent.memory.td_errors = td_errors_np.tolist()\n",
    "agent.policy_net.load_state_dict(checkpoint['policy_net']) # load partially trained policy network weights\n",
    "agent.policy_net = torch.compile(agent.policy_net)\n",
    "agent.target_net.load_state_dict(checkpoint['target_net'])\n",
    "\n",
    "agent.beta = IS_BETA\n",
    "agent.alpha = PER_ALPHA\n",
    "\n",
    "# agent = Agent(action_size, mem_path)  \n",
    "# agent = Agent(action_size)\n",
    "\n",
    "#########################\n",
    "#### LOAD CHECKPOINT ####\n",
    "# metadata = agent.load_checkpoint(\"Run8_Stdized_DDQN_750K_frames\", 2999)  #Edit episode number\n",
    "# frame = metadata['global_frame']\n",
    "# agent.load_replay_buffer(\"Run8_Stdized_DDQN_750K_frames\", 711354)\n",
    "# ep_start = metadata['global_episode']\n",
    "# evaluation_reward = metadata['eval_rewards']\n",
    "# rewards = metadata['rewards']    \n",
    "# episodes = metadata['episodes']\n",
    "# losses_window = metadata['last_10_ep_losses']\n",
    "# loss_history = metadata['loss_tracker']\n",
    "# training_started_flg = True\n",
    "########################\n",
    "\n",
    "\n",
    "start_train_immediate = True\n",
    "frame_max = TRAINING_STEPS\n",
    "e = ep_start\n",
    "\n",
    "while e < EPISODES:\n",
    "    #limit number of frames for consistent testing\n",
    "    if frame >= frame_max:\n",
    "        break\n",
    "\n",
    "    done = False\n",
    "    score = 0\n",
    "    episode_losses = []\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state, _ = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "    fire_ready = True\n",
    "    no_reward_steps = 0\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        #limit number of frames for consitent testing\n",
    "        if frame >= frame_max:\n",
    "            break\n",
    "\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Selet Action (with robust check for FIRE action)\n",
    "        if fire_ready:  \n",
    "            next_state, force_done = reset_after_life_loss(env, history)\n",
    "            if force_done:\n",
    "                break\n",
    "            action = 1\n",
    "            fire_ready = False\n",
    "        else:\n",
    "            trainable_action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "            action = TRAINABLE_ACTIONS[trainable_action]\n",
    "\n",
    "        # Step the environment\n",
    "        state = next_state\n",
    "        next_state, reward, terminations, truncations, info = env.step(action)  \n",
    "        done = truncations or terminations\n",
    "        \n",
    "        # Failsafe to force reset if no reward for 3000 steps (prevents agent from getting stuck)\n",
    "        stuck_limit = 3000\n",
    "        if no_reward_steps > stuck_limit:\n",
    "            done = True\n",
    "            print(f\"[WARNING] No reward for {stuck_limit} steps, forcing reset | \", \"Episode:\", e, \"  Frame:\", frame, ) \n",
    "             \n",
    "        frame_next_state = get_frame(next_state)\n",
    "             \n",
    "        # append next state to history\n",
    "        history[4, :, :] = frame_next_state\n",
    "        \n",
    "        # life handling\n",
    "        lost_life = check_live(life, info['lives'])  #check if the agent has lost a life\n",
    "        if lost_life:\n",
    "            fire_ready = True\n",
    "        life = info['lives']\n",
    "        \n",
    "        r = reward\n",
    "        if r == 0:\n",
    "            no_reward_steps += 1\n",
    "        else:\n",
    "            no_reward_steps = 0 \n",
    "\n",
    "        # Store the transition in replay buffer if it was not a FIRE action\n",
    "        \n",
    "        if action in TRAINABLE_ACTIONS:\n",
    "            trainable_index = TRAINABLE_ACTIONS.index(action)\n",
    "            term_state = done or lost_life\n",
    "            if type(agent.memory).__name__ == \"CircularReplayMemoryPER\":\n",
    "                # print(\"[DEBUG] Using CircularReplayMemoeryPER.push()\")\n",
    "                # agent.memory.push(agent, deepcopy(frame_next_state), trainable_index, r, term_state)  # for use when model is used to estimate new TD-errors\n",
    "                mean_td = np.mean(agent.recent_td_errors) if len(agent.recent_td_errors) > 200 else 1.0\n",
    "                agent.memory.push(frame_next_state.copy(), trainable_index, r, term_state, mean_recent_td_error=mean_td)\n",
    "            else:\n",
    "                agent.memory.push(frame_next_state.copy(), trainable_index, r, term_state)\n",
    "        \n",
    "        # Start training after random sample generation\n",
    "        if training_started_flg == False and (frame == train_frame or (start_train_immediate and frame == 1)):\n",
    "            print(\"Starting training\")\n",
    "            training_started_flg = True\n",
    "            e = ep_start  #reset episode counter when training starts\n",
    "        if(training_started_flg): \n",
    "            if frame % train_interval == 0: # Use adaptive training interval\n",
    "                loss, q_stats = agent.train_policy_net()\n",
    "                episode_losses.append(loss)\n",
    "                episode_q_means.append(q_stats['q_mean'])\n",
    "                episode_q_maxs.append(q_stats['q_max'])\n",
    "                episode_q_mins.append(q_stats['q_min'])\n",
    "            # Update the target network\n",
    "            if (frame % (train_interval * update_target_network_frequency)) == 0:\n",
    "                agent.update_target_net()\n",
    "                print(\"Target network updated at frame: \", frame)\n",
    "        \n",
    "        # Update score and history\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]  # shift history by one erasing oldest frame\n",
    "\n",
    "        ## DEBUG ##\n",
    "        if frame % 1000 == 0 and len(agent.memory.td_errors) > 0:\n",
    "            agent.memory.log_td_error_distribution()\n",
    "            \n",
    "        if done:\n",
    "            e += 1\n",
    "            fire_ready = True\n",
    "            evaluation_reward.append(score)            \n",
    "            \n",
    "\n",
    "            # print episode information every X episodes\n",
    "            if e % 1 == 0:\n",
    "                print(\"episode:\", e, \"  frame:\", frame, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", round(agent.epsilon, 5), \n",
    "                  \"  steps:\", step, \"  lr:\", agent.optimizer.param_groups[0]['lr'], \"  PER_beta:\", round(agent.beta,5), \"PER Alpha:\", round(agent.alpha,5),\n",
    "                  \"  reward MA:\", round(np.mean(evaluation_reward), 3), \n",
    "                  \"  mean loss:\", round(mean_loss, 5), \"  mean max Q:\", round(np.mean(episode_q_maxs), 4))\n",
    "                #   \"  latest step Q max:\", round(q_stats['q_max'], 4))\n",
    "\n",
    "\n",
    "            \n",
    "            if training_started_flg:\n",
    "\n",
    "\n",
    "\n",
    "                episodes.append(e)\n",
    "                rewards.append(np.mean(evaluation_reward))  # record moving average of last evaluation_reward_length episodes\n",
    "\n",
    "                # # adapt training interval to agent performance\n",
    "                # if np.mean(evaluation_reward) < 8:\n",
    "                #     train_interval = 4\n",
    "                # elif np.mean(evaluation_reward) < 15:\n",
    "                #     train_interval = 2\n",
    "                # else:\n",
    "                #     train_interval = 1\n",
    "\n",
    "                ## DEBUG ##\n",
    "                # Check TD-error distribution in Replay Buffer\n",
    "                if e>0 and e % 100 == 0:\n",
    "                    agent.memory.log_td_error_distribution()\n",
    "\n",
    "                # save rolling loss everages every X episodes\n",
    "                if episode_losses:\n",
    "                    mean_loss = sum(episode_losses) / len(episode_losses)\n",
    "                    losses_window.append(mean_loss)\n",
    "                    if e > 9 and e % 10 == 0:\n",
    "                        loss_history.append((np.mean(losses_window), e))\n",
    "\n",
    "                # save rolling Q-score stat averages\n",
    "                q_mean_window.append(np.mean(episode_q_means))\n",
    "                q_max_window.append(np.mean(episode_q_maxs))\n",
    "                q_min_window.append(np.mean(episode_q_mins))\n",
    "                q_stats_history['mean'].append(np.mean(q_mean_window))\n",
    "                q_stats_history['max'].append(np.mean(q_max_window))\n",
    "                q_stats_history['min'].append(np.mean(q_min_window))\n",
    "                q_stats_history['episode'].append(e)\n",
    "                episode_q_means = []\n",
    "                episode_q_maxs = []\n",
    "                episode_q_mins = []\n",
    "\n",
    "                # plot the rewards every X episodes\n",
    "                if e > 0 and e % 50 == 0:\n",
    "                    # clear_output(wait=True)\n",
    "                    pylab.plot(episodes, rewards, 'b')\n",
    "                    pylab.xlabel('Episode #')\n",
    "                    pylab.ylabel('Rolling Mean Episode Scores') \n",
    "                    pylab.title('DQN w PER & Cropped Scoreboard \\n Scores')\n",
    "                    plot_path = f\"./save_graph/run{run_num}/{run_name}_ep{e}_scores.png\"\n",
    "                    os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "                    pylab.savefig(plot_path)\n",
    "                    print(f\"[SAVED PLOT] {plot_path}\")\n",
    "                    pylab.show()\n",
    "                    pylab.close()\n",
    "                \n",
    "                # every X episodes, plot the mean losses\n",
    "                if e > 0 and e % 50 == 0:\n",
    "                    x = [entry[1] for entry in loss_history]\n",
    "                    y = [entry[0] for entry in loss_history]\n",
    "                    pylab.plot(x, y, 'r')\n",
    "                    pylab.xlabel('Episode #')\n",
    "                    pylab.ylabel('Rolling Mean Loss per Episode') \n",
    "                    pylab.title('DQN w PER & Cropped Scoreboard \\n Loss')\n",
    "                    plot_path = f\"./save_graph/run{run_num}/{run_name}_ep{e}_losses.png\"\n",
    "                    os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "                    pylab.savefig(plot_path)\n",
    "                    print(f\"[SAVED PLOT] {plot_path}\")\n",
    "                    pylab.show()\n",
    "                    pylab.close()\n",
    "\n",
    "                # plot Q-value statistics every X episodes\n",
    "                if e > 0 and e % 25 == 0:\n",
    "                    pylab.plot(q_stats_history['episode'], q_stats_history['mean'], label='Q-Mean')\n",
    "                    pylab.plot(q_stats_history['episode'], q_stats_history['max'], label='Q-Max')\n",
    "                    pylab.plot(q_stats_history['episode'], q_stats_history['min'], label='Q-Min')\n",
    "                    pylab.xlabel('Episode #')\n",
    "                    pylab.ylabel('Q-Value')\n",
    "                    pylab.title('Avg per Episode Q-Value Stats')\n",
    "                    pylab.legend(loc='upper left')\n",
    "                    plot_path = f\"./save_graph/run{run_num}/{run_name}_ep{e}_Qstats.png\"\n",
    "                    os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "                    pylab.savefig(plot_path)\n",
    "                    print(f\"[SAVED PLOT] {plot_path}\")\n",
    "                    pylab.show()\n",
    "                    pylab.close()\n",
    "\n",
    "\n",
    "                    ## DEBUG ##\n",
    "                    # print(f\"[PLOT DEBUG] Last 5 q_means: {q_stats_history['mean'][-5:]}\")\n",
    "\n",
    "                # Checkpoint the training process every X episodes \n",
    "                if e > 0 and e % 250 == 0:\n",
    "                    metadata = create_metadata(agent, e, frame, evaluation_reward, rewards, episodes, losses_window, loss_history, q_stats_history, q_mean_window, \\\n",
    "                                               q_max_window, q_min_window, agent.epsilon, alpha=agent.alpha, beta=agent.beta)\n",
    "                    agent.save_checkpoint(metadata, run_name, e)\n",
    "                if e > 0 and e % 500 == 0:\n",
    "                    agent.save_replay_buffer(run_name, frame)\n",
    "\n",
    "                # Check if reward has improved\n",
    "                curr_mean_reward = np.mean(evaluation_reward)\n",
    "                if curr_mean_reward > best_eval_reward:\n",
    "                    best_eval_reward = curr_mean_reward\n",
    "                    episodes_since_improvement = 0\n",
    "                else:\n",
    "                    episodes_since_improvement += 1\n",
    "                \n",
    "                # save model if it is good\n",
    "                if curr_mean_reward > 8 and curr_mean_reward > (1.05 * last_save_reward):\n",
    "                    model_path = f\"./save_model/run{run_num}/good_{run_name}_{e}_eps.pth\"\n",
    "                    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "                    torch.save(agent.policy_net.state_dict(), model_path)\n",
    "                    print(f\"[SAVED MODEL] {model_path}\")\n",
    "                    last_save_reward = curr_mean_reward\n",
    "               \n",
    "                # # # Apply epsilon bump if plateauing\n",
    "                # if e > soonest_bump and episodes_since_improvement >= plateau_patience:\n",
    "                #     if agent.epsilon < agent.epsilon_max:\n",
    "                #         agent.epsilon = min(agent.epsilon + epsilon_bump, agent.epsilon_max)\n",
    "                #         print(f\"[BUMP] Epsilon bumped to {agent.epsilon:.4f} after {plateau_patience} stagnant episodes.\")\n",
    "                #     episodes_since_improvement = 0  # Reset counter after bump\n",
    "\n",
    "\n",
    "\n",
    "# Checkpoint the model at the end of training loop\n",
    "metadata = create_metadata(agent, e, frame, evaluation_reward, rewards, episodes, losses_window, loss_history, q_stats_history, q_mean_window, \\\n",
    "                            q_max_window, q_min_window, agent.epsilon, alpha=agent.alpha, beta=agent.beta)\n",
    "agent.save_checkpoint(metadata, run_name, e)\n",
    "agent.save_replay_buffer(run_name, frame)\n",
    "\n",
    "print(\"Training complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End logging\n",
    "sys.stdout = tee.ipython_stdout\n",
    "tee.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Load checkpoint files ===\n",
    "ddqn_ckpt = torch.load(\"./checkpoints/Run12_DDQN_750K_frames_imprvdBatching_3007_checkpoint.pt\")\n",
    "dqn_ckpt  = torch.load(\"./checkpoints/Run13_StdDQN_750K_frames_imprvdBatching_2673_checkpoint.pt\")\n",
    "\n",
    "# === Extract metadata ===\n",
    "ddqn_meta = ddqn_ckpt[\"metadata\"]\n",
    "dqn_meta = dqn_ckpt[\"metadata\"]\n",
    "\n",
    "# # === Extract episode indices and rolling mean rewards ===\n",
    "# ddqn_episodes = ddqn_meta[\"episodes\"]\n",
    "# ddqn_rewards = ddqn_meta[\"rewards\"]\n",
    "\n",
    "# dqn_episodes = dqn_meta[\"episodes\"]\n",
    "# dqn_rewards = dqn_meta[\"rewards\"]\n",
    "\n",
    "# # === Plot ===\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(dqn_episodes, dqn_rewards, label=\"DQN\", color=\"red\")\n",
    "# plt.plot(ddqn_episodes, ddqn_rewards, label=\"DDQN\", color=\"blue\")\n",
    "# plt.xlabel(\"Episode #\", fontweight=\"bold\")\n",
    "# plt.ylabel(\"Mean Episode Score (Moving Avg)\", fontweight=\"bold\")\n",
    "# plt.legend(loc = \"upper left\")\n",
    "# # Main title\n",
    "# pylab.text(0.5, 1.05, 'Std DQN vs DDQN Scores',\n",
    "#            ha='center', va='bottom', transform=pylab.gca().transAxes,\n",
    "#            fontsize=14, fontweight='semibold')\n",
    "\n",
    "# # Subtitle (smaller font)\n",
    "# pylab.text(0.5, 1.01, '(750K training steps)',\n",
    "#            ha='center', va='bottom', transform=pylab.gca().transAxes,\n",
    "#            fontsize=10, color='gray')\n",
    "\n",
    "\n",
    "# plot_path = f\"./presentation_assets/DQN_vs_DDQN_750K_steps_SCORES.png\"\n",
    "# os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "# pylab.savefig(plot_path, bbox_inches='tight')\n",
    "# print(f\"[SAVED PLOT] {plot_path}\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # === Extract loss data ===\n",
    "ddqn_loss_history = ddqn_meta[\"loss_tracker\"]\n",
    "dqn_loss_history = dqn_meta[\"loss_tracker\"]\n",
    "\n",
    "ddqn_episodes = [entry[1] for entry in ddqn_loss_history]\n",
    "ddqn_losses = [entry[0] for entry in ddqn_loss_history]\n",
    "\n",
    "dqn_episodes = [entry[1] for entry in dqn_loss_history]\n",
    "dqn_losses = [entry[0] for entry in dqn_loss_history]\n",
    "\n",
    "# === Plot ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dqn_episodes, dqn_losses, label=\"DQN\", color=\"red\")\n",
    "plt.plot(ddqn_episodes, ddqn_losses, label=\"DDQN\", color=\"blue\")\n",
    "plt.xlabel(\"Episode #\", fontweight=\"bold\")\n",
    "plt.ylabel(\"Mean Episode Loss (Moving Avg)\", fontweight=\"bold\")\n",
    "plt.legend(loc = \"best\")\n",
    "# Main title\n",
    "pylab.text(0.5, 1.05, 'Std DQN vs DDQN Huber Loss',\n",
    "           ha='center', va='bottom', transform=pylab.gca().transAxes,\n",
    "           fontsize=14, fontweight='semibold')\n",
    "\n",
    "# Subtitle (smaller font)\n",
    "pylab.text(0.5, 1.01, '(750K training steps)',\n",
    "           ha='center', va='bottom', transform=pylab.gca().transAxes,\n",
    "           fontsize=10, color='gray')\n",
    "\n",
    "\n",
    "plot_path = f\"./presentation_assets/DQN_vs_DDQN_750K_steps_LOSSES.png\"\n",
    "os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "pylab.savefig(plot_path, bbox_inches='tight')\n",
    "print(f\"[SAVED PLOT] {plot_path}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chkpt = torch.load(\"./checkpoints/Run12_DDQN_750K_frames_imprvdBatching_3007_checkpoint.pt\")\n",
    "from model import DQN_DualBranch\n",
    "action_size = len(TRAINABLE_ACTIONS)\n",
    "model = DQN_DualBranch(action_size)\n",
    "model.eval()\n",
    "\n",
    "#create dummy input for model\n",
    "dummy_input = torch.rand(BATCH_SIZE, 6, 84, 84)\n",
    "\n",
    "#export to ONNX\n",
    "torch.onnx.export(model, dummy_input, \"Branched Network.onnx\",\n",
    "                  input_names=[\"input\"], output_names=[\"output\"],\n",
    "                  dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "                  opset_version=11)\n",
    "\n",
    "print(\"ONNX model exported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timediff Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using a 4 frame history to train the model, here we will feed the model the 4 frames plus the \"diff\" between each of the 4 frames to help isolate the ball position which is the most important thing for the model to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play Game in Window or Save Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# set random seed to sync visual and recorded game\n",
    "seed = 55\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Load and setup Agent\n",
    "print(\"Instantiating agent\")\n",
    "agent = Agent(action_size)\n",
    "agent.load_policy_net(\"./save_model/run3/good_Run3_DDQN_Serial_InvTimeEpsilon_ddqn_886_eps.pth\")\n",
    "agent.policy_net.eval()\n",
    "agent.epsilon = 0  # Set agent to only exploit the best action\n",
    "\n",
    "\n",
    "# Create environments\n",
    "env_human = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=sticky_action_prob, full_action_space=False, render_mode=\"human\")\n",
    "# env_record = RecordVideo(env_record, video_folder=\"./videos\", episode_trigger=lambda e: True)\n",
    "\n",
    "# Reset to seed\n",
    "state_h, _ = env_human.reset(seed=seed)\n",
    "state_h = do_random_actions(env_human, 20)\n",
    "\n",
    "\n",
    "# Setup History\n",
    "history = np.zeros([HISTORY_SIZE + 1, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state_h, HISTORY_SIZE)\n",
    "\n",
    "done = False\n",
    "fire_ready = True\n",
    "life = number_lives\n",
    "score = 0\n",
    "step = 0\n",
    "\n",
    "while not done:\n",
    "    step += 1\n",
    "\n",
    "    # Select action\n",
    "    if fire_ready:\n",
    "        print(f\"[DEBUG] Agent is not acting  sending FIRE at step {step}\")\n",
    "        action = 1\n",
    "        fire_ready = False\n",
    "    else:\n",
    "        model_action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        action = TRAINABLE_ACTIONS[model_action]\n",
    "\n",
    "    # Step the environment\n",
    "    state_h, reward, term_h, trunc_h, info = env_human.step(action)\n",
    "    done = term_h or trunc_h\n",
    "\n",
    "    # update history\n",
    "    frame_next_state = get_frame(state_h)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    history[:4, :, :] = history[1:, :, :]  #shift history by one erasing oldest frame\n",
    "    \n",
    "    # check if life has been lost\n",
    "    lost_life = check_live(life, info['lives'])  #check if the agent has lost a life\n",
    "    if lost_life:\n",
    "        print(f\"[DEBUG] Lost life detected at step {step}\")\n",
    "        fire_ready = True\n",
    "    life = info['lives']\n",
    "    \n",
    "    # keep track of score\n",
    "    score += reward # update total score\n",
    "\n",
    "    \n",
    "env_human.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play Game Rendered in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display as ipythondisplay, clear_output\n",
    "\n",
    "def show_state_live(frame, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(frame)\n",
    "    plt.title(f\"Step: {step} {info}\")\n",
    "    plt.axis('off')\n",
    "    clear_output(wait=True)\n",
    "    ipythondisplay(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "\n",
    "# set random seed to sync visual and recorded game\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Load and setup Agent\n",
    "# Choose whether to use double DQN\n",
    "double_dqn = False # set to True if using double DQN agent\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "#Initialize agent\n",
    "print(\"Instantiating agent\")\n",
    "agent = Agent(action_size)\n",
    "agent.load_policy_net(\"./save_model/Run1_serial/good_breakout_dqn_1256_eps.pth\")\n",
    "agent.policy_net.eval()\n",
    "agent.epsilon = 0  # Set agent to only exploit the best action\n",
    "\n",
    "\n",
    "# Create environments\n",
    "env = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=sticky_action_prob, full_action_space=False, render_mode=\"rgb_array\")\n",
    "# Use RecordVideo to save the video to the \"videos\" directory\n",
    "# env = RecordVideo(env, video_folder=\"./videos\", episode_trigger=lambda episode_id: True)\n",
    "\n",
    "# Reset the environment\n",
    "state, _ = env.reset(seed=seed)\n",
    "\n",
    "# Setup History\n",
    "history = np.zeros([HISTORY_SIZE + 1, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "#Initialize variables\n",
    "step = 0\n",
    "done = False\n",
    "fire_ready = True\n",
    "life = number_lives\n",
    "score = 0\n",
    "\n",
    "while not done:\n",
    "\n",
    "    # Render the current frame live in the notebook\n",
    "    show_state_live(state, step)   \n",
    "\n",
    "    # Select action\n",
    "    if fire_ready:\n",
    "        action = 1\n",
    "        fire_ready = False\n",
    "    else:\n",
    "        model_action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        action = TRAINABLE_ACTIONS[model_action]\n",
    "\n",
    "    # Step the environment\n",
    "    next_state, reward, term, trunc, info = env.step(action)\n",
    "    done = term or trunc\n",
    "\n",
    "    # update history\n",
    "    frame_next_state = get_frame(state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    history[:4, :, :] = history[1:, :, :]  #shift history by one erasing oldest frame\n",
    "    \n",
    "    # check if life has been lost\n",
    "    lost_life = check_live(life, info['lives'])  #check if the agent has lost a life\n",
    "    if lost_life:\n",
    "        fire_ready = True  \n",
    "        # do_random_actions(env_human, 30) #IMPT: introduce randomness to game and paddle position before next life\n",
    "    life = info['lives']\n",
    "    \n",
    "    # keep track of score\n",
    "    score += reward # update total score\n",
    "\n",
    "    state = next_state\n",
    "    step += 1\n",
    "\n",
    "    \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from config import *\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "\n",
    "# Load and setup Agent\n",
    "# Choose which agent class to load\n",
    "from agent import Agent\n",
    "# from agent_timediff import Agent\n",
    "\n",
    "# 50 unique, fixed seeds\n",
    "seeds = [765,817,53,705,990,511,236,661,654,418,804,968,1,749,125,293,985,574, \n",
    "         447,948,687,317,280,645,927,842,309,616,717,930,778,323,595,798,195,11,\n",
    "         483,316,690,951,196,307,906,558,516,844,410,965,371,886]\n",
    "\n",
    "\n",
    "#Initialize agent\n",
    "print(\"Instantiating agent\")\n",
    "agent = Agent(action_size)\n",
    "# agent.load_policy_net(\"./save_model/run12/good_Run12_DDQN_750K_frames_imprvdBatching_2319_eps.pth\")\n",
    "agent.load_policy_net(\"./save_model/run13/good_Run13_StdDQN_750K_frames_imprvdBatching_2477_eps.pth\")\n",
    "# agent.load_policy_net(\"./save_model/run14/good_Run14_StdDQN_750Kfr_timediff_new_get_frame_2425_eps.pth\")\n",
    "agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "agent.target_net.eval()\n",
    "agent.policy_net.eval()\n",
    "agent.epsilon = 0.05  # Set agent to use model action 95% of the time for robustness\n",
    "\n",
    "\n",
    "# Create environments\n",
    "env = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=0.0, full_action_space=False, render_mode=\"rgb_array\")\n",
    "# Use RecordVideo to save the video to the \"videos\" directory\n",
    "# video_path = f\"./videos/testing/run13\"\n",
    "# if not os.path.exists(video_path):\n",
    "#     os.makedirs(video_path)\n",
    "# env = RecordVideo(env, video_folder=video_path, episode_trigger=lambda e: True)\n",
    "\n",
    "\n",
    "scores = []\n",
    "\n",
    "for e, seed in enumerate(seeds):\n",
    "    set_seed(seed)\n",
    "    # Reset the environment\n",
    "    state, _ = env.reset(seed=seed)\n",
    "\n",
    "    # Setup History\n",
    "    history = np.zeros([HISTORY_SIZE + 1, 84, 84], dtype=np.uint8)\n",
    "    get_init_state(history, state, HISTORY_SIZE)  #non cropped version\n",
    "    # new_get_init_state(history, state, HISTORY_SIZE)  #cropped version\n",
    "\n",
    "    #Initialize variables\n",
    "    step = 0\n",
    "    done = False\n",
    "    fire_ready = True\n",
    "    life = number_lives\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        # Selet Action (with robust check for FIRE action)\n",
    "        if fire_ready:\n",
    "            next_state, force_done = reset_after_life_loss(env, history)\n",
    "            if force_done:\n",
    "                break\n",
    "            action = 1\n",
    "            fire_ready = False\n",
    "        else:\n",
    "            trainable_action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "            action = TRAINABLE_ACTIONS[trainable_action]\n",
    "\n",
    "        # Step the environment\n",
    "    \n",
    "        next_state, reward, term, trunc, info = env.step(action)\n",
    "        done = term or trunc\n",
    "        # update total score\n",
    "        score += reward \n",
    "\n",
    "        # update history\n",
    "        history[4, :, :] = get_frame(next_state)  #non cropped version\n",
    "        # history[4, :, :] = new_get_frame(next_state)  #cropped version\n",
    "        history[:4, :, :] = history[1:, :, :]  #shift history by one erasing oldest frame\n",
    "        \n",
    "        # check if life has been lost\n",
    "        lost_life = check_live(life, info['lives'])  #check if the agent has lost a life\n",
    "        if lost_life:\n",
    "            fire_ready = True  \n",
    "        life = info['lives']\n",
    "        \n",
    "        \n",
    "        step += 1\n",
    "    \n",
    "    print(\"episode:\", e, \"  seed:\", seed, \"  score:\", score, \"  epsilon:\", round(agent.epsilon, 5), \n",
    "    \"  steps:\", step)\n",
    "    \n",
    "    scores.append(score)\n",
    "\n",
    "env.close()\n",
    "print(\"Mean Score: \", np.mean(scores))\n",
    "print(\"Std Score: \", np.std(scores))\n",
    "print(\"Max Score: \", np.max(scores))\n",
    "print(\"Min Score: \", np.min(scores))\n",
    "print(\"Median Score: \", np.median(scores))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ALE/Breakout-v5', frameskip=4, repeat_action_probability=0.0, full_action_space=False)\n",
    "env.reset()\n",
    "next_state, reward, terminations, truncations, info = env.step(torch.tensor([[1]]))\n",
    "done = truncations or terminations\n",
    "print(\"reward: \", reward)\n",
    "print(\"done: \", done)\n",
    "print(\"info: \", info)\n",
    "print(next_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)\n",
    "print(\"Expected type:\", type(env.action_space.sample()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"agent memory type: \", type(agent.memory.memory[0]))\n",
    "print(\"agent memory[0]: \", agent.memory.memory[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu118\n",
      "11.8\n",
      "True\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "torch._inductor.config.is_triton_enabled does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\utils\\_config_module.py:143\u001b[0m, in \u001b[0;36mConfigModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;66;03m# make hasattr() work properly\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'is_triton_enabled'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_built())\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTriton available:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inductor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_triton_enabled\u001b[49m())\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\utils\\_config_module.py:146\u001b[0m, in \u001b[0;36mConfigModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config[name]\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;66;03m# make hasattr() work properly\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: torch._inductor.config.is_triton_enabled does not exist"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cuda.is_built())\n",
    "print(\"Triton available:\", torch._inductor.config.is_triton_enabled())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\compile_fx.py:150: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "backend='inductor' raised:\nRuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m84\u001b[39m, \u001b[38;5;241m84\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 23\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompile + inference worked:\u001b[39m\u001b[38;5;124m\"\u001b[39m, out\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:433\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    428\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    429\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[0;32m    430\u001b[0m )\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[0;32m    437\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[0;32m    438\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1116\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[1;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[0;32m   1110\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[0;32m   1111\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[0;32m   1112\u001b[0m             )\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[1;32m-> 1116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:948\u001b[0m, in \u001b[0;36mConvertFrame.__call__\u001b[1;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[0;32m    946\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    947\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 948\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    951\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    952\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:472\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[1;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[0;32m    458\u001b[0m compile_id \u001b[38;5;241m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[0;32m    460\u001b[0m signpost_event(\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m     },\n\u001b[0;32m    470\u001b[0m )\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_utils_internal.py:84\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m     83\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStrobelightCompileTimeProfiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofile_compile_time\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_strobelight\\compile_time_profiler.py:129\u001b[0m, in \u001b[0;36mStrobelightCompileTimeProfiler.profile_compile_time\u001b[1;34m(cls, func, phase_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprofile_compile_time\u001b[39m(\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mcls\u001b[39m, func: Any, phase_name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m--> 129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprofiler is not set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:817\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[0;32m    815\u001b[0m guarded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 817\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[0;32m    820\u001b[0m     Unsupported,\n\u001b[0;32m    821\u001b[0m     TorchRuntimeError,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    828\u001b[0m     BisectValidationException,\n\u001b[0;32m    829\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\utils.py:231\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    230\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 231\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    233\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:636\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[1;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[0;32m    634\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 636\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py:1185\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[1;34m(code, transformations, safe)\u001b[0m\n\u001b[0;32m   1182\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[0;32m   1183\u001b[0m propagate_line_nums(instructions)\n\u001b[1;32m-> 1185\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:178\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m cleanup \u001b[38;5;241m=\u001b[39m setup_compile_debug()\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:582\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[1;34m(instructions, code_options)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[1;32m--> 582\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[0;32m    584\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2451\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 2451\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:893\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 893\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    894\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:805\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 805\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException:\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2642\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[1;34m(self, inst)\u001b[0m\n\u001b[0;32m   2641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mRETURN_VALUE\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[1;32m-> 2642\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2627\u001b[0m, in \u001b[0;36mInstructionTranslator._return\u001b[1;34m(self, inst)\u001b[0m\n\u001b[0;32m   2622\u001b[0m _step_logger()(\n\u001b[0;32m   2623\u001b[0m     logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[0;32m   2624\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo done tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2625\u001b[0m )\n\u001b[0;32m   2626\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m triggered compile\u001b[39m\u001b[38;5;124m\"\u001b[39m, inst\u001b[38;5;241m.\u001b[39mopname)\n\u001b[1;32m-> 2627\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_subgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraphCompileReason\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   2631\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2632\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2633\u001b[0m return_inst \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2634\u001b[0m     create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2636\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_CONST\u001b[39m\u001b[38;5;124m\"\u001b[39m, argval\u001b[38;5;241m=\u001b[39minst\u001b[38;5;241m.\u001b[39margval)\n\u001b[0;32m   2637\u001b[0m )\n\u001b[0;32m   2638\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39madd_output_instructions([return_inst])\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\output_graph.py:1098\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[1;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[0;32m   1095\u001b[0m append_prefix_insts()\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;66;03m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_call_fx_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mreversed\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstack_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1099\u001b[0m     \u001b[38;5;241m+\u001b[39m [create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNPACK_SEQUENCE\u001b[39m\u001b[38;5;124m\"\u001b[39m, arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(stack_values))]\n\u001b[0;32m   1100\u001b[0m )\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;66;03m# restore all the live local vars\u001b[39;00m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[0;32m   1103\u001b[0m     [PyCodegen(tx)\u001b[38;5;241m.\u001b[39mcreate_store(var) \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(restore_vars)]\n\u001b[0;32m   1104\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\output_graph.py:1318\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[1;34m(self, tx, rv, root)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracing_context\u001b[38;5;241m.\u001b[39mfake_mode \u001b[38;5;241m=\u001b[39m backend_fake_mode\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_global_state():\n\u001b[1;32m-> 1318\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lazy_graph_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _LazyGraphModule\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compiled_fn, _LazyGraphModule) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(compiled_fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), _LazyGraphModule)\n\u001b[0;32m   1324\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m compiled_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_lazy_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1328\u001b[0m     \u001b[38;5;66;03m# this is a _LazyGraphModule. This makes it easier for dynamo to\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m     \u001b[38;5;66;03m# optimize a _LazyGraphModule.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\utils.py:231\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    230\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 231\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    233\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\output_graph.py:1409\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[1;34m(self, gm)\u001b[0m\n\u001b[0;32m   1407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendCompilerFailed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiler_fn, e)\u001b[38;5;241m.\u001b[39mwith_traceback(\n\u001b[0;32m   1410\u001b[0m         e\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[0;32m   1411\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m signpost_event(\n\u001b[0;32m   1414\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1415\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1421\u001b[0m     },\n\u001b[0;32m   1422\u001b[0m )\n\u001b[0;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\output_graph.py:1390\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[1;34m(self, gm)\u001b[0m\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverify_correctness:\n\u001b[0;32m   1389\u001b[0m     compiler_fn \u001b[38;5;241m=\u001b[39m WrapperBackend(compiler_fn)\n\u001b[1;32m-> 1390\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1392\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(compiled_fn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiler_fn did not return callable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py:129\u001b[0m, in \u001b[0;36mWrapBackendDebug.__call__\u001b[1;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\__init__.py:1951\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[1;34m(self, model_, inputs_)\u001b[0m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_, inputs_):\n\u001b[0;32m   1949\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_fx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[1;32m-> 1951\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\compile_fx.py:1505\u001b[0m, in \u001b[0;36mcompile_fx\u001b[1;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m inference_compiler(unlifted_gm, example_inputs_)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m V\u001b[38;5;241m.\u001b[39mset_fake_mode(fake_mode), torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mtracing(\n\u001b[0;32m   1503\u001b[0m     tracing_context\n\u001b[0;32m   1504\u001b[0m ), compiled_autograd\u001b[38;5;241m.\u001b[39mdisable(), functorch_config\u001b[38;5;241m.\u001b[39mpatch(unlift_effect_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m-> 1505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maot_autograd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1512\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\backends\\common.py:69\u001b[0m, in \u001b[0;36mAotAutograd.__call__\u001b[1;34m(self, gm, example_inputs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# NB: NOT cloned!\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[1;32m---> 69\u001b[0m         cg \u001b[38;5;241m=\u001b[39m \u001b[43maot_module_simplified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m         counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m disable(cg)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_functorch\\aot_autograd.py:954\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[1;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler)\u001b[0m\n\u001b[0;32m    938\u001b[0m aot_config \u001b[38;5;241m=\u001b[39m AOTConfig(\n\u001b[0;32m    939\u001b[0m     fw_compiler\u001b[38;5;241m=\u001b[39mfw_compiler,\n\u001b[0;32m    940\u001b[0m     bw_compiler\u001b[38;5;241m=\u001b[39mbw_compiler,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    950\u001b[0m     no_tangents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    951\u001b[0m )\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compiled_autograd\u001b[38;5;241m.\u001b[39mdisable():\n\u001b[1;32m--> 954\u001b[0m     compiled_fn, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctional_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mGmWrapper):\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# This function is called by the flatten_graph_inputs wrapper, which boxes\u001b[39;00m\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;66;03m# the inputs so that they can be freed before the end of this scope.\u001b[39;00m\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;66;03m# For overhead reasons, this is not the default wrapper, see comment:\u001b[39;00m\n\u001b[0;32m    964\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/122535/files#r1560096481\u001b[39;00m\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mboxed_forward\u001b[39m(runtime_args: List[Any]):\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\utils.py:231\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    230\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 231\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    233\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_functorch\\aot_autograd.py:687\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[1;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[0;32m    683\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m aot_dispatch_base\n\u001b[0;32m    685\u001b[0m compiler_fn \u001b[38;5;241m=\u001b[39m choose_dispatcher(needs_autograd, aot_config)\n\u001b[1;32m--> 687\u001b[0m compiled_fn, fw_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn, fw_metadata\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py:168\u001b[0m, in \u001b[0;36maot_dispatch_base\u001b[1;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[0;32m    161\u001b[0m     tracing_context\u001b[38;5;241m.\u001b[39mfw_metadata \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    162\u001b[0m         fw_metadata\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m maybe_subclass_meta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m maybe_subclass_meta\u001b[38;5;241m.\u001b[39mfw_metadata\n\u001b[0;32m    165\u001b[0m     )\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TracingContext\u001b[38;5;241m.\u001b[39mreport_output_strides() \u001b[38;5;28;01mas\u001b[39;00m fwd_output_strides:\n\u001b[1;32m--> 168\u001b[0m     compiled_fw \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfw_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdated_flat_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fakified_out_wrapper\u001b[38;5;241m.\u001b[39mneeds_post_compile:\n\u001b[0;32m    171\u001b[0m     fakified_out_wrapper\u001b[38;5;241m.\u001b[39mset_fwd_output_strides(fwd_output_strides)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\utils.py:231\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    230\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 231\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    233\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\compile_fx.py:1410\u001b[0m, in \u001b[0;36mcompile_fx.<locals>.fw_compiler_base\u001b[1;34m(model, example_inputs, is_inference)\u001b[0m\n\u001b[0;32m   1402\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m orig_output_end_idx \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m num_model_outputs\n\u001b[0;32m   1404\u001b[0m     user_visible_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mfromkeys(\n\u001b[0;32m   1405\u001b[0m         n\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m   1406\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m model_outputs[original_output_start_index:orig_output_end_idx]\n\u001b[0;32m   1407\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mNode)\n\u001b[0;32m   1408\u001b[0m     )\n\u001b[1;32m-> 1410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_input_idxs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_static_input_idxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfixed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_inference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboxed_forward_device_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforward_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1418\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_visible_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_visible_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1419\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py:84\u001b[0m, in \u001b[0;36mwrap_compiler_debug.<locals>.debug_wrapper\u001b[1;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrepro_after \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# Call the compiler_fn - which is either aot_autograd or inductor\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;66;03m# with fake inputs\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     inner_compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# TODO: Failures here are troublesome because no real inputs,\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# need a different serialization strategy\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrepro_after \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\debug.py:304\u001b[0m, in \u001b[0;36mDebugContext.wrap.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m DebugContext():\n\u001b[1;32m--> 304\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\utils.py:231\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    230\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 231\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    233\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\compile_fx.py:527\u001b[0m, in \u001b[0;36mcompile_fx_inner\u001b[1;34m(gm, example_inputs, cudagraphs, static_input_idxs, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, boxed_forward_device_index, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[0;32m    517\u001b[0m     compiled_graph \u001b[38;5;241m=\u001b[39m FxGraphCache\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    518\u001b[0m         fx_codegen_and_compile,\n\u001b[0;32m    519\u001b[0m         gm,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m         remote\u001b[38;5;241m=\u001b[39mfx_graph_remote_cache,\n\u001b[0;32m    525\u001b[0m     )\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 527\u001b[0m     compiled_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFX codegen and compilation took \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start)\n\u001b[0;32m    533\u001b[0m \u001b[38;5;66;03m# check cudagraph disabling reasons from inductor lowering\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\compile_fx.py:831\u001b[0m, in \u001b[0;36mfx_codegen_and_compile\u001b[1;34m(gm, example_inputs, cudagraphs, static_input_idxs, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[0;32m    828\u001b[0m             output_strides\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    830\u001b[0m _check_triton_bf16_support(graph)\n\u001b[1;32m--> 831\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    832\u001b[0m num_bytes, nodes_num_elem, node_runtimes \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mcount_bytes()\n\u001b[0;32m    833\u001b[0m metrics\u001b[38;5;241m.\u001b[39mnum_bytes_accessed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_bytes\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\graph.py:1751\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_fn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AotCodeCompiler\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;28mself\u001b[39m, code, serialized_extern_kernel_nodes, cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda\n\u001b[0;32m   1749\u001b[0m     )\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcall\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\utils.py:231\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    230\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 231\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    233\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\graph.py:1680\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1675\u001b[0m \u001b[38;5;129m@dynamo_timed\u001b[39m(phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_gen\u001b[39m\u001b[38;5;124m\"\u001b[39m, fwd_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1677\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodecache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyCodeCache\n\u001b[0;32m   1679\u001b[0m     code, linemap \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1680\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodegen_with_cpp_wrapper() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpp_wrapper \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1681\u001b[0m     )\n\u001b[0;32m   1683\u001b[0m     output_code_log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput code: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, code)\n\u001b[0;32m   1684\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\graph.py:1636\u001b[0m, in \u001b[0;36mGraphLowering.codegen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscheduler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scheduler\n\u001b[0;32m   1634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_wrapper_code()\n\u001b[1;32m-> 1636\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m \u001b[43mScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1637\u001b[0m V\u001b[38;5;241m.\u001b[39mdebug\u001b[38;5;241m.\u001b[39mdraw_orig_fx_graph(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_gm, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mnodes)\n\u001b[0;32m   1639\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapper_code\u001b[38;5;241m.\u001b[39mpush_codegened_graph(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\utils.py:231\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    230\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 231\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    233\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\scheduler.py:1364\u001b[0m, in \u001b[0;36mScheduler.__init__\u001b[1;34m(self, nodes)\u001b[0m\n\u001b[0;32m   1356\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_grad_graph_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(_post_grad_graph_counter)\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_buffer_names \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1359\u001b[0m     \u001b[38;5;241m*\u001b[39mV\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mgraph_inputs\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[0;32m   1360\u001b[0m     \u001b[38;5;241m*\u001b[39mV\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mconstants\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[0;32m   1361\u001b[0m     \u001b[38;5;241m*\u001b[39mV\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mtorchbind_constants\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[0;32m   1362\u001b[0m }\n\u001b[1;32m-> 1364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_scheduler_node(n) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes]\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;66;03m# some new constants could have been created above\u001b[39;00m\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_buffer_names\u001b[38;5;241m.\u001b[39mupdate(V\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mconstants\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\scheduler.py:1364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1356\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_grad_graph_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(_post_grad_graph_counter)\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_buffer_names \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1359\u001b[0m     \u001b[38;5;241m*\u001b[39mV\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mgraph_inputs\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[0;32m   1360\u001b[0m     \u001b[38;5;241m*\u001b[39mV\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mconstants\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[0;32m   1361\u001b[0m     \u001b[38;5;241m*\u001b[39mV\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mtorchbind_constants\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[0;32m   1362\u001b[0m }\n\u001b[1;32m-> 1364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_scheduler_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes]\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;66;03m# some new constants could have been created above\u001b[39;00m\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_buffer_names\u001b[38;5;241m.\u001b[39mupdate(V\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mconstants\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\scheduler.py:1462\u001b[0m, in \u001b[0;36mScheduler.create_scheduler_node\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m   1460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NopKernelSchedulerNode(\u001b[38;5;28mself\u001b[39m, node)\n\u001b[0;32m   1461\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, (ir\u001b[38;5;241m.\u001b[39mComputedBuffer, ir\u001b[38;5;241m.\u001b[39mTemplateBuffer)):\n\u001b[1;32m-> 1462\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSchedulerNode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, ir\u001b[38;5;241m.\u001b[39mExternKernel):\n\u001b[0;32m   1464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ExternKernelSchedulerNode(\u001b[38;5;28mself\u001b[39m, node)\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\scheduler.py:731\u001b[0m, in \u001b[0;36mSchedulerNode.__init__\u001b[1;34m(self, scheduler, node)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    726\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    727\u001b[0m     scheduler: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    728\u001b[0m     node: Union[ir\u001b[38;5;241m.\u001b[39mComputedBuffer, ir\u001b[38;5;241m.\u001b[39mTemplateBuffer],\n\u001b[0;32m    729\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(scheduler, node)\n\u001b[1;32m--> 731\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_attrs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\scheduler.py:742\u001b[0m, in \u001b[0;36mSchedulerNode._compute_attrs\u001b[1;34m(self, extra_indexing_constraints)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode, (ir\u001b[38;5;241m.\u001b[39mComputedBuffer, ir\u001b[38;5;241m.\u001b[39mTemplateBuffer))\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sizes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39msimplify_and_reorder(\n\u001b[0;32m    739\u001b[0m     extra_indexing_constraints\u001b[38;5;241m=\u001b[39mextra_indexing_constraints\n\u001b[0;32m    740\u001b[0m )\n\u001b[1;32m--> 742\u001b[0m group_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgroup_fn\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mget_device(), group_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sizes))\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode, ir\u001b[38;5;241m.\u001b[39mTemplateBuffer):\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\scheduler.py:2663\u001b[0m, in \u001b[0;36mScheduler.get_backend\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m   2661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_backend\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: torch\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseScheduling\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2662\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackends:\n\u001b[1;32m-> 2663\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackends[device] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackends[device]\n",
      "File \u001b[1;32mc:\\Users\\rbisk\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_inductor\\scheduler.py:2655\u001b[0m, in \u001b[0;36mScheduler.create_backend\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m   2651\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2652\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_props\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_props\u001b[38;5;241m.\u001b[39mmajor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_props\u001b[38;5;241m.\u001b[39mminor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: B950\u001b[39;00m\n\u001b[0;32m   2653\u001b[0m         )\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m is_gpu(device\u001b[38;5;241m.\u001b[39mtype):\n\u001b[1;32m-> 2655\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2656\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: B950\u001b[39;00m\n\u001b[0;32m   2657\u001b[0m         )\n\u001b[0;32m   2659\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m device_scheduling(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mBackendCompilerFailed\u001b[0m: backend='inductor' raised:\nRuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "class Simple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 84 * 84, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = Simple().cuda()\n",
    "compiled_model = torch.compile(model)\n",
    "\n",
    "x = torch.randn(1, 4, 84, 84, device='cuda')\n",
    "with torch.no_grad():\n",
    "    out = compiled_model(x)\n",
    "\n",
    "print(\"Compile + inference worked:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug Harnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import memory as mem\n",
    "\n",
    "# Use a small capacity for easy debugging\n",
    "capacity = 10\n",
    "history_size = 3\n",
    "memory = mem.CircularReplayMemoryPER(capacity, history_size)\n",
    "\n",
    "# Push 30 fake transitions (will cause multiple wraps)\n",
    "for step in range(30):\n",
    "    frame = np.ones((84, 84), dtype=np.uint8) * step\n",
    "    action = random.randint(0, 3)\n",
    "    reward = random.random()\n",
    "    done = (step % 7 == 0)  # Random done flag every few steps\n",
    "\n",
    "    memory.push(frame, action, reward, done)\n",
    "\n",
    "    num_flags = sum(memory.valid_flags)\n",
    "    num_indices = len(memory.valid_indices)\n",
    "\n",
    "    if num_flags != num_indices:\n",
    "        print(f\" Mismatch at step {step}:\")\n",
    "        print(f\"   valid_flags count = {num_flags}\")\n",
    "        print(f\"   valid_indices count = {num_indices}\")\n",
    "        print(f\"   position = {memory.position}\")\n",
    "        print(f\"   size = {memory.size}\")\n",
    "        print(f\"   flags: {memory.valid_flags}\")\n",
    "        print(f\"   valid_indices: {memory.valid_indices}\")\n",
    "        raise AssertionError(\"valid_flags and valid_indices out of sync\")\n",
    "\n",
    "print(\" All checks passed  valid_flags == valid_indices through all pushes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import memory as RepBuff\n",
    "import traceback\n",
    "\n",
    "# Configuration\n",
    "capacity = 1000\n",
    "history_size = 4\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Configuration\n",
    "capacity = 1000\n",
    "history_size = 4\n",
    "batch_size = 16\n",
    "alpha = 0.6\n",
    "\n",
    "# Initialize memory\n",
    "memory = RepBuff.CircularReplayMemoryPER(capacity=capacity, history_size=history_size)\n",
    "\n",
    "# Track previous cache values for comparison\n",
    "previous_probs = None\n",
    "\n",
    "# Simulate a bunch of pushes\n",
    "for step in range(1500):\n",
    "    print(f\"Step {step}, \")\n",
    "    frame = np.ones((84, 84), dtype=np.uint8) * step\n",
    "    action = random.randint(0, 3)\n",
    "    reward = random.random()\n",
    "    done = (step % 50 == 0)\n",
    "\n",
    "\n",
    "    memory.push(frame, action, reward, done)\n",
    "\n",
    "    # Occasionally update TD-errors\n",
    "    if step > 100 and len(memory.valid_indices) >= batch_size:\n",
    "        indices = random.sample(range(len(memory.valid_indices)), min(len(memory.valid_indices), batch_size))\n",
    "        print(len(indices))\n",
    "        new_td = np.random.rand(batch_size).astype(np.float32)\n",
    "        memory.update_td_errors(indices, new_td)\n",
    "\n",
    "    # Check probability stability and cache correctness\n",
    "    if len(memory.valid_indices) >= batch_size:\n",
    "        try:\n",
    "            # Trigger cache update and record internal state\n",
    "            probs = memory.get_sampling_probs(memory.valid_indices)\n",
    "            total_prob = probs.sum()\n",
    "\n",
    "            # Cache sanity checks\n",
    "            assert hasattr(memory, \"_cached_probs\"), \"Missing cached_probs\"\n",
    "            assert memory._cached_probs is not None, \"Cached probs not populated\"\n",
    "            assert np.allclose(memory._cached_probs.sum(), 1.0, atol=1e-4), \"Cached probs do not sum to 1\"\n",
    "\n",
    "            # Compare with previous cache state\n",
    "            if previous_probs is not None:\n",
    "                if np.array_equal(previous_probs, memory._cached_probs):\n",
    "                    print(f\"[DEBUG] Cache unchanged at step {step}  may be okay if no new TDs pushed\")\n",
    "                else:\n",
    "                    print(f\"[DEBUG] Cache updated at step {step}\")\n",
    "            previous_probs = memory._cached_probs.copy()\n",
    "\n",
    "            if np.isnan(total_prob) or total_prob == 0:\n",
    "                print(\"[ERROR] Total probability is invalid at step:\", step)\n",
    "                print(\"TD errors:\", [memory.td_errors[i] for i in memory.valid_indices[:10]])\n",
    "                print(\"Sampling probs:\", probs[:10])\n",
    "                raise AssertionError(\"Invalid sampling distribution\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"[EXCEPTION]\", str(e))\n",
    "            traceback.print_exc()\n",
    "            break\n",
    "\n",
    "print(\" Debug harness completed without total_priority = 0 and with valid cache behavior\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsl_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
